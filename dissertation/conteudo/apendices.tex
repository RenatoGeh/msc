%!TeX root=../tese.tex

\chapter{Appendices}

\section{Proofs}

\begin{theorem}
  \label{thm:summix}
  Let $\mathcal{C}$ a probabilistic circuit whose first $l$ layers are composed solely of sum
  nodes. Call $\Nodes$ the set of all nodes in layer $l+1$. $\mathcal{C}$ is equivalent to a PC
  $\mathcal{C}'$ whose root is a sum node with $\Nodes$ as children.
\end{theorem}
\begin{proof}
  We adapt a similar proof due to \citet{jaini18b}. Every sum node is of the form
  \begin{equation*}
    \Sum(\set{x})=\sum_{\Child\in\Ch(\Sum)}w_{\Sum,\Child}\cdot\Child(\set{x}).
  \end{equation*}
  Particularly, every child $\Child$ in a sum node in layer $1\leq i\leq l-1$, is a sum node, and
  so for the first layer we have that
  \begin{align*}
    \Sum(\set{x})&=\sum_{\Child_1\in\Ch(\Sum)}w_{\Sum,\Child_1}\sum_{\Child_2\in\Ch(\Child_1)}
    w_{\Child_1,\Child_2}\Child_2(\set{x})\\
                 &=\sum_{\Child_1\in\Ch(\Sum)}\sum_{\Child_2\in\Ch(\Child_1)}w_{\Sum,\Child_1}
    w_{\Child_1,\Child_2}\Child_2(\set{x}).
  \end{align*}
  Define a one-to-one mapping that takes a tuple $(\Child_1,\Child_2)$ where $\Child_1\in\Ch(\Sum)$
  and $\Child_2\in\Ch(\Child_1)$ and returns a (unique) path from $\Sum$ to every grandchild
  $\Child_2$ of $\Sum$. Call $\set{K}$ the set of all paths, and $w_{\Sum,\Child_1}$ and
    $w_{\Child_1,\Child_2}$ the weights for one such path. We can merge these two weights into a
  single weight $w_{\Sum,\Child_2}'= w_{\Sum,\Child_1}\cdot w_{\Child_1,\Child_2}$, yielding
  \begin{equation*}
    \Sum(\set{x})=\sum_{(w_{\Sum,\Child_1},w_{\Child_1,\Child_2})\in\set{K}} w_{\Sum,\Child_2}'
    \Child_2(\set{x}).
  \end{equation*}
  This ensures that two consecutive sum layers can be collapsed into a single layer. Particularly,
  for the first (root) and second layers, the above transformation generates a circuit with one
  fewer layer and whose root has $\bigo(nm)$ edges, where $n$ and $m$ are the number of edges coming
  from the original root and its children respectively. We can apply this procedure until there are
  no more consecutive sum nodes. This results in a PC of the form
  \begin{equation*}
    \Sum(\set{x})=\sum_{\Child\in\Ch(S)} w_{\Sum,\Child}\Node(\set{x}),
  \end{equation*}
  where $\Node\in\Nodes$. The number of children of the resulting root sum node will be exponential
  on the number of edges of its children.
\end{proof}

\begin{theorem}[Standardization]
  \label{thm:standard}
  Any probabilistic circuit $\mathcal{C}$ can be reduced to a circuit where every sum node contains
  only products or inputs and every product node contains only sums or inputs.
\end{theorem}
\begin{proof}
  If $\mathcal{C}$ is already standard we are done. Otherwise, there exists either (i) a sum node
  $\Sum$ with a sum $\Sum'$ as child; or (ii) a product node $\Prod$ with a product $\Prod'$ as
  child. We first address (i): let $w$ be the weight of edge $\edge{\Sum\Sum'}$ and $\theta_i$
  the weights from all edges coming out from $\Sum'$.
  \begin{center}
    \begin{tikzpicture}
      \newSumNode[label=above right:{$\Sum$}]{s}{0,0};
      \newProdNode{p1}{$(s) + (-1.25,-1)$};
      \newSumNode[label=above left:{$\Sum'$}]{sl}{$(s) + (0,-1.5)$};
      \newProdNode{p2}{$(s) + (1.25,-1)$};
      \newProdNode{q1}{$(sl) + (-1.25,-1)$};
      \newProdNode{q2}{$(sl) + (0,-1)$};
      \newProdNode{q3}{$(sl) + (1.25,-1)$};
      \draw[edge] (s) edge (p1);
      \draw[edge] (s) edge node[midway,right] {\scriptsize$w$} (sl);
      \draw[edge] (s) edge (p2);
      \draw[edge] (sl) edge node[midway,left] {\scriptsize$\theta_1$} (q1);
      \draw[edge] (sl) edge node[midway,left] {\scriptsize$\theta_2$} (q2);
      \draw[edge] (sl) edge node[midway,left] {\scriptsize$\theta_3$} (q3);

      \draw[edge,very thick,red] (2.25,-1.5) -- node[midway,above] {Standardize} (5.5,-1.5);

      \newSumNode[label=above right:{$\Sum$}]{s}{8,0};
      \newProdNode{p1}{$(s) + (-1.5,-1.0)$};
      \newProdNode{p2}{$(s) + (1.5,-1.0)$};
      \newProdNode{q1}{$(s) + (-1.5,-2.5)$};
      \newProdNode{q2}{$(s) + (-0,-2.5)$};
      \newProdNode{q3}{$(s) + (1.5,-2.5)$};
      \draw[edge] (s) edge (p1);
      \draw[edge] (s.south) -- ++(0,-1.25) -| node[midway,below right] {\scriptsize$w\cdot\theta_1$} (q1);
      \draw[edge] (s.south) -- ++(0,-1.25) -| node[midway,below right] {\scriptsize$w\cdot\theta_2$} (q2);
      \draw[edge] (s.south) -- ++(0,-1.25) -| node[midway,below right] {\scriptsize$w\cdot\theta_3$} (q3);
      \draw[edge] (s) edge (p2);
    \end{tikzpicture}
  \end{center}
  Connect $\Sum$ with every child of $\Sum'$, assigning as weight $w\cdot\theta_i$ for each child
  $i$. Delete $\Sum'$ and all edges coming out from it. The resulting circuit is computationally
  equivalent but now without a consecutive pair of sums. This transformation is visualized by the
  figure above. We do a similar procedure in (ii), but now instead remove $\Prod'$ and connect all
  children of $\Prod'$ to $\Prod$, as we show below.
  \begin{center}
    \begin{tikzpicture}
      \newProdNode[label=above right:{$\Prod$}]{s}{0,0};
      \newSumNode{p1}{$(s) + (-1.25,-1)$};
      \newProdNode[label=above left:{$\Prod'$}]{sl}{$(s) + (0,-1.5)$};
      \newSumNode{p2}{$(s) + (1.25,-1)$};
      \newSumNode{q1}{$(sl) + (-1.25,-1)$};
      \newSumNode{q2}{$(sl) + (0,-1)$};
      \newSumNode{q3}{$(sl) + (1.25,-1)$};
      \draw[edge] (s) edge (p1);
      \draw[edge] (s) edge (sl);
      \draw[edge] (s) edge (p2);
      \draw[edge] (sl) edge (q1);
      \draw[edge] (sl) edge (q2);
      \draw[edge] (sl) edge (q3);

      \draw[edge,very thick,red] (2.25,-1.5) -- node[midway,above] {Standardize} (5.5,-1.5);

      \newProdNode[label=above right:{$\Prod$}]{s}{8,0};
      \newSumNode{p1}{$(s) + (-1.5,-1.0)$};
      \newSumNode{p2}{$(s) + (1.5,-1.0)$};
      \newSumNode{q1}{$(s) + (-1.5,-2.5)$};
      \newSumNode{q2}{$(s) + (-0,-2.5)$};
      \newSumNode{q3}{$(s) + (1.5,-2.5)$};
      \draw[edge] (s) edge (p1);
      \draw[edge] (s.south) -- ++(0,-1.25) -| (q1);
      \draw[edge] (s.south) -- ++(0,-1.25) -| (q2);
      \draw[edge] (s.south) -- ++(0,-1.25) -| (q3);
      \draw[edge] (s) edge (p2);
    \end{tikzpicture}
  \end{center}
\end{proof}

\begin{theorem}[2-Standardization]
\label{thm:2standard}
Any probabilistic circuit $\mathcal{C}$ can be transformed into a circuit where every sum node
contains only products or inputs and every product node contains only \emph{two} sums or inputs.
\end{theorem}
\begin{proof}
  For sums, apply the same standardization procedure as \Cref{thm:standard}. Let $\Prod$ a product
  and call $n=|\Ch(\Prod)|$. If $n=1$ and $\Ch(\Prod)$ is a product, then remove $\Prod$ and
  connect all previous parents of $\Prod$ with its child. If $n=1$ and $\Ch(\Prod)$ is not a
  product, remove $\Prod$ and apply the standardization procedure for sums on all of $\Pa(\Prod)$.

  For $n>2$, we simply need to split into 2-products recursively. We prove this by induction. The
  base case is when $n=2$, which is already done, or $n=3$, in which case we need apply the
  transformation below.
  \begin{center}
    \begin{tikzpicture}
      \newProdNode{r}{0,0};
      \newSumNode{s1}{$(r) + (-1.5,-2)$};
      \newSumNode{s2}{$(r) + (0,-2)$};
      \newSumNode{s3}{$(r) + (1.5,-2)$};
      \draw[edge] (r) -- (s1);
      \draw[edge] (r) -- (s2);
      \draw[edge] (r) -- (s3);

      \draw[edge,very thick,red] (2.25,-0.5) -- node[midway,above] {2-Standardize} (5.5,-0.5);

      \newProdNode{r}{7,0};
      \newSumNode[fill=boxorange!80]{t}{$(r) + (-0.5,-1)$};
      \newSumNode{s1}{$(r) + (0.5,-1)$};
      \newProdNode[fill=boxgoldenrod!70]{p}{$(t) + (-0.5,-1)$};
      \newSumNode{s2}{$(p) + (-0.5,-1)$};
      \newSumNode{s3}{$(p) + (0.5,-1)$};
      \draw[edge] (r) -- (t);
      \draw[edge] (r) -- (s1);
      \draw[edge] (t) -- (p);
      \draw[edge] (p) -- (s2);
      \draw[edge] (p) -- (s3);
    \end{tikzpicture}
  \end{center}
  Where \inode[fill=boxorange!80]{\newSumNode} and \inode[fill=boxgoldenrod!70]{\newProdNode} are
  newly introduced nodes. When $n>3$, we create two products $\Prod_1$ and $\Prod_2$, each
  connected with a sum and product, and with $\left\lfloor\frac{n}{2}\right\rfloor$ and
  $\left\lceil\frac{n}{2}\right\rceil$ potential children. By the induction hypothesis, we can
  recursively binarize the subsequent grandchildren products.
  \begin{center}
    \begin{tikzpicture}
      \newProdNode{r}{0,0};
      \newSumNode{s1}{$(r) + (-2,-2)$};
      \newSumNode{s2}{$(r) + (-1,-2)$};
      \newSumNode{s3}{$(r) + (0,-2)$};
      \newSumNode{s4}{$(r) + (1,-2)$};
      \newSumNode{s5}{$(r) + (2,-2)$};
      \draw[edge] (r) -- (s1);
      \draw[edge] (r) -- (s2);
      \draw[edge] (r) -- (s3);
      \draw[edge] (r) -- (s4);
      \draw[edge] (r) -- (s5);

      \draw[edge,very thick,red] (2.25,-0.5) -- node[midway,above] {2-Standardize} (5.5,-0.5);

      \newProdNode{r}{7,0};
      \newSumNode[fill=boxorange!80]{t1}{$(r) + (-0.5,-1)$};
      \newSumNode[fill=boxorange!80]{t2}{$(r) + (0.5,-1)$};
      \newProdNode[fill=boxgoldenrod!70]{p1}{$(t1) + (-0.5,-1)$};
      \newProdNode[fill=boxgoldenrod!70]{p2}{$(t2) + (0.5,-1)$};
      \newSumNode{s1}{$(p1) + (-0.5,-1)$};
      \newSumNode{s2}{$(p1) + (0.5,-1)$};
      \newSumNode{s3}{$(p2) + (-0.75,-1)$};
      \newSumNode{s4}{$(p2) + (0.0,-1)$};
      \newSumNode{s5}{$(p2) + (0.75,-1)$};
      \draw[edge] (r) -- (t1);
      \draw[edge] (r) -- (t2);
      \draw[edge] (t1) -- (p1);
      \draw[edge] (t2) -- (p2);
      \draw[edge] (p1) -- (s1);
      \draw[edge] (p1) -- (s2);
      \draw[edge] (p2) -- (s3);
      \draw[edge] (p2) -- (s4);
      \draw[edge] (p2) -- (s5);
    \end{tikzpicture}
  \end{center}
  As an example, we have $n=5$ in the figure above. We introduce the sums
  \inode[fill=boxorange!80]{\newSumNode} and products \inode[fill=boxgoldenrod!70]{\newProdNode}
  and then recursively apply the transformation again on the
  \inode[fill=boxgoldenrod!70]{\newProdNode}s.

  When $\Ch(\Prod)$ are product nodes we do the same procedure as before, but with the added
  post-process addition of a sum node connecting \inode[fill=boxgoldenrod!70]{\newProdNode} to
  every $\Ch(\Prod)$.
\end{proof}

\linevi*
\begin{proof}
  \label{proof:linevi}
  For a sum node $\Sum$ and query variables $\set{X}$, we have the following marginalization of
  variables $\set{Y}$
  \begin{align*}
    \int\Sum(\set{x},\set{y})\dif\set{y}&=\int\sum_{\Child\in\Ch(\Sum)}w_{\Sum,\Child}\Child(\set{x},\set{y})\dif\set{y}\\
                                        &=\sum_{\Child\in\Ch(\Sum)}w_{\Sum,\Child}\int\Child(\set{x},\set{y})\dif\set{y}.
  \end{align*}
  Analogously, for a product node
  \begin{align*}
    \int\Prod(\set{x},\set{y})\dif\set{y}&=\int\prod_{\Child\in\Ch(\Prod)}\Child(\set{x},\set{y})\dif\set{y}\\
                                         &=\prod_{\Child\in\Ch(\Prod)}\int\Child(\set{x},\set{y})\dif\set{y}.
  \end{align*}
  This ensures that marginals are pushed down to children. This can be done recursively until
  $\Child$ is an input node $\Leaf_p$, in which case we marginalize $\set{y}$ according to $p$,
  which by definition should be tractable and here we assume can be done in $\bigo(1)$. We have
  proved the case for \mar{}. For \evi{}, we simply assign $\set{y}=\emptyset$ with input nodes
  acting as probability density functions. Conditionals can easily be computed by an \evi{} or
  \mar{} followed by a second pass marginalizing the conditional variables
  $p(\set{x}|\set{y})=\frac{p(\set{x},\set{y})}{p(\set{y})}$ which are both done in linear time as
  we have seen here.
\end{proof}

\det*
\begin{proof}
  \label{proof:det}
  For a sum node $\Sum$, we want to compute the following query
  \begin{equation*}
    \max_{\set{y}}\Sum(\set{y}|\set{x})=\frac{1}{\Sum(\set{x})}\max_{\set{y}}\Sum(\set{y},\set{x})
    =\frac{1}{\Sum(\set{x})}\max_{\set{y}}\sum_{\Child\in\Ch(\Sum)}w_{\Sum,\Child}\Child(\set{y},\set{x}),
  \end{equation*}
  yet notice that for any assignment of $\set{x}$ and $\set{y}$ only one $\Child\in\Ch(\Sum)$ must
  have a nonnegative value by the definition of determinism, so we may replace the summation with
  a maximization over the children, giving
  \begin{equation*}
    \max_{\set{y}}\Sum(\set{y}|\set{x})=\frac{1}{\Sum(\set{x})}\max_{\set{y}}\max_{\Child\in\Ch(\Sum)}w_{\Sum,\Child}\Child(\set{y},\set{x})
    =\frac{1}{\Sum(\set{x})}\max_{\Child\in\Ch(\Sum)}\max_{\set{y}}w_{\Sum,\Child}\Child(\set{y},\set{x}).
  \end{equation*}
  For a product node $\Prod$, we compute
  \begin{align*}
    \max_{\set{y}}\Prod(\set{y}|\set{x})=\frac{1}{\Prod(\set{x})}\max_{\set{y}}\Prod(\set{y},\set{x})
    =\frac{1}{\Prod(\set{x})}\max_{\set{y}}\prod_{\Child\in\Ch(\Prod)}\Child(\set{y},\set{x})
    =\frac{1}{\Prod(\set{x})}\prod_{\Child\in\Ch(\Prod)}\max_{\set{y}}\Child(\set{y},\set{x}).
  \end{align*}
  This is equivalent to an inductive top-down pass where we maximize instead of sum until we reach
  all input nodes, in which case we simply maximize the supposedly tractable functions. Once these
  are computed, we unroll the induction, maximizing over all values.
\end{proof}

\expcthm*
\begin{proof}
  For completeness, we show the proof of this claim as stated in \cite{choi20}. We assume, without
  loss of generality, that the layers of both $\mathcal{C}$ and $\mathcal{L}$ are compatible, i.e.\
  they both have the same number of layers and if the $i$-th layer of $\mathcal{C}$ is made out of
  sums (resp. products), then the $i$-th layer of $\mathcal{L}$ is made out of disjunctions (resp.
  conjunctions). The expectation $\mathbb{E}_\mathcal{C}\left[\mathcal{L}\right]$ has the following
  form when the root of $\mathcal{C}$ is a product
  \begin{align*}
    \mathbb{E}_\mathcal{C}\left[\mathcal{L}\right]&=\int\mathcal{C}(\set{x})\mathcal{L}(\set{x})\dif\set{x}=
    \int\left(\mathcal{C}_p(\set{x})\mathcal{C}_s(\set{x})\right)
    \left(\mathcal{L}_p(\set{x})\mathcal{L}_s(\set{x})\right)\dif\set{x}\\
    &=\int\left(\mathcal{C}_p(\set{x})\mathcal{L}_p(\set{x})\right)\left(\mathcal{C}_s(\set{x})\mathcal{L}_s(\set{x})\right)\dif\set{x}
    =\int\left(\mathcal{C}_p(\set{x})\mathcal{L}_p(\set{x})\right)\dif\set{x}
    \int\left(\mathcal{C}_s(\set{x})\mathcal{L}_s(\set{x})\right)\dif\set{x}\\
    &=\mathbb{E}_{\mathcal{C}_p}\left[\mathcal{L}_p\right]\cdot\mathbb{E}_{\mathcal{C}_s}\left[\mathcal{L}_s\right],
  \end{align*}
  where the subscript $p$ and $s$ indicate the prime and sub of a node. When the root is a sum
  \begin{align*}
    \mathbb{E}_\mathcal{C}\left[\mathcal{L}\right]&=\int\mathcal{C}(\set{x})\mathcal{L}(\set{x})\dif\set{x}
    =\int\left(\sum_{\Child'\in\Ch(\mathcal{C})}w_{\mathcal{C},\Child'}\Child'(\set{x})\right)
    \left(\sum_{\Child''\in\Ch(\mathcal{L})}\Child''(\set{x})\right)\dif\set{x}\\
    &=\int\sum_{\Child'\in\Ch(\mathcal{C})}\sum_{\Child''\in\Ch(\mathcal{L})}w_{\mathcal{C},\Child'}\cdot\Child'(\set{x})\cdot\Child''(\set{x})\dif\set{x}
    =\sum_{\Child'\in\Ch(\mathcal{C})}\sum_{\Child''\in\Ch(\mathcal{L})}w_{\mathcal{C},\Child'}\int\Child'(\set{x})\Child''(\set{x})\dif\set{x}\\
    &=\sum_{\Child'\in\Ch(\mathcal{C})}\sum_{\Child''\in\Ch(\mathcal{L})}w_{\mathcal{C},\Child'}\cdot\mathbb{E}_{\Child'}\left[\Child''\right].
  \end{align*}
  Therefore, if expectation is tractable for input nodes, then expectation is tractable for the
  whole circuit.
\end{proof}

\section{\textproc{SamplePSDD}}
\label{sec:samplepsdd-app}

We now transcribe the supplemental material found in \citet{geh21a}. In this section, we describe a
fast implementation of \textproc{SamplePSDD} that avoids the use of the costly Forget operation. We
then show additional experiments with \num{1000} iterations of \textproc{LearnPSDD} and
\textproc{Strudel}, followed by the complete log-likelihood results in table form. Finally, we list
in more detail the logical constraints used in each of the domains described in
\Cref{sec:logical-exp}.

\subsection{Fast implementation of \textsc{SamplePSDD}}

In order to produce valid (partial) partitions, \textsc{SamplePSDD} requires that the
\textsc{Forget} operation be called for every sub lest the scope of the formula contradicts the
respective scope in vtree. Despite \textsc{Forget} taking polynomial time in the size of the BDD,
we can make sampling more efficient by directly ``forgetting''  a variable when returning a PSDD
structure. \Cref{alg:fastsamplepsdd} modifies \Cref{alg:samplepsdd} to handle variables not
appearing in the formula $\phi$. Lines 2 -- 7 ensure that the formula correctly accounts for the
forgetting of variables not in the scope of the vtree. Hence, we can omit the \textsc{Forget}
operation in \Cref{alg:samplepartial}, resulting in \Cref{alg:samplepartial-fast}. Since the
restricion $\psi|X$ is linearithmic in the size of the BDD, and constructing a conjunction of
literals $\alpha$ is linear in $|\Sc(\alpha)|$, the algorithm is highly efficient.

\setcounter{algorithm}{20}
\begin{algorithm}
  \caption{\textproc{FastSamplePSDD}}\label{alg:fastsamplepsdd}
  \begin{algorithmic}[1]
    \Require BDD $\phi$, vtree node $v$, number of primes $k$
    \Ensure A sampled PSDD structure
    \If{$v$ is a leaf}
      \If{$v\in\phi$}
        \IIf{$\phi$ is a literal}{\textbf{return} $\phi$ as a literal node}
        \IIf{$\phi|_v\equiv\top$}{\textbf{return} $v$ as a literal node}
        \IIf{$\phi|_{\neg v}\equiv\top$}{\textbf{return} $\neg v$ as a literal node}
      %\Else \textbf{ return} a Bernoulli over $v$ 
      \EndIf
        \State \textbf{return} a Bernoulli over $v$
    \ElsIf{$\phi\equiv\top$}
      \State \textbf{return} a fully factorized circuit over $\Sc(v)$
    \EndIf
    \State $\set{E}\gets\Call{FastSamplePartialPartition}{\phi,\Sc(v^{\gets}),k}$
    \State Create an OR gate $\Sum$
    \State Randomly compress elements in $\set{E}$ with equal subs
    \State Randomly merge elements in $\set{E}$ with equal subs
    \For{each element $(p,s)\in\set{E}$}
      \State $l\gets\Call{SampleExactPSDD}{p,v^\gets,k}$
      \State $r\gets\Call{FastSamplePSDD}{s,v^\to,k}$
      \State Add an AND gate with inputs $l$ and $r$ as a child of $\Sum$
    \EndFor
    \State \textbf{return} $\Sum$
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
  \caption{\textproc{FastSamplePartialPartition}}\label{alg:samplepartial-fast}
  \begin{algorithmic}[1]
    \Require BDD $\phi$, vtree node $v$, number of primes $k$
    \Ensure A set of sampled elements
    \State Define $\set{E}$ as an empty collection of sampled elements
    \State Sample an ordering $X_1,\dotsc,X_m$ of $\Sc(v^\gets)\cap\Sc(\phi)$
    \State Let $\set{Q}$ be a queue initially containing $(\phi,1,\{\})$
    \State $j\gets 1$ \Comment{Counter of sampled elements}
    \While{$|\set{E}|<k$}
      \State Pop top item $(\psi,i,p)$ from $\set{Q}$
      \If{$j\geq k$ or $i>m$ or $\psi\equiv\top$}
%        \State Define $e\gets\left(\bigwedge_{X\in\set{V}}X,\Forget(\phi|_{\set{V}},\Sc(v^\gets))\right)$
 %       \State Add new element $e$ to $\set{E}$
        \State Add $\left(p,\phi|_{p}\right)$ to $\set{E}$
        \State \textbf{continue}
      \EndIf
      %\State Let $X$ be the $i$-th variable in $\set{O}$
      \State $\alpha\gets\psi|_{X_i}$, $\beta\gets\psi|_{\neg X_i}$
      \If{$\alpha \equiv \beta$} enqueue $(\psi,i+1,p)$ in $\set{Q}$
      \Else
        \IIf{$\alpha\not\equiv\bot$}{push $(\alpha,i+1,p \wedge X_i)$ to $\set{Q}$}
        \IIf{$\beta\not\equiv\bot$}{push $(\beta,i+1,p\wedge\neg X_i)$ to $\set{Q}$}
        \State $j\gets j+1$
      \EndIf
    \EndWhile
    \State \textbf{return} $\set{E}$
  \end{algorithmic}
\end{algorithm}

\subsection{Additional Experiments}

We repeat the accuracy vs.\  sample size plots including the results of running \textsc{Strudel}
and \textsc{MixStrudel} for 1000 iterations, as used in the original paper. \Cref{fig:performance}
shows all results with the added Strudel and MixStrudel with 1000 iterations curves.

\begin{figure*}
  \begin{subfigure}[b]{0.495\textwidth}
    \centering\includegraphics[width=\textwidth]{supplementary/all_ll_led_ensembles.pdf}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.495\textwidth}
    \centering\includegraphics[width=\textwidth]{supplementary/all_ll_led_pixels_ensembles.pdf}
    \caption{}
  \end{subfigure}\\ 
  \begin{subfigure}[b]{0.495\textwidth}
    \centering\includegraphics[width=\textwidth]{supplementary/all_ll_sushi_choose_ensembles.pdf}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.495\textwidth}
    \centering\includegraphics[width=\textwidth]{supplementary/all_ll_sushi_ranking_ensembles.pdf}
    \caption{}
  \end{subfigure}\\
  \begin{subfigure}[t]{0.495\textwidth}
    \centering\includegraphics[width=\textwidth]{supplementary/all_ll_dota_ensembles.pdf}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.495\textwidth}
    \centering\includegraphics[width=0.9\columnwidth]{supplementary/all_sushi_time.pdf}
    \caption{}
  \end{subfigure}
  \caption{\uncaption{(a)} Log-likelihoods for the unpixelized \texttt{led}, \uncaption{(b)}
    \texttt{led-pixels}, \uncaption{(c)} \texttt{sushi} $10$-choose-$5$, \uncaption{(d)}
    \texttt{sushi} ranking, and \uncaption{(e)} \texttt{dota} datasets. \uncaption{(f)} Mean
    average in seconds of each PSDD learning algorithm.}
  \label{fig:performance}
\end{figure*}

\subsection{Tables with all results}

Tables \ref{tab:all-led} through \ref{tab:all-led-pixels} show all log-likelihood values for all learned
circuits mentioned in the article.

\input{supplementary/all_tables}

\newpage

\subsection{Logic constraints}

We next show all the logic constraints used for each dataset.

\subsubsection{LED}

\begin{figure}
  \resizebox{\columnwidth}{!}{
  \begin{tikzpicture}
    \node at (0, 0) {\sevensegnum{4}};
    \node at (0, 1) {1};
    \node at (0.75, 0.3) {2};
    \node at (0.75, -0.3) {3};
    \node at (0, -1) {4};
    \node at (-0.75, -0.3) {5};
    \node at (-0.75, 0.3) {6};
    \node at (0, 0.25) {7};
    \node at (5.0, 0.25) {$d_4=\neg X_1\wedge X_2\wedge X_3\wedge\neg X_4\wedge\neg X_5\wedge X_6\wedge X_7$};
    \node at (2.525, -0.5) {$\phi=\bigvee_{i=0}^9 d_i$};
  \end{tikzpicture}
  }
  \caption{LED segment numbering (left), and the corresponding formula for that digit (right).}
\end{figure}

Let $Y_1,Y_2,\ldots,Y_7$ be the observable segments of a 7-segment LED display, with each $Y_i$
representing whether the $i$-th segment (read from the top segment clockwise with the middle
segment last) is observably on (true/1) or off (false/0). We assign a latent variable for each
segment $i$ and call it $X_i$. The latent variable indicates the true intent of the segment (i.e.
whether it was supposed to be on or off regardless of technical problems). For each digit $d_i$, we
add a positive literal if it is supposedly on, and a negative literal if it is supposedly off.
Observable variables are free variables with no constraints. The final formula is given by a
disjunction over all digits, as shown below.

\begin{align*}
\phi=
&\phantom{\neg}X_1 \wedge \phantom{\neg}X_2 \wedge \phantom{\neg}X_3 \wedge \phantom{\neg}X_4 \wedge \phantom{\neg}X_5 \wedge \phantom{\neg}X_6 \wedge \neg X_7 \vee\\
&\neg X_1 \wedge \phantom{\neg}X_2 \wedge \phantom{\neg}X_3 \wedge \neg X_4 \wedge \neg X_5 \wedge \neg X_6 \wedge \neg X_7 \vee\\
&\phantom{\neg}X_1 \wedge \phantom{\neg}X_2 \wedge \neg X_3 \wedge \phantom{\neg}X_4 \wedge \phantom{\neg}X_5 \wedge \neg X_6 \wedge \phantom{\neg}X_7 \vee\\
&\phantom{\neg}X_1 \wedge \phantom{\neg}X_2 \wedge \phantom{\neg}X_3 \wedge \phantom{\neg}X_4 \wedge \neg X_5 \wedge \neg X_6 \wedge \phantom{\neg}X_7 \vee\\
&\neg X_1 \wedge \phantom{\neg}X_2 \wedge \phantom{\neg}X_3 \wedge \neg X_4 \wedge \neg X_5 \wedge \phantom{\neg}X_6 \wedge \phantom{\neg}X_7 \vee\\
&\phantom{\neg}X_1 \wedge \neg X_2 \wedge \phantom{\neg}X_3 \wedge \phantom{\neg}X_4 \wedge \neg X_5 \wedge \phantom{\neg}X_6 \wedge \phantom{\neg}X_7 \vee\\
&\phantom{\neg}X_1 \wedge \neg X_2 \wedge \phantom{\neg}X_3 \wedge \phantom{\neg}X_4 \wedge \phantom{\neg}X_5 \wedge \phantom{\neg}X_6 \wedge \phantom{\neg}X_7 \vee\\
&\phantom{\neg}X_1 \wedge \phantom{\neg}X_2 \wedge \phantom{\neg}X_3 \wedge \neg X_4 \wedge \neg X_5 \wedge \neg X_6 \wedge \neg X_7 \vee\\
&\phantom{\neg}X_1 \wedge \phantom{\neg}X_2 \wedge \phantom{\neg}X_3 \wedge \phantom{\neg}X_4 \wedge \phantom{\neg}X_5 \wedge \phantom{\neg}X_6 \wedge \phantom{\neg}X_7 \vee\\
&\phantom{\neg}X_1 \wedge \phantom{\neg}X_2 \wedge \phantom{\neg}X_3 \wedge \phantom{\neg}X_4 \wedge \neg X_5 \wedge \phantom{\neg}X_6 \wedge \phantom{\neg}X_7
\end{align*}

\subsubsection{LED Pixels}

The LED with pixels dataset follows the same idea as LED, but with added pixels as latent variables
instead of observable segments. We manually observed critical key pixels for each segment (i.e.
pixels which are often set to true/1 if the segment is on. We count pixels row-wise from top left
to bottom right. The following are the critical key pixels for each segment:

\begin{align*}
  S_1&=\{24, 25, 26, 27, 15, 16, 28, 35, 36\}\\
  S_2&=\{27, 28, 37, 38, 47, 48, 57, 58, 49, 59, 69\}\\
  S_3&=\{77, 78, 87, 88, 109, 98, 99, 108, 118\}\\
  S_4&=\{124, 125, 126, 127, 128, 135, 136, 114, 115, 116\}\\
  S_5&=\{93, 94, 103, 104, 113, 114, 124, 82, 92, 83\}\\
  S_6&=\{33, 34, 43, 53, 52, 63, 73\}\\
  S_7&=\{64, 65, 66, 67, 75, 76, 85, 86, 95, 96, 94\}
\end{align*}

Each $S_i$ corresponds to the critical key pixels of segment $i$. The formula for the key pixels
is then set to
\begin{equation*}
  \alpha=\bigvee_{i=1}^7\left(\bigwedge_{p\in S_i}p\right)\wedge X_i.
\end{equation*}

We also add a constraint for pixels which are never on for a given digit. Let $f(i)$ a function
that maps a digit $i$ to the set of all pixels which are always off when $d_i$ is true. We set
\begin{equation*}
  \beta=\bigvee_{i=0}^9 d_i\wedge\left(\bigwedge_{p\in f(i)}\neg p\right).
\end{equation*}

The final constraint is then $\phi=\alpha\wedge\beta$.

\subsubsection{Sushi}

For the sushi ranking dataset, we used the same constraints as \citep{choi15}. For the sushi
10-choose-5, we used the same constraints as \citep{shen17}.

\subsubsection{Dota 2}

To model the constraints, we used a cardinality constraint of $\text{Exactly}(5, 113)$ for the
first and equivalently for the second team. To do this, each character $i$ had a pair of variables
$(X_i,Y_i)$, where $X_i$ attributed the character for the first team, and $Y_i$ to the second. A
cardinality constraint $\sum_{X_i}x_i=5$ was set to the first team, and $\sum_{Y_i}y_i=5$ to the
second.

