\chapter{Contributions, Discussion and Future Work}
\label{ch:conclusion}

In this chapter we conclude this dissertation and provide a brief discussion on the topics touched
throughout this work, highlighting our contributions and pointing to possible future work on
scalably learning probabilistic circuits.

\section{Contributions}

The objectives of this dissertation were two-fold: to provide a concise review of state-of-the-art
literature on the subject of structure learning of probabilistic circuits; and propose two ideas
for scalably learning the structure of PCs. For the former, we classified learning algorithms into
three classes (\Cref{ch:learning}): \emph{divide-and-conquer} (\divclass{}) learning, where we
recursively divide available knowledge (data or logical formula) into smaller partitions,
eventually joining them together (\Cref{sec:divconq}); \emph{incremental} learning (\incrclass{}),
where we iteratively grow a circuit through local transformations usually guided by a score
(\Cref{sec:incremental}); and finally \emph{random} learning, based around the concept of sampling
circuits either from knowledge or completely random (\Cref{sec:random}).

We follow this literature review and taxonomy on structure learning by addressing two cases of
\randclass{} algorithms, both of which draw inspiration from \divclass{} and \incrclass{}
(\Cref{ch:logical,ch:data}). Each tackles the problem of learning PCs from two distinct point of views:
we first propose \textproc{SamplePSDD} to learn a circuit from certain knowledge, constructing a
smooth, structure decomposable and deterministic PC (i.e.\ a PSDD) from both logical formula and
data. To scale up to the hundreds of variables we restrict the PSDD's support to only a relaxation
of the formula, showing that by aggregating several sampled circuits into an ensemble we achieve
competitive performance against the state-of-the-art (\Cref{ch:logical}). Next, we describe a very
simple \randclass{} structure learning algorithm based on random projections, which we call
\textproc{LearnRP}, for producing smooth and structure decomposable PCs solely from data. We show
that our approach is orders of magnitude faster compared to competitors, achieving relatively
competitive performance on binary data (\Cref{ch:data}).

\section{Discussion and Future Work}

Despite the interesting results reported in \Cref{ch:logical,ch:data}, there is much room for
improvement. We end this dissertation by addressing the flaws of both \textproc{SamplePSDD} and
\textproc{LearnRP}, pointing to their weaknesses and suggesting possible ideas for further work.

\subsection{\textproc{SamplePSDD}}

We now explore some interesting paths to take for further work on \textproc{SamplePSDD}. We
summarize them through the topics below, providing a short discussion on future work.

\paragraph{Smooth, structured decomposable and deterministic decompositions.} The exact partitions
constructed by decomposing formulae as conjunctions of literals and their restriction are, in a
sense, a generalization of Shannon's decomposition. As discussed in \Cref{ch:logical}, the main
drawback of this approach is the exponential number of terms required for completely encoding this
expansion. This kind of decomposition also limits primes to only conjunctions of literals, which
possibly hinders expressivity. Finding decompositions whose disjunction terms are smooth,
structured decomposable and deterministic would allow possibly more expressibly PSDDs to be
sampled.

\paragraph{Search-based sampling.} \textproc{SamplePSDD} blindly samples circuits from a logical
formula regardless of how well it models data. Guiding which variables are sampled as primes and
which primes are compressed or merged could provide a better data fit model.

\paragraph{Simultaneously learning the vtree.} Our proposed algorithm for \textproc{SamplePSDD} is
completely decoupled from the process of learning a vtree. Learning the vtree during sampling could
potentially provide a better fit to the overall model.

\subsection{\textproc{LearnRP}}

In this section, we provide a discussion on \textproc{LearnRP} and possible paths from both a
theoretical as well as a more practical point of view.

\paragraph{Extending the theoretical works from random projections.} As mentioned in \Cref{sec:rp},
there are several interesting theoretical results coming primarily from the decision tree
literature. Now that \citet{correia20} have made clear the connection between decision trees and
PCs, it would be interesting to understand whether the results from \citet{dasgupta08b} also extend
to more general PCs learned from random projections.

\paragraph{Enforcing determinism.} Random projection trees are naturally deterministic, however
circuits learned through \textproc{LearnRP} are not. This means \textproc{LearnRP} PCs are not as
interpretable as, say density estimation trees, where each assignment produces a clear path
``explaining'' the decisions taken by the model. We thus pose the following question: is it
possible to enforce determinism to \textproc{LearnRP} while at the same time retaining smoothness
and decomposability?

\paragraph{Simultaneously learning the vtree.} Similar to \textproc{SamplePSDD}, \textproc{LearnRP}
fixes a learned vtree and produces a PC from it. The choice of which variables to partition deeply
impacts how the random projections are sampled. Choosing a partitioning according to some score
could greatly enhance data fitness.

