\chapter{Probabilistic Circuits}
\label{ch:pc}

As we briefly mentioned in the last chapter, Probabilistic Circuits (PCs) are conceptualized as
computational graphs under special conditions. In this chapter, we formally define PCs and give an
intuition on their syntax, viewing other probabilistic models through the lenses of the PC
framework and formalizing the many structural constraints that give sufficient conditions for
tractable inference. We then address PCs as knowledge bases, representing logic formulae through
circuits.

Before we formally introduce probabilistic circuits, we start with a brief preliminary on notation
and nomenclature. Call $\set{X}=\{X_1,X_2,\ldots,X_m\}$ a set of random variables (RVs); we denote
by $\set{x}=\{x_1,x_2,\ldots,x_m\}$ the \emph{assignment} of each value $x_i$ to RV $X_i$. Let $p$
be a probability distribution over variables $\set{X}$; we use the notation $p(\set{X}=\set{x})$ to
represent the probability of $\set{X}$ taking values $\set{x}$ according to $p$. On that note, if
$\set{X}$ is the set of all RVs of probability distribution $p$, then we say that $\set{X}$ is the
\emph{scope} of $p$, here denoted by the functional $\Sc(p)$. When an assignment $\set{y}$ over RVs
$\set{Y}$ covers only a portion of $\Sc(p)=\set{X}$, or in other words $\set{Y}\subset\set{X}$,
then $\set{y}$ is called a \emph{partial assignment}; otherwise, if it captures the entire scope,
then it is said to be a \emph{complete assignment}.

We borrow a few concepts from graph theory and say that, for a graph
$\mathcal{G}=(\Nodes,\set{E})$, where $\Nodes$ is the set of nodes in $\mathcal{G}$ and $\set{E}$
the set of edges, the function $\Ch(\Node)$ maps a node $\Node\in\Nodes$ to the set of all children
of $\Node$, that is, all nodes which have an edge \emph{coming from} $\Node$. Similarly,
$\Pa(\Node)$ maps $\Node$ to its parents: the set of nodes which have an edge \emph{going to}
$\Node$. We assume that edge connections are unique, meaning that for two connected nodes $\Node_1$
and $\Node_2$, there can only exist a single edge connecting them, denoting
$\edge{\Node_1\Node_2}$ the edge coming from $\Node_1$ to $\Node_2$.

\section{Distributions as Computational Graphs}
\label{sec:pc}

Probabilistic circuits are computational graphs usually recursively defined in terms of their
computational units. We start with a broad definition of probabilistic circuits.

\begin{definition}[Probabilistic circuit]
  A \emph{probabilistic circuit} $\mathcal{C}$ is a rooted connected DAG whose nodes describe
  non-negative functions: a \emph{sum node} $\Sum$ represents a weighted summation over its
  children $\Sum(\set{x})=\sum_{\Child\in\Ch(\Sum)} w_{\Sum,\Child}\cdot\Child(\set{x})$, a
  \emph{product node} $\Prod$ multiplies all of its children
  $\Prod(\set{x})=\prod_{\Child\in\Ch(\Prod)} \Child(\set{x})$, and \emph{input nodes}, i.e.\ nodes
  with no outgoing edges, are defined as univariate probability density functions.\label{def:pc}
  The \emph{size} of $\mathcal{C}$ is the number of nodes and edges of its graph, denoted by
  $|\mathcal{C}|$.
\end{definition}

In its simplest form, a PC is a single input node annotated with an unnormalized probability
distribution over a single variable\footnote{Although we define inputs as univariate, weaker
definitions of PCs often permit inputs to be multivariate as well, provided mild conditions are
met. In general, there is not much loss of expressivity or generality in assuming only univariate
inputs, as multivariate inputs can often be represented as a PC instead.}. In practice, however,
inputs are typically portrayed as normalized parametric density (or mass) functions, and we shall
assume them as so throughout this dissertation unless explicitly stated otherwise. We also assume,
for purposes of simplifying analysis, that any query on an input node is computed in $\bigo(1)$ on
the size of the input. To simplify notation, from here on out we shall use the term distribution to
mean a probability density (or mass) function and argue that input nodes represent probability
distributions. The semantics of this shall be clear given the context.

\begin{figure}[t]
  \begin{subfigure}[t]{0.32\textwidth}
    \begin{tikzpicture}
      \pgfplotsset{
        every axis/.append style={
          axis line style={->},
          tick label style={font={\scriptsize\bfseries}},
          x tick label style={color=white,above right},
          y tick label style={color=white,above right},
          grid style={black,dashed},
        }
      }
      \begin{axis}[
        no markers, domain=-5:5, samples=35,
        height=2.75cm, width=\columnwidth,
        xtick={0.8}, ytick={0.29},
        xticklabels={\colorbox{boxblue}{\textbf{0.8}}},
        yticklabels={\colorbox{boxgreen}{\textbf{0.29}}},
        axis lines*=left, xlabel=$x$, ylabel=$p(x)$,
        every axis y label/.style={font=\scriptsize,at={(axis description cs:-0.1,0.9)},anchor=south},
        every axis x label/.style={font=\scriptsize,at=(current axis.right of origin),anchor=west},
        enlargelimits=false, clip=false, axis on top,
        grid = major
      ]
        \path[name path=axis] (axis cs:0,0) -- (axis cs:5,0);
        \addplot[very thick,boxteal,name path=g] {gauss(0,1)};
        \addplot[boxteal!40] fill between [of=g and axis];
      \end{axis}

      \node (px) at (0.525, -0.7) {$p(x)$};
      \newGaussNode{g}{$(px) + (1.25,0)$};
      \node (x) at ($(g) + (1,0)$) {$x$};
      \draw[boxdgray,edge] (g) -- (px);
      \draw[boxdgray,edge] (x) -- (g);

      \newGaussNode[label=below:$X$]{gv}{$(g) - (0,1.0)$};
      \node (pxv) at ($(gv) - (1.25,0)$) {\colorbox{boxgreen}{\color{white}$\mathbf{.29}$}};
      \node (xv) at ($(gv) + (1.25,0)$) {\colorbox{boxblue}{\color{white}$\mathbf{.80}$}};
      \draw[boxdgray,edge] (gv) -- (pxv);
      \draw[boxdgray,edge] (xv) -- (gv);
    \end{tikzpicture}
    \caption{}
    \label{fig:units-input}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newSumNode[fill=boxmunsel]{r}{0, 0};
      \node (c1) at ($(r) + (1.45,-1.5)$) {\scriptsize\colorbox{boxteal}{\color{white}$\mathbf{.75}$}};
      \node (c2) at ($(r) + (1.45,0)$) {\scriptsize\colorbox{boxorange}{\color{white}$\mathbf{.65}$}};
      \node (c3) at ($(r) + (1.45,1.5)$) {\scriptsize\colorbox{boxpurple}{\color{white}$\mathbf{.35}$}};
      \draw[edge] (r) edge[bend right=5] node[below,pos=0.3,yshift=-0.2cm] {\scriptsize$.25$} (c1);
      \draw[edge] (r) edge[bend right=5] node[above,pos=0.5,yshift=0.1cm] {\scriptsize$.30$} (c2);
      \draw[edge] (r) edge[bend right=5] node[above,pos=0.3,yshift=0.3cm] {\scriptsize$.45$} (c3);
      \draw[edge,boxdgray] (c1) edge[bend left=-5] (r);
      \draw[edge,boxdgray] (c2) edge[bend left=-5] (r);
      \draw[edge,boxdgray] (c3) edge[bend left=-5] (r);
      \node (t) at ($(r) + (-1.0,0)$) {\scriptsize\colorbox{boxgreen}{\color{white}$\mathbf{.54}$}};
      \draw[edge,boxdgray] (r) -- (t);
      \node (b) at ($(c2) + (1.35,0)$) {\scriptsize\colorbox{boxblue}{\color{white}$\set{x}$}};
      \draw[edge,boxdgray] (b) -- (c1);
      \draw[edge,boxdgray] (b) -- (c2);
      \draw[edge,boxdgray] (b) -- (c3);
    \end{tikzpicture}
    }
    \caption{}
    \label{fig:units-sum}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newProdNode[fill=boxred!70]{r}{0, 0};
      \node (c1) at ($(r) + (1.45,-1.5)$) {\scriptsize\colorbox{boxteal}{\color{white}$\mathbf{.40}$}};
      \node (c2) at ($(r) + (1.45,0)$) {\scriptsize\colorbox{boxorange}{\color{white}$\mathbf{.80}$}};
      \node (c3) at ($(r) + (1.45,1.5)$) {\scriptsize\colorbox{boxpurple}{\color{white}$\mathbf{.50}$}};
      \draw[edge] (r) edge[bend right=5] (c1);
      \draw[edge] (r) edge[bend right=5] (c2);
      \draw[edge] (r) edge[bend right=5] (c3);
      \draw[edge,boxdgray] (c1) edge[bend left=-5] (r);
      \draw[edge,boxdgray] (c2) edge[bend left=-5] (r);
      \draw[edge,boxdgray] (c3) edge[bend left=-5] (r);
      \node (t) at ($(r) + (-1.0,0)$) {\scriptsize\colorbox{boxgreen}{\color{white}$\mathbf{.16}$}};
      \draw[edge,boxdgray] (r) -- (t);
      \node (b) at ($(c2) + (1.35,0)$) {\scriptsize\colorbox{boxblue}{\color{white}$\set{x}$}};
      \draw[edge,boxdgray] (b) -- (c1);
      \draw[edge,boxdgray] (b) -- (c2);
      \draw[edge,boxdgray] (b) -- (c3);
    \end{tikzpicture}
    }
    \caption{}
    \label{fig:units-prod}
  \end{subfigure}
  \caption{An input node as a Gaussian distribution \uncaption{(a)}, a sum node \uncaption{(b)},
    and a product node \uncaption{(c)}, the last two with three children each. Arrows in black
    signal (possibly weighted) edges in the computational graph, while gray edges indicate the
    computational flow: given an assignment \colorbox{boxblue}{\color{white}$\set{x}$}, the
    computation flows the opposite direction, starting on inputs and going up to the root,
    resulting in the final value in \colorbox{boxgreen}{\color{white}\textup{green}}.}
  \label{fig:units}
\end{figure}

Let $\Leaf$ be a PC input node representing a distribution $p$ whose scope is over variable $X$.
For an assignment $x$ of variable $X$, we use the notation $\Leaf(X=x)$ to mean $p(X=x)$, often
omitting $X$ when meaning is clear from context. We say that the \emph{value} of input node $\Leaf$
with respect to some query is the value of $p$ with respect to that same query. As an example,
\Cref{fig:units-input} shows the case when $\Leaf$ represents a Gaussian. If we were to compute the
probability of $x=0.29$ according to $\Leaf$, then we would have to perform that same query on
$\Leaf$'s distribution $p$, in this case $p(x=0.29)=0.80$. Similarly, the values of inner nodes
with respect to a query correspond to the resulting values of their functions given an assignment
(and the query). \Cref{fig:units-sum} shows a sum node $\Sum$ taking value
$\Sum(\set{x})=0.45\cdot0.35+0.30\cdot 0.65+0.25\cdot 0.75=0.54$, while \Cref{fig:units-prod} shows
the same, but for a product $\Prod(\set{x})=0.50\cdot 0.80\cdot 0.40=0.16$. This concept of value
of a node shall become clearer when we talk about reasoning in \Cref{sec:inf} and how to compute
queries in PCs. For now, we assume that this value is simply the result of the computation of the
function it represents given an assignment and a query.

As previously mentioned, an inner node indicates an operation to be computed from the value of its
children. Although \Cref{def:pc} only references sums and products, the subject of more general
operations and their benefits in terms of inference is an interesting question which we briefly
touch in \Cref{rem:optract}, but otherwise remains out of the scope of this work. In this
dissertation, we restrict ourselves to the study of sums and products as inner nodes. More
precisely on the subject of sum nodes, we are interested in sums whose weights are non-negative and
sum to one, or in other words, nodes which correspond to convex combinations of their children

Semantically, the root of a PC represents an unnormalized probability distribution composed out of
the functions of its descendant computational units. When all sums in a PC are convex combinations
and every one of its inputs are normalized probability distributions, then the encompassing PC is
also normalized, meaning that the distribution it represents is normalized. \textbf{Throughout this
dissertation, we shall assume sums to be convex combinations and inputs to be
normalized\footnote{This assumption incurs in no loss of generality, as \cite{peharz15} show.}.}
Locally, sum nodes have the semantic interpretation of a mixture model over its children,
essentially acting as a latent variable over the component distributions \citep{poon11,peharz16}.
\Cref{eg:mixgauss} shows a case of a Gaussian mixture model as a PC, that is, a single sum node
over Gaussian distributions as inputs.

\newcommand\mone{1}%
\newcommand\sone{0.65}%
\newcommand\mtwo{2.5}%
\newcommand\stwo{0.85}%
\newcommand\mthr{4}%
\newcommand\sthr{0.6}%
\begin{example}[sidebyside,lefthand width=0.55\textwidth]{Gaussian mixture models as probabilistic circuits}{mixgauss}
  A Gaussian Mixture Model (GMM) defines a mixture over Gaussian components. Say we wish to compute
  the probability of $X=x$ for a GMM $\mathcal{G}$ with three components
  $\gaussian_1(\mu_1=\mone,\sigma_1=\sone)$, $\gaussian_2(\mu_2=\mtwo, \sigma_1=\stwo)$ and
  $\gaussian_3(\mu_3=\mthr,\sigma^2_3=\sthr)$, and suppose we have weights set to
  $\phi=(0.4,0.25,0.35)$. Computing the probability of $x$ according to $\mathcal{G}$ amounts to
  the weighted summation
  \begin{align*}
    \mathcal{G}(X=x)=&0.4\cdot\gaussian_1(x;\mu_1,\sigma_2)+0.25\cdot\gaussian_2(x;\mu_2,\sigma_2)+\\
                     &0.35\cdot\gaussian_3(x;\mu_3,\sigma_3),
  \end{align*}
  which is equivalent to a computational graph (i.e. a PC) with a sum node as root and whose
  weights are set to $\phi$ and children to the components of the mixture. The figure on the right
  shows $\mathcal{G}$ (top) and its corresponding PC (middle). Given $x=1.5$ (in blue), input nodes
  are computed following the computation flow (bottom, gray edges) up to the root sum node
  \inode[fill=boxred!70]{\newSumNode}, where a weighted summation is computed to output the
  probability (in green).

  \tcblower
  \begin{center}
    \begin{tikzpicture}
      \pgfplotsset{
        every axis/.append style={
          axis line style={->},
          tick label style={font={\scriptsize\bfseries}},
          x tick label style={color=white,below},
          y tick label style={color=white,left},
          grid style={black,dashed},
        }
      }
      \begin{axis}[
        no markers, domain=-1:6, samples=35,
        height=3.75cm, width=\columnwidth,
        xtick={1.5}, ytick={0.2414},
        xticklabels={\colorbox{boxblue}{\textbf{1.5}}},
        yticklabels={\colorbox{boxgreen}{\textbf{0.24}}},
        axis lines*=left, xlabel=$x$, ylabel=$p(x)$,
        every axis y label/.style={font=\scriptsize,at={(axis description cs:-0.1,0.9)},anchor=south},
        every axis x label/.style={font=\scriptsize,at=(current axis.right of origin),anchor=west},
        enlargelimits=false, clip=false, axis on top,
        grid = major
      ]
        \path[name path=axis] (axis cs:0,0) -- (axis cs:5,0);
        \addplot[very thick,boxteal,name path=g1] {gauss(\mone,\sone)};
        \addplot[very thick,boxorange,name path=g2] {gauss(\mtwo,\stwo)};
        \addplot[very thick,boxpurple,name path=g3] {gauss(\mthr,\sthr)};
        \addplot[very thick,boxred] {mixgauss3(\mone,\sone,\mtwo,\stwo,\mthr,\sthr,0.4,0.25,0.35)};
        \addplot[boxteal!60] fill between [of=g1 and axis];
        \addplot[boxorange!50] fill between [of=g2 and axis];
        \addplot[boxpurple!40] fill between [of=g3 and axis];
        \node at (\mone, 0.7) {\tiny$\mu_1=\mone$};
        \node at (\mtwo, 0.5) {\tiny$\mu_2=\mtwo$};
        \node at (\mthr, 0.75) {\tiny$\mu_3=\mthr$};
      \end{axis}
    \end{tikzpicture}

    \begin{tikzpicture}
      \newSumNode[fill=boxred!70]{s}{0,0};
      \newGaussNode[fill=boxteal]{g1}{$(s) + (-1.5,-1)$};
      \newGaussNode[fill=boxorange!80]{g2}{$(s) + (0,-1)$};
      \newGaussNode[fill=boxpurple!60]{g3}{$(s) + (1.5,-1)$};
      \draw[edge] (s) edge (g1);
      \draw[edge] (s) edge (g2);
      \draw[edge] (s) edge (g3);
      \node at ($(s) + (-1.2,-0.35)$) {\scriptsize$.40$};
      \node at ($(s) + (-0.3,-0.5)$) {\scriptsize$.25$};
      \node at ($(s) + (1,-0.35)$) {\scriptsize$.35$};
      \node (l1) at ($(g1) + (0,-0.5)$) {\scriptsize$\gaussian_1(\mone,\sone)$};
      \node (l2) at ($(g2) + (0,-0.5)$) {\scriptsize$\gaussian_2(\mtwo,\stwo)$};
      \node (l3) at ($(g3) + (0,-0.5)$) {\scriptsize$\gaussian_3(\mthr,\sthr)$};
    \end{tikzpicture}

    \begin{tikzpicture}
      \newSumNode[fill=boxred!70]{s}{0,0};
      \newGaussNode[fill=boxteal]{g1}{$(s) + (-1.5,-1)$};
      \newGaussNode[fill=boxorange!80]{g2}{$(s) + (0,-1)$};
      \newGaussNode[fill=boxpurple!60]{g3}{$(s) + (1.5,-1)$};
      \draw[edge] (s) edge[bend right=5] (g1);
      \draw[edge] (s) edge[bend right=5] (g2);
      \draw[edge] (s) edge[bend right=5] (g3);
      \node at ($(s) + (-1.2,-0.35)$) {\scriptsize$.40$};
      \node at ($(s) + (-0.3,-0.5)$) {\scriptsize$.25$};
      \node at ($(s) + (1,-0.35)$) {\scriptsize$.35$};
      \node (inp) at ($(g2) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathbf{1.5}$}};
      \node (out) at ($(s) + (0,1.0)$) {\scriptsize\colorbox{boxgreen}{\color{white}$\mathbf{0.24}$}};
      \draw[edge,boxdgray] (s) -- (out);
      \node (l1) at ($(g1) + (0,-0.5)$) {\scriptsize\colorbox{boxteal}{\color{white}$\mathbf{0.45}$}};
      \node (l2) at ($(g2) + (0,-0.5)$) {\scriptsize\colorbox{boxorange}{\color{white}$\mathbf{0.23}$}};
      \node (l3) at ($(g3) + (0,-0.5)$) {\scriptsize\colorbox{boxpurple!80}{\color{white}$\mathbf{0.00}$}};
      \draw[edge,boxdgray] (inp) edge (l1);
      \draw[edge,boxdgray] (inp) edge (l2);
      \draw[edge,boxdgray] (inp) edge (l3);
      \draw[edge,boxdgray] (g1) edge[bend left=-5] (s);
      \draw[edge,boxdgray] (g2) edge[bend left=-5] (s);
      \draw[edge,boxdgray] (g3) edge[bend left=-5] (s);
    \end{tikzpicture}
  \end{center}
\end{example}

With the understanding that a PC determines a probability distribution, we may now extend the
notion of \emph{scope} to PCs, discoverable in an inductive fashion: the scope of an input node is
the scope of its distribution, while the scope of an inner node $\Node$ is the union of the scopes
of its children $\Sc(\Node)=\bigcup_{\Child\in\Ch(\Node)}\Sc(\Child)$. As an example, take the
circuit from \Cref{eg:factors}. The scope of input nodes \inode[fill=boxteal]{\newGaussNode} and
\inode[fill=boxorange!80]{\newGaussNode} are
$\Sc(\inode[fill=boxteal]{\newGaussNode})=\Sc(\inode[fill=boxorange!80]{\newGaussNode})=\{X\}$,
while $\Sc(\inode[fill=boxpink!50]{\newGaussNode})=\Sc(\inode[fill=boxgoldenrod!70]{\newGaussNode})
=\{Y\}$. Consequentially, their parent sum nodes will have the same scope as their children
$\Sc(\inode[fill=boxred!70]{\newSumNode})=\{X\}$ and $\Sc(\inode[fill=boxpurple!60]{\newSumNode})=
\{Y\}$, yet the root node's scope is $\Sc(\inode[fill=boxgreen]{\newProdNode})=\{X,Y\}$, since its
childrens' scopes are distinct. The notion of scope is essential to many structural properties in
PCs, many of which provide sufficient conditions for tractably computing a wide range of inference
queries on probabilistic circuits as feedforward passes on the computational graph, as we detail in
\Cref{sec:inf}.

When computing the value of a node $\Node$, the domain of the function it computes
$\Node(\set{X}=\set{x})$ is restricted to the node's scope $\Sc(\Node)=\set{X}$. For instance, in
the case of \Cref{eg:factors}, the domain of \inode[fill=boxgreen]{\newProdNode} is over $X$ and
$Y$, while \inode[fill=boxred!70]{\newSumNode} and \inode[fill=boxpurple!60]{\newSumNode} are only
over $X$ and $Y$ respectively. Having said that, for simplification purposes we often abuse
notation and assume that, given an assignment $\set{x}$ of RVs $\set{X}$, the application of
$\Node(\set{x})$ is strictly over the variables in $\Sc(\Node)$, meaning that the assignments of
any RVs outside of $\Node$'s scope are simply ignored.

\newcommand\xmone{2}%
\newcommand\xsone{0.5}%
\newcommand\xmtwo{4}%
\newcommand\xstwo{0.8}%
\newcommand\ymone{3}%
\newcommand\ysone{0.7}%
\newcommand\ymtwo{5}%
\newcommand\ystwo{0.4}%
\begin{example}[sidebyside,lefthand width=0.55\textwidth]{Factors as probabilistic circuits}{factors}
  Say we have two GMMs $\mathcal{G}_1$ and $\mathcal{G}_2$. The first is a mixture model over
  variable $X$, with component weights $\phi_1=(0.3,0.7)$ and gaussians
  $\gaussian_1(\mu_1=\xmone,\sigma_1=\xsone)$ and $\gaussian_2(\mu_2=\xmtwo,\sigma_2=\xstwo)$. The
  second is composed of $\gaussian_3(\mu_3=\ymone,\sigma_2=\ysone)$ and $\gaussian_4(\mu_4=\ymtwo,
  \sigma_2=\ystwo)$, both distributions over variable $Y$ and with weights $\phi_2=(0.6,0.4)$.

  Suppose $X\indep Y$, and we wish to compute the joint probability of both $x$ and $y$. If
  $X\indep Y$, then $p(x,y)=p(x)p(y)=\mathcal{G}_1(x)\mathcal{G}_2(y)$, which corresponds to a
  factoring of mixtures. This is represented as a product node \inode[fill=boxgreen]{\newProdNode}
  over the two mixture models \inode[fill=boxred!70]{\newSumNode} and
  \inode[fill=boxpurple!60]{\newSumNode}. The resulting joint of this circuit is shown below.
\begin{tikzpicture}
  \pgfplotsset{
    every axis/.append style={
      axis line style={->},
      axis lines=center,
      grid style={black,dashed},
      x tick label style={color=white,below},
      y tick label style={color=white,right},
      z tick label style={color=white,left},
    }
  }
  \begin{axis}[
    no markers, width=0.9\columnwidth,
    xtick={2}, ytick={4}, ztick={0.035},
    xticklabels={\colorbox{boxblue}{\textbf{2}}},
    yticklabels={\colorbox{boxblue}{\textbf{4}}},
    zticklabels={\colorbox{boxgreen}{\textbf{0.035}}},
    xlabel=$x$, ylabel=$y$, zlabel={$p(x,y)$},
    axis lines*=left,
    xlabel style={anchor=north west},
    ylabel style={anchor=south west},
    zlabel style={anchor=south west},
    enlargelimits=false, clip=false, axis on top,
    grid = major
  ]
    \addplot3[
      surf, samples=30,
      domain=0.5:6.5,
      y domain=1:6
    ] {((0.3*exp(-((x-\xmone)^2)/(2*\xsone^2))/\xsone+0.7*exp(-((x-\xmtwo)^2)/(2*\xstwo^2))/\xstwo)/2.5066)*(((0.6*exp(-((y-\ymone)^2)/(2*\ysone^2))/\ysone+0.4*exp(-((y-\ymtwo)^2)/(2*\ystwo^2))/\ystwo)/2.5066))};
  \end{axis}
\end{tikzpicture}

  \tcblower
  \begin{center}
    \begin{tikzpicture}
      \pgfplotsset{
        every axis/.append style={
          axis line style={->},
          tick label style={font={\scriptsize\bfseries}},
          x tick label style={color=white,below},
          y tick label style={color=white,left},
          grid style={black,dashed},
        }
      }
      \begin{axis}[
        no markers, domain=0:7, samples=35,
        height=3.0cm, width=\columnwidth,
        xtick={2}, ytick={0.25},
        xticklabels={\colorbox{boxblue}{\textbf{2}}},
        yticklabels={\colorbox{boxred}{\textbf{0.25}}},
        axis lines*=left, xlabel=$x$, ylabel=$p(x)$,
        every axis y label/.style={font=\scriptsize,at={(axis description cs:-0.1,0.9)},anchor=south},
        every axis x label/.style={font=\scriptsize,at=(current axis.right of origin),anchor=west},
        enlargelimits=false, clip=false, axis on top,
        grid = major
      ]
        \path[name path=axis] (axis cs:0,0) -- (axis cs:7,0);
        \addplot[very thick,boxteal,name path=g1] {gauss(\xmone,\xsone)};
        \addplot[very thick,boxorange,name path=g2] {gauss(\xmtwo,\xstwo)};
        \addplot[very thick,boxred] {mixgauss2(\xmone,\xsone,\xmtwo,\xstwo,0.3,0.7)};
        \addplot[boxteal!60] fill between [of=g1 and axis];
        \addplot[boxorange!50] fill between [of=g2 and axis];
        \node at (axis cs:\xmone,{egauss(\xmone,\xsone,\xmone)+0.1}) {\tiny$\mu_1=\xmone$};
        \node at (axis cs:\xmtwo,{egauss(\xmtwo,\xstwo,\xmtwo)+0.1}) {\tiny$\mu_2=\xmtwo$};
      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}
      \pgfplotsset{
        every axis/.append style={
          axis line style={->},
          tick label style={font={\scriptsize\bfseries}},
          x tick label style={color=white,below},
          y tick label style={color=white,left},
          grid style={black,dashed},
        }
      }
      \begin{axis}[
        no markers, domain=1:7, samples=50,
        height=3.0cm, width=\columnwidth,
        xtick={4}, ytick={0.14},
        xticklabels={\colorbox{boxblue}{\textbf{4}}},
        yticklabels={\colorbox{boxpurple}{\textbf{0.14}}},
        axis lines*=left, xlabel=$y$, ylabel=$p(y)$,
        every axis y label/.style={font=\scriptsize,at={(axis description cs:-0.1,0.9)},anchor=south},
        every axis x label/.style={font=\scriptsize,at=(current axis.right of origin),anchor=west},
        enlargelimits=false, clip=false, axis on top,
        grid = major
      ]
        \path[name path=axis] (axis cs:0,0) -- (axis cs:7,0);
        \addplot[very thick,boxpink,name path=g1] {gauss(\ymone,\ysone)};
        \addplot[very thick,boxgoldenrod,name path=g2] {gauss(\ymtwo,\ystwo)};
        \addplot[very thick,boxpurple] {mixgauss2(\ymone,\ysone,\ymtwo,\ystwo,0.6,0.4)};
        \addplot[boxpink!40] fill between [of=g1 and axis];
        \addplot[boxgoldenrod!50] fill between [of=g2 and axis];
        \node at (axis cs:\ymone,{egauss(\ymone,\ysone,\ymone)+0.1}) {\tiny$\mu_3=\ymone$};
        \node at (axis cs:\ymtwo,{egauss(\ymtwo,\ystwo,\ymtwo)+0.1}) {\tiny$\mu_4=\ymtwo$};
      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}
      \newProdNode[fill=boxgreen]{r}{0,0};
      \newSumNode[fill=boxred!70]{p}{$(r) + (-1.25,-0.75)$};
      \newSumNode[fill=boxpurple!60]{q}{$(r) + (1.25,-0.75)$};
      \newGaussNode[fill=boxteal]{x1}{$(p) + (-0.6,-1)$};
      \newGaussNode[fill=boxorange!80]{x2}{$(p) + (0.65,-1)$};
      \newGaussNode[fill=boxpink!50]{y1}{$(q) + (-0.6,-1)$};
      \newGaussNode[fill=boxgoldenrod!70]{y2}{$(q) + (0.6,-1)$};
      \draw[edge] (r) edge (p);
      \draw[edge] (r) edge (q);
      \draw[edge] (p) edge (x1);
      \draw[edge] (p) edge (x2);
      \draw[edge] (q) edge (y1);
      \draw[edge] (q) edge (y2);
      \node at ($(p) + (-0.5,-0.4)$) {\scriptsize$.3$};
      \node at ($(p) + (0.5,-0.4)$) {\scriptsize$.7$};
      \node at ($(q) + (-0.5,-0.4)$) {\scriptsize$.6$};
      \node at ($(q) + (0.5,-0.4)$) {\scriptsize$.4$};
      \node (l1) at ($(x1) + (0,-0.5)$) {\scriptsize$\gaussian_1(\xmone,\xsone)$};
      \node (l2) at ($(x2) + (0,-0.5)$) {\scriptsize$\gaussian_2(\xmtwo,\xstwo)$};
      \node (l3) at ($(y1) + (0,-0.5)$) {\scriptsize$\gaussian_3(\ymone,\ysone)$};
      \node (l4) at ($(y2) + (0,-0.5)$) {\scriptsize$\gaussian_4(\ymtwo,\ystwo)$};
    \end{tikzpicture}
    \begin{tikzpicture}
      \newProdNode[fill=boxgreen]{r}{0,0};
      \newSumNode[fill=boxred!70]{p}{$(r) + (-1.25,-0.75)$};
      \newSumNode[fill=boxpurple!60]{q}{$(r) + (1.25,-0.75)$};
      \newGaussNode[fill=boxteal]{x1}{$(p) + (-0.6,-1)$};
      \newGaussNode[fill=boxorange!80]{x2}{$(p) + (0.65,-1)$};
      \newGaussNode[fill=boxpink!50]{y1}{$(q) + (-0.6,-1)$};
      \newGaussNode[fill=boxgoldenrod!70]{y2}{$(q) + (0.6,-1)$};
      \draw[edge] (r) edge[bend right=5] (p);
      \draw[edge] (r) edge[bend right=5] (q);
      \draw[edge] (p) edge[bend right=5] (x1);
      \draw[edge] (p) edge[bend right=5] (x2);
      \draw[edge] (q) edge[bend right=5] (y1);
      \draw[edge] (q) edge[bend right=5] (y2);
      \node at ($(p) + (-0.5,-0.4)$) {\scriptsize$.3$};
      \node at ($(p) + (0.5,-0.4)$) {\scriptsize$.7$};
      \node at ($(q) + (-0.5,-0.4)$) {\scriptsize$.6$};
      \node at ($(q) + (0.5,-0.4)$) {\scriptsize$.4$};
      \node (l1) at ($(x1) + (0,-0.5)$) {\scriptsize\colorbox{boxteal}{\color{white}$\mathbf{0.80}$}};
      \node (l2) at ($(x2) + (0,-0.5)$) {\scriptsize\colorbox{boxorange}{\color{white}$\mathbf{0.02}$}};
      \node (l3) at ($(y1) + (0,-0.5)$) {\scriptsize\colorbox{boxpink}{\color{white}$\mathbf{0.20}$}};
      \node (l4) at ($(y2) + (0,-0.5)$) {\scriptsize\colorbox{boxgoldenrod}{\color{white}$\mathbf{0.04}$}};
      \draw[edge,boxdgray] (p) edge[bend left=-5] (r);
      \draw[edge,boxdgray] (q) edge[bend left=-5] (r);
      \draw[edge,boxdgray] (x1) edge[bend left=-5] (p);
      \draw[edge,boxdgray] (x2) edge[bend left=-5] (p);
      \draw[edge,boxdgray] (y1) edge[bend left=-5] (q);
      \draw[edge,boxdgray] (y2) edge[bend left=-5] (q);
      \node at ($(p) + (-0.3,0.6)$) {\scriptsize\colorbox{boxred}{\color{white}$\mathbf{0.25}$}};
      \node at ($(q) + (0.3,0.6)$) {\scriptsize\colorbox{boxpurple}{\color{white}$\mathbf{0.14}$}};
      \node (x) at ($(p) + (0,-2.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut\mathbf{x=2}$}};
      \node (y) at ($(q) + (0,-2.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut\mathbf{y=4}$}};
      \draw[edge,boxdgray] (x) edge (l1);
      \draw[edge,boxdgray] (x) edge (l2);
      \draw[edge,boxdgray] (y) edge (l3);
      \draw[edge,boxdgray] (y) edge (l4);
      \node (out) at ($(r) + (0,0.9)$) {\scriptsize\colorbox{boxgreen}{\color{white}$\mathbf{0.035}$}};
      \draw[edge,boxdgray] (r) edge (out);
    \end{tikzpicture}
  \end{center}
\end{example}

Before we address reasoning in probabilistic circuits, we first describe indicator nodes,
subcircuits, circuit size, induced subcircuits, and standard circuits, all of which are basic
concepts of PCs that we shall need later throughout the text. We start with a special case of an
input node. An \emph{indicator node} is an input node whose distribution is the indicator function
$f(x)=\liv x=k\riv$, i.e. a degenerate distribution with all of its mass on $k$ and zero anywhere
else. A special case is when $X$ is binary and $k=1$, in which case we say the input node is a
literal node, denoting by the usual propositional notation $X$ for when $k=1$ and $\neg X$ for
$k=0$. Graphically, we shall use either \inode{\newLeafNode} or just the textual Iverson bracket
$\liv\cdot\riv$ representation for indicators, while literals will be denoted by their textual
propositional notation.

Let $\mathcal{C}$ be a probabilistic circuit and consider a node $\Node\in\mathcal{C}$. We say that
$\Node_\mathcal{C}$ is a subcircuit of $\mathcal{C}$ rooted at $\Node$ if $\Node_\mathcal{C}$'s
root is $\Node$, all nodes and edges in $\mathcal{C}_{\Node}$ are also in $\mathcal{C}$ and
$\mathcal{C}_{\Node}$ is also a probabilistic circuit.

\begin{definition}[Induced subcircuit]\label{def:inducedsub}
  Let $\mathcal{C}$ be a probabilistic circuit. An \emph{induced subcircuit} $\mathcal{S}$ of
  $\mathcal{C}$ is a subcircuit of $\mathcal{C}$ rooted at $\mathcal{C}$'s root such that all edges
  coming out of product nodes in $\mathcal{C}$ are also in $\mathcal{S}$, and for each sum node,
  out of all edges coming out of it, only one is in $\mathcal{S}$.
\end{definition}

Examples of induced subcircuits are visualized in \Cref{fig:induced}. When the induced subcircuit
is tree-shaped, as is the case in \Cref{fig:induced}, they are referred to as induced trees
\citep{zhao15,zhao16b}.

\begin{figure}[t]
  \begin{subfigure}[t]{0.245\textwidth}
    \begin{center}
      \resizebox{\textwidth}{!}{
      \begin{tikzpicture}
        \newSumNode[fill=boxgreen]{r}{0,0};
        \newProdNode[fill=boxred!70]{p1}{$(r) + (-1.5,-1)$};
        \newProdNode[fill=boxred!70]{p2}{$(r) + (0,-1)$};
        \newProdNode[fill=boxred!70]{p3}{$(r) + (1.5,-1)$};
        \newSumNode[fill=boxpurple!60]{s1}{$(p2) + (-2.0,-1)$};
        \newSumNode[fill=boxpurple!60]{s2}{$(p2) + (-0.66,-1)$};
        \newSumNode[fill=boxpurple!60]{s3}{$(p2) + (0.66,-1)$};
        \newSumNode[fill=boxpurple!60]{s4}{$(p2) + (2.0,-1)$};
        \newGaussNode[fill=boxteal]{g1}{$(s1) + (0,-1)$};
        \newGaussNode[fill=boxorange!80]{g2}{$(s2) + (0,-1)$};
        \newGaussNode[fill=boxpink!50]{g3}{$(s3) + (0,-1)$};
        \newGaussNode[fill=boxgoldenrod!70]{g4}{$(s4) + (0,-1)$};
        \draw[edge] (r) edge (p1);
        \draw[edge] (r) edge (p2);
        \draw[edge] (r) edge (p3);
        \draw[edge] (p1) edge (s1);
        \draw[edge] (p1) edge (s3);
        \draw[edge] (p2) edge (s2);
        \draw[edge] (p2) edge (s3);
        \draw[edge] (p3) edge (s2);
        \draw[edge] (p3) edge (s4);
        \draw[edge] (s1) edge (g1);
        \draw[edge] (s1) edge (g2);
        \draw[edge] (s2) edge (g1);
        \draw[edge] (s2) edge (g2);
        \draw[edge] (s3) edge (g3);
        \draw[edge] (s3) edge (g4);
        \draw[edge] (s4) edge (g3);
        \draw[edge] (s4) edge (g4);
      \end{tikzpicture}
      }
    \end{center}
    \caption{}
    \label{fig:pc}
  \end{subfigure}
  \begin{subfigure}[t]{0.245\textwidth}
    \begin{center}
      \resizebox{\textwidth}{!}{
      \begin{tikzpicture}
        \newSumNode[fill=boxgreen]{r}{0,0};
        \newProdNode[fill=boxred!70]{p1}{$(r) + (-1.5,-1)$};
        \newProdNode[fill=boxred!70]{p2}{$(r) + (0,-1)$};
        \newProdNode[fill=boxred!70]{p3}{$(r) + (1.5,-1)$};
        \newSumNode[fill=boxpurple!60]{s1}{$(p2) + (-2.0,-1)$};
        \newSumNode[fill=boxpurple!60]{s2}{$(p2) + (-0.66,-1)$};
        \newSumNode[fill=boxpurple!60]{s3}{$(p2) + (0.66,-1)$};
        \newSumNode[fill=boxpurple!60]{s4}{$(p2) + (2.0,-1)$};
        \newGaussNode[fill=boxteal]{g1}{$(s1) + (0,-1)$};
        \newGaussNode[fill=boxorange!80]{g2}{$(s2) + (0,-1)$};
        \newGaussNode[fill=boxpink!50]{g3}{$(s3) + (0,-1)$};
        \newGaussNode[fill=boxgoldenrod!70]{g4}{$(s4) + (0,-1)$};
        \draw[thick,red,edge] (r) edge (p1);
        \draw[boxgray,edge] (r) edge (p2);
        \draw[boxgray,edge] (r) edge (p3);
        \draw[thick,red,edge] (p1) edge (s1);
        \draw[thick,red,edge] (p1) edge (s3);
        \draw[boxgray,edge] (p2) edge (s2);
        \draw[boxgray,edge] (p2) edge (s3);
        \draw[boxgray,edge] (p3) edge (s2);
        \draw[boxgray,edge] (p3) edge (s4);
        \draw[thick,red,edge] (s1) edge (g1);
        \draw[boxgray,edge] (s1) edge (g2);
        \draw[boxgray,edge] (s2) edge (g1);
        \draw[boxgray,edge] (s2) edge (g2);
        \draw[thick,red,edge] (s3) edge (g3);
        \draw[boxgray,edge] (s3) edge (g4);
        \draw[boxgray,edge] (s4) edge (g3);
        \draw[boxgray,edge] (s4) edge (g4);
      \end{tikzpicture}
      }
    \end{center}
    \caption{}
  \end{subfigure}\begin{subfigure}[t]{0.245\textwidth}
    \begin{center}
      \resizebox{\textwidth}{!}{
      \begin{tikzpicture}
        \newSumNode[fill=boxgreen]{r}{0,0};
        \newProdNode[fill=boxred!70]{p1}{$(r) + (-1.5,-1)$};
        \newProdNode[fill=boxred!70]{p2}{$(r) + (0,-1)$};
        \newProdNode[fill=boxred!70]{p3}{$(r) + (1.5,-1)$};
        \newSumNode[fill=boxpurple!60]{s1}{$(p2) + (-2.0,-1)$};
        \newSumNode[fill=boxpurple!60]{s2}{$(p2) + (-0.66,-1)$};
        \newSumNode[fill=boxpurple!60]{s3}{$(p2) + (0.66,-1)$};
        \newSumNode[fill=boxpurple!60]{s4}{$(p2) + (2.0,-1)$};
        \newGaussNode[fill=boxteal]{g1}{$(s1) + (0,-1)$};
        \newGaussNode[fill=boxorange!80]{g2}{$(s2) + (0,-1)$};
        \newGaussNode[fill=boxpink!50]{g3}{$(s3) + (0,-1)$};
        \newGaussNode[fill=boxgoldenrod!70]{g4}{$(s4) + (0,-1)$};
        \draw[boxgray,edge] (r) edge (p1);
        \draw[thick,blue,edge] (r) edge (p2);
        \draw[boxgray,edge] (r) edge (p3);
        \draw[boxgray,edge] (p1) edge (s1);
        \draw[boxgray,edge] (p1) edge (s3);
        \draw[thick,blue,edge] (p2) edge (s2);
        \draw[thick,blue,edge] (p2) edge (s3);
        \draw[boxgray,edge] (p3) edge (s2);
        \draw[boxgray,edge] (p3) edge (s4);
        \draw[boxgray,edge] (s1) edge (g1);
        \draw[boxgray,edge] (s1) edge (g2);
        \draw[boxgray,edge] (s2) edge (g1);
        \draw[thick,blue,edge] (s2) edge (g2);
        \draw[boxgray,edge] (s3) edge (g3);
        \draw[thick,blue,edge] (s3) edge (g4);
        \draw[boxgray,edge] (s4) edge (g3);
        \draw[boxgray,edge] (s4) edge (g4);
      \end{tikzpicture}
      }
    \end{center}
    \caption{}
    \label{fig:subcircs}
  \end{subfigure}\begin{subfigure}[t]{0.245\textwidth}
    \begin{center}
      \resizebox{\textwidth}{!}{
      \begin{tikzpicture}
        \newSumNode[fill=boxgreen]{r}{0,0};
        \newProdNode[fill=boxred!70]{p1}{$(r) + (-1.5,-1)$};
        \newProdNode[fill=boxred!70]{p2}{$(r) + (0,-1)$};
        \newProdNode[fill=boxred!70]{p3}{$(r) + (1.5,-1)$};
        \newSumNode[fill=boxpurple!60]{s1}{$(p2) + (-2.0,-1)$};
        \newSumNode[fill=boxpurple!60]{s2}{$(p2) + (-0.66,-1)$};
        \newSumNode[fill=boxpurple!60]{s3}{$(p2) + (0.66,-1)$};
        \newSumNode[fill=boxpurple!60]{s4}{$(p2) + (2.0,-1)$};
        \newGaussNode[fill=boxteal]{g1}{$(s1) + (0,-1)$};
        \newGaussNode[fill=boxorange!80]{g2}{$(s2) + (0,-1)$};
        \newGaussNode[fill=boxpink!50]{g3}{$(s3) + (0,-1)$};
        \newGaussNode[fill=boxgoldenrod!70]{g4}{$(s4) + (0,-1)$};
        \draw[boxgray,edge] (r) edge (p1);
        \draw[boxgray,edge] (r) edge (p2);
        \draw[thick,green!20!black,edge] (r) edge (p3);
        \draw[boxgray,edge] (p1) edge (s1);
        \draw[boxgray,edge] (p1) edge (s3);
        \draw[boxgray,edge] (p2) edge (s2);
        \draw[boxgray,edge] (p2) edge (s3);
        \draw[thick,green!20!black,edge] (p3) edge (s2);
        \draw[thick,green!20!black,edge] (p3) edge (s4);
        \draw[boxgray,edge] (s1) edge (g1);
        \draw[boxgray,edge] (s1) edge (g2);
        \draw[thick,green!20!black,edge] (s2) edge (g1);
        \draw[boxgray,edge] (s2) edge (g2);
        \draw[boxgray,edge] (s3) edge (g3);
        \draw[boxgray,edge] (s3) edge (g4);
        \draw[boxgray,edge] (s4) edge (g3);
        \draw[thick,green!20!black,edge] (s4) edge (g4);
      \end{tikzpicture}
      }
    \end{center}
    \caption{}
  \end{subfigure}
  \caption{A probabilistic circuit \uncaption{(a)} and 3 of its 12 possible induced subcircuits
    \uncaption{(b-d)}.}
  \label{fig:induced}
\end{figure}

A probabilistic circuit $\mathcal{C}$ that contains no consecutive sums or products (i.e. for every
sum all of its children are either inputs or products, and for products their children are either
inputs or sums) is said to be in \emph{standard} form circuit. Any PC can be efficiently
transformed into a \emph{standard} circuit in a process we call \emph{standardization} (see
\Cref{thm:standard}).

\begin{remark}[breakable]{On operators and tractability}{optract}
  Throughout this work we consider only products and convex combinations (apart from the implicit
  operations contained within input nodes) as potential computational units. The subject of whether
  any other operator could be used to gain expressivity without loss of tractability is without a
  doubt an interesting research question, and one that is actively being pursued. However, this is
  certainly out of the scope of this dissertation, and so we restrict discussion on this topic and
  only give a brief comment on operator tractability here, pointing to existing literature in this
  area of research.

  \citet{friesen16} formalize the notion of replacing sums and products in PCs with any pair of
  operators in a commutative semiring, giving results on the conditions for marginalization to be
  tractable. They provide examples of common semirings and to which known formalisms they
  correspond to. One such example are PCs under the Boolean semiring $(\{0,1\},\vee,\wedge,0,1)$
  for logical inference, which are equivalent to Negation Normal Form (NNF, \cite{barwise82}) and
  constitute an instance of Logic Circuits (LCs), of which Sentential Decision Diagrams (SDDs,
  \cite{darwiche11}) and Binary Decision Diagrams (BDDs, \cite{akers78}) are a part of. Another
  less common semiring in PCs is the real min-sum semiring $(\mathbb{R}_{\infty}, \min,+,\infty,0)$
  for nonconvex optimization \citep{friesen15}.

  Recently, \citet{vergari21} extensively covered tractability conditions and complexity bounds for
  convex combinations, products, $\exp$ (and more generally powers in both naturals and reals),
  quotients and logarithms, even giving results for complex information-theoretic queries, such as
  entropies and divergences. Notably, they analyze whether structural constraints (and thus, in a
  sense, tractability) under these conditions are preserved.

  Up to now, we have only considered summations as nonnegative weighted sums. Indeed, in most
  literature the sum node is defined as a convex combination. However, negative weights have
  appeared in Logistic Circuits \citep{liang19} for discriminative modeling; and in Probabilistic
  Generating Circuits \citep{zhang21}, a class of tractable probabilistic models that subsume PCs.
  \citet{maua17a} and \citet{mattei20b} extend (nonnegative) weights in sum nodes with probability
  intervals, effectively inducing a credal set \citep{cozman00} for measuring imprecision.

  Other works include PCs with quotients \citep{sharir18a}, transformations \citep{pevny20a}, max
  \citep{melibari16a}, and einsum \citep{peharz20b} operations.
\end{remark}

\section{Reasoning with Probabilistic Circuits}
\label{sec:inf}

Up to now, we have only vaguely touched on the issue of computing the value of a probabilistic
circuit. Throughout this section, we define some of the possible inference tasks available to PCs,
describe sufficient conditions for enabling these queries within this framework, and
algorithmically show how to tractably compute them in the computational graph when these conditions
are met. We start defining the four most basic probabilistic queries in PCs: probability of
evidence, marginal probability, conditional probability and maximum a posteriori probability.

Consider a normalized probability distribution $p$ whose scope is $\Sc(p)=\set{X}$ and let
$\set{x}$ be a complete assignment of $\set{X}$. We define the \emph{probability of evidence},
shortened to \evi{} for convenience, as the query $p(\set{x})$, i.e.\ the probability of $\set{X}$
taking values $\set{x}$ according to $p$, with $\set{X}$ being called the \emph{query variables}.
In a somewhat similar vein, given a set of variables $\set{Y}$ such that $\set{Y}\subset\set{X}$,
and calling $\set{y}$ a partial assignment of it, we say that the query $p(\set{y})$ is the
\emph{marginal probability}, here denoted by \mar{}, of query variables $\set{Y}$ with the
remaining variables $\set{Z}=\set{X}\cap \set{Y}$ marginalized. This corresponds to the summing out
$\sum_{\set{z}}p(\set{y}, \set{z})$ when in the discrete, and the integral
$\int_{\set{z}}p(\set{y},\set{z})\dif\set{z}$ in the continuous. Aside from \evi{} and \mar{},
prediction tasks such as classification often require computing the \emph{conditional probability}
$p(\set{y}|\set{z})=\frac{p(\set{y},\set{z})}{p(\set{z})}$, which we shall call by the shorthand
\con{}, of query variables $\set{Y}$ given evidence variables $\set{Z}$ in order to understand, for
instance in the case of image classification, the probability of a certain label given pixel
values. A related yet more difficult task is to compute the \emph{maximum a posteriori} probability
(\map{}), which involves finding the most probable assignment of a set of RVs $\set{Y}$, said to be
the query variables, conditioned on evidence variables $\set{X}$, say for image reconstruction. To
do so, we must compute the most probable assignment $\set{y}$ (for instance, the missing pixels to
be reconstructed) conditioned on evidence $\set{x}$ (for example, values of the known pixels), or
more formally
\begin{equation}
  \max_{\set{y}}p(\set{y}|\set{x})=\max_{\set{y}}\frac{p(\set{y},\set{x})}{p(\set{x})}=
  \frac{\max_{\set{y}}p(\set{y},\set{x})}{p(\set{x})}=\max_{\set{y}}p(\set{y},\set{x}).
  \label{eq:map}
\end{equation}
For this dissertation, we shall only consider the case of \emph{full} \map{}, i.e.\ when
$\set{x}\cup\set{y}$ forms a complete assignment of the scope, since computing the \emph{partial}
\map{}, i.e.\ when $\set{x}\cup\set{y}$ forms a partial assignment, is hard in most PCs
\citep{peharz16,decampos11}. Unless explicitly stated, \map{} shall mean full \map{}. Full \map{}
also goes by the name of \emph{most probable explanation} (MPE, \cite{darwiche09}) in literature.
Although at first it may seem like \map{} should be no harder than computing a \con{}, it turns out
that for smooth and decomposable PCs \map{} is NP-hard \citep{conaty17,mei18}. Now that we have
properly defined the queries we are interested in, we begin listing sufficient conditions for their
tractability in PCs and the algorithms that compute them.

As we have already (informally) seen in the previous section, computing the probability of evidence
for an assignment $\set{x}$ according to a PC $\mathcal{C}$ amounts to the computation of the value
of $\mathcal{C}$'s root by a bottom-up computation of node values. Starting with inputs, we compute
their value by querying their distribution for their probability of evidence; this is then
followed by simply computing inner node values normally. \Cref{alg:evi} shows this procedure
algorithmically, computing the \evi{} in linear time to the size of the circuit. Of note is the
fact that, if the PC is not normalized, then the result of this procedure must be normalized by a
constant, extractable in a similar way; however, since we assume all PCs to be normalized, the
value returned by \Cref{alg:evi} is already normalized.

\begin{algorithm}[t]
  \caption{\evi}\label{alg:evi}
  \begin{algorithmic}[1]
    \Require A PC $\mathcal{C}$ and complete assignment $\set{x}$
    \Ensure Value $\mathcal{C}(\set{x})$
    \State Let $v$ be a hash function mapping a node to its value
    \For{each $\Node$ in reverse topological order}
      \IIf{$\Node$ is an input}{$v(\Node)\gets\Node(\set{x})$}
      \IElseIf{$\Node$ is a sum}{$v(\Node)\gets\sum_{\Child\in\Ch(\Node)}w_{\Node,\Child}v_{\Child}$}
      \IElseIf{$\Node$ is a product}{$v(\Node)\gets\prod_{\Child\in\Ch(\Node)}v_{\Child}$}%
    \EndFor%
    \State \textbf{return} $v(\normalfont{R})$, where $\textsf{R}$ is $\mathcal{C}$'s root
  \end{algorithmic}
\end{algorithm}

By our definition of PCs, computing the \evi{} of a PC is always tractable. Computing the \mar{},
on the other hand, does not always come for free. We now introduce the first two structural
constraints for probabilistic circuits which, together, enable tractable \mar{} in PCs.

\begin{definition}[Smoothness]
  A probabilistic circuit $\mathcal{C}$ is said to be \emph{smooth} if for every sum node $\Sum$ in
  $\mathcal{C}$, $\Sc(\Child_1)=\Sc(\Child_2)$ for $\Child_1,\Child_2\in\Ch(\Sum)$.
\end{definition}

\begin{definition}[Decomposability]
  A probabilistic circuit $\mathcal{C}$ is said to be \emph{decomposable} if for every product node
  $\Prod$ in $\mathcal{C}$, $\Sc(\Child_1)\cap\Sc(\Child_2)=\emptyset$ for any two
  $\Child_1,\Child_2\in \Ch(\Prod)$.
\end{definition}

When a probabilistic circuit $\mathcal{C}$ is both \emph{smooth} and \emph{decomposable}, any
marginal is linear time computable in $\mathcal{C}$ \citep{poon11,peharz15}. Although smoothness
and decomposability are sufficient for tractably computing marginals, they are not necessary. In
fact, \emph{consistency} is a weaker constraint on products that, coupled with smoothness, confers
efficient \mar{} \citep{poon11}. \Cref{fig:smooth-decomp} shows examples of smooth and decomposable
circuits. Although there exist PCs that are neither smooth nor decomposable (or consistent) and
also have tractable \mar{}, as is the case of \Cref{eg:det}, these two properties are often adopted
during learning due to their intuitive and uncomplicated syntax. In fact, \textbf{all PCs shown
throughout this dissertation shall be \emph{at least} smooth and decomposable unless explicitly
stated otherwise.}

\begin{algorithm}[t]
  \caption{\mar}\label{alg:mar}
  \begin{algorithmic}[1]
    \Require A smooth and decomposable PC $\mathcal{C}$ and partial assignment $\set{x}$
    \Ensure Value $\int_\set{y}\mathcal{C}(\set{x},\set{y})\dif\set{y}$\Comment{Call
    $\set{Y}$ the remaining variables not in $\set{X}$}
    \State Let $v$ be a hash function mapping a node to its value
    \For{each $\Node$ in reverse topological order}
    \IIf{$\Node$ is an input}{$v(\Node)\gets\int_\set{y}\Node(\set{x},\set{y})\dif\set{y}$}
    \IElseIf{$\Node$ is a sum}{$v(\Node)\gets\sum_{\Child\in\Ch(\Node)}w_{\Node,\Child}v_{\Child}$}
    \IElseIf{$\Node$ is a product}{$v(\Node)\gets\prod_{\Child\in\Ch(\Node)}v_{\Child}$}%
    \EndFor%
    \State \textbf{return} $v(\normalfont{R})$, where $\textsf{R}$ is $\mathcal{C}$'s root
  \end{algorithmic}
\end{algorithm}

\begin{figure}[t]
  \begin{subfigure}[t]{0.31\textwidth}
    \begin{center}
      \begin{tikzpicture}
        \newSumNode[fill=boxgreen]{r}{0,0};
        \newProdNode[fill=boxred!70]{p1}{$(r) + (-1.5,-1)$};
        \newProdNode[fill=boxred!70]{p2}{$(r) + (0,-1)$};
        \newProdNode[fill=boxred!70]{p3}{$(r) + (1.5,-1)$};
        \newGaussNode[fill=boxteal,label=below:{$A$}]{a}{$(p1) + (0,-1)$};
        \newGaussNode[fill=boxorange!80,label=below:{$B$}]{b}{$(p2) + (0,-1)$};
        \newGaussNode[fill=boxpink!50,label=below:{$C$}]{c}{$(p3) + (0,-1)$};
        \draw[edge] (r) edge (p1);
        \draw[edge] (r) edge (p2);
        \draw[edge] (r) edge (p3);
        \draw[edge] (p1) edge (a);
        \draw[edge] (p1) edge (b);
        \draw[edge] (p2) edge (a);
        \draw[edge] (p2) edge (c);
        \draw[edge] (p3) edge (b);
        \draw[edge] (p3) edge (c);
      \end{tikzpicture}
    \end{center}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.31\textwidth}
    \begin{center}
      \begin{tikzpicture}
        \newProdNode[fill=boxgreen]{r}{0,0};
        \newSumNode[fill=boxred!70]{p1}{$(r) + (-2.0,-1)$};
        \newSumNode[fill=boxred!70]{p2}{$(r) + (-0.66,-1)$};
        \newSumNode[fill=boxred!70]{p3}{$(r) + (0.66,-1)$};
        \newSumNode[fill=boxred!70]{p4}{$(r) + (2.0,-1)$};
        \newGaussNode[fill=boxteal,label=below:{$A$}]{a1}{$(p1) + (0,-1)$};
        \newGaussNode[fill=boxteal,label=below:{$A$}]{a2}{$(p2) + (0,-1)$};
        \newGaussNode[fill=boxorange!80,label=below:{$B$}]{b1}{$(p3) + (0,-1)$};
        \newGaussNode[fill=boxorange!80,label=below:{$B$}]{b2}{$(p4) + (0,-1)$};
        \draw[edge] (r) -- (p1);
        \draw[edge] (r) -- (p2);
        \draw[edge] (r) -- (p3);
        \draw[edge] (r) -- (p4);
        \draw[edge] (p1) -- (a1);
        \draw[edge] (p1) -- (a2);
        \draw[edge] (p2) -- (a1);
        \draw[edge] (p2) -- (a2);
        \draw[edge] (p3) -- (b1);
        \draw[edge] (p3) -- (b2);
        \draw[edge] (p4) -- (b1);
        \draw[edge] (p4) -- (b2);
      \end{tikzpicture}
    \end{center}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.31\textwidth}
    \begin{center}
      \begin{tikzpicture}
        \newProdNode[fill=boxgreen]{r}{0,0};
        \newSumNode[fill=boxred!70]{s1}{$(r) + (-1.0,-0.5)$};
        \newSumNode[fill=boxred!70]{s2}{$(r) + (1.0,-0.5)$};
        \newProdNode[fill=boxpurple!60]{p1}{$(s1) + (-0.5,-0.75)$};
        \newProdNode[fill=boxpurple!60]{p2}{$(s1) + (0.5,-0.75)$};
        \newProdNode[fill=boxpurple!60]{p3}{$(s2) + (-0.5,-0.75)$};
        \newProdNode[fill=boxpurple!60]{p4}{$(s2) + (0.5,-0.75)$};
        \newGaussNode[fill=boxteal,label=below:{$A$}]{a}{$(p1) + (0,-0.75)$};
        \newGaussNode[fill=boxorange!80,label=below:{$B$}]{b}{$(p2) + (0,-0.75)$};
        \newGaussNode[fill=boxpink!50,label=below:{$C$}]{c}{$(p3) + (0,-0.75)$};
        \newGaussNode[fill=boxgoldenrod!70,label=below:{$D$}]{d}{$(p4) + (0,-0.75)$};
        \draw[edge] (r) -- (s1);
        \draw[edge] (r) -- (s2);
        \draw[edge] (s1) -- (p1);
        \draw[edge] (s1) -- (p2);
        \draw[edge] (s2) -- (p3);
        \draw[edge] (s2) -- (p4);
        \draw[edge] (p1) -- (a);
        \draw[edge] (p1) -- (b);
        \draw[edge] (p2) -- (a);
        \draw[edge] (p2) -- (b);
        \draw[edge] (p3) -- (c);
        \draw[edge] (p3) -- (d);
        \draw[edge] (p4) -- (c);
        \draw[edge] (p4) -- (d);
      \end{tikzpicture}
    \end{center}
    \caption{}
  \end{subfigure}
  \caption{Decomposable but non-smooth \uncaption{(a)}, smooth but non-decomposable
    \uncaption{(b)}, and smooth and decomposable \uncaption{(c)} circuits. Labels below inputs
    indicate their scope.}
  \label{fig:smooth-decomp}
\end{figure}

\begin{restatable}[\cite{poon11,pclec,vergari21}]{theorem}{linevi}
  \label{thm:linevi}
  Let $\mathcal{C}$ be a \emph{smooth} and \emph{decomposable} PC. Any one of \evi{}, \mar{} or
  \con{} can be computed in linear time (in the size of $\mathcal{C}$).
\end{restatable}

\begin{example}[sidebyside,lefthand width=0.55\textwidth]{Density estimation trees as probabilistic circuits}{det}
  A density estimation tree (DET) is a decision tree for the task of density estimation
  \citep{ram11}. Briefly, a decision tree $\mathcal{T}$ partitions the data space (usually
  $\mathbb{R}^d$) into cells by laying out hyperplanes usually orthogonal to the axes, creating a
  latent variable for each node that essentially determines in which region an observation should
  fall into. The density function of a tree $\mathcal{T}$ given a dataset $\set{D}$ of dimension
  $d$ is defined as the piecewise function
  \begin{equation}
    p_\mathcal{T}(\set{x})=\sum_{\Leaf\in\operatorname{Leaves}(\mathcal{T})}\frac{\left|\set{D}\in\Leaf\right|}{|\set{D}|}\cdot\frac{\liv\set{x}\in\Leaf\riv}{\operatorname{Vol}(\Leaf)},
  \end{equation}
  where $\left|\set{D}\in\Leaf\right|$ indicates the number of assignments $\set{x}$ in the
  training dataset $\set{D}$ which fall inside the cell determined by leaf $\Leaf$, and
  $\operatorname{Vol}(\Leaf)$ retuns the volume of the $d$-dimensional cell $\Leaf$. The top figure
  on the right shows a two-dimensional data space being partitioned, with the corresponding
  decision tree below it. Each node in the tree is a (hyper)plane partitioning data, with every
  edge determining which cell the observation falls into.

  Equivalently, a smooth but nondecomposable PC whose sum nodes are followed by products which
  determine which side of the hyperplane an assignment goes configures the same semantics as a DET
  decision node. Inputs in this PC act as the leaf nodes in the equivalent DET. The PC on the right
  translates the DET on top of it, with matching colors in the PC showing the equivalent leaves and
  decision nodes in the DET. Because products define disjoint restricted regions of the data space,
  integration can be pushed to the leaves, making marginalization tractable. This highlights the
  sufficiency but not necessity of smoothness and decomposability. An alternative smooth and
  decomposable formulation of DETs is given by \citet{correia20}.
  \tcblower
  \centering
  \begin{tikzpicture}
    \fill[boxpink!30] (0,0) rectangle (2,2.5);
    \fill[boxgoldenrod!40] (2,0) rectangle (3.5,2.5);
    \draw[very thick,boxblue,dashed] (3.5,0) -- node[left,pos=0.9] {\color{boxblue}$\set{A}$} (3.5,5);
    \draw[very thick,boxgreen,dashed] (0,2.5) -- node[right,pos=1] {\color{boxgreen}$\set{B}$} (3.5,2.5);
    \draw[very thick,boxpurple!80,dashed] (2,0) -- node[above,pos=1] {\color{boxpurple}$\set{C}$} (2,2.5);
    \node at (1,1.25) {$\Leaf_1$};
    \node at (2.75,1.25) {$\Leaf_2$};
    \node[circle,inner sep=0pt,minimum size=3pt,fill=black,label=above right:{$\set{x}$}] at (1.5,1.9) {};
    \draw[very thick] (0,0) rectangle (5, 5);

    \node[inner sep=1pt,minimum size=12pt,circle,fill=boxblue!80] (a) at (3,-1.0) {$\set{A}$};
    \node[inner sep=1pt,minimum size=12pt,circle,fill=boxgreen!80] (b) at ($(a) + (-1,-1.5)$) {$\set{B}$};
    \node[inner sep=1pt,minimum size=12pt,circle,fill=boxpurple!60] (c) at ($(b) + (1,-1.5)$) {$\set{C}$};
    \node[draw,fill=boxpink!30] (l1) at ($(c) + (-1,-1.5)$) {$L_1$};
    \node[draw,fill=boxgoldenrod!40] (l2) at ($(c) + (1,-1.5)$) {$L_2$};
    \draw (a) -- node[below,pos=1] {$\vdots$} node[above right,pos=0.5] {$\set{x}\in\set{A}^+$} ($(a) + (1,-1.5)$);
    \draw (a) -- node[above left,pos=0.5] {$\set{x}\in\set{A}^-$} (b);
    \draw[edge,boxdgray] (a) edge[bend left=15] (b);
    \draw (b) -- node[below,pos=1] {$\vdots$} node[above left,pos=0.5] {$\set{x}\in\set{B}^-$} ($(b) + (-1,-1.5)$);
    \draw (b) -- node[above right,pos=0.5] {$\set{x}\in\set{B}^+$} (c);
    \draw[edge,boxdgray] (b) edge[bend right=15] (c);
    \draw (c) -- node[above left,pos=0.5] {$\set{x}\in\set{C}^-$} (l1);
    \draw (c) -- node[above right,pos=0.5] {$\set{x}\in\set{C}^+$} (l2);
    \draw[edge,boxdgray] (c) edge[bend left=15] (l1);

    \newSumNode[fill=boxblue!80,label=above left:{$\set{A}$}]{a}{$(a) + (-0.5,-6.0)$};
    \newProdNode[fill=boxblue!80]{al}{$(a) + (-0.5,-1)$};
    \newProdNode[fill=boxblue!80]{ar}{$(a) + (0.5,-1)$};
    \newLeafNode[fill=boxblue!80,label=below:{$\left\liv\set{x}\in\set{A}^-\right\riv$}]{ai}{$(al) + (-1.0,-1)$};
    \newLeafNode[fill=boxblue!80,label=below:{$\left\liv\set{x}\in\set{A}^+\right\riv$}]{nai}{$(ar) + (1.0,-1)$};
    \draw[edge] (a) -- (al); \draw[edge] (al) -- (ai);
    \draw[edge] (a) -- (ar); \draw[edge] (ar) -- (nai);
    \draw[edge] (ar) -- node[below,pos=1] {$\vdots$} ($(ar) + (0,-1)$);

    \newSumNode[fill=boxgreen!80,label=above right:{$\set{B}$}]{b}{$(al) + (0,-1)$};
    \newProdNode[fill=boxgreen!80]{bl}{$(b) + (0,-1)$};
    \newProdNode[fill=boxgreen!80]{br}{$(b) + (1,-1)$};
    \newLeafNode[fill=boxgreen!80,label=below:{$\left\liv\set{x}\in\set{B}^-\right\riv$}]{bi}{$(bl) + (-1.0,-1)$};
    \newLeafNode[fill=boxgreen!80,label=below:{$\left\liv\set{x}\in\set{B}^+\right\riv$}]{nbi}{$(br) + (1.0,-1)$};
    \draw[edge] (al) -- (b);
    \draw[edge] (b) -- (bl); \draw[edge] (bl) -- (bi);
    \draw[edge] (b) -- (br); \draw[edge] (br) -- (nbi);
    \draw[edge] (bl) -- node[below,pos=1] {$\vdots$} ($(bl) + (0,-1)$);

    \newSumNode[fill=boxpurple!60,label=above left:{$\set{C}$}]{c}{$(br) + (0,-1)$};
    \newProdNode[fill=boxpurple!60]{cl}{$(c) + (-1,-1)$};
    \newProdNode[fill=boxpurple!60]{cr}{$(c) + (0,-1)$};
    \newLeafNode[fill=boxpurple!60,label=below:{$\left\liv\set{x}\in\set{C}^-\right\riv$}]{ci}{$(cl) + (-1.0,-1)$};
    \newLeafNode[fill=boxpurple!60,label=below:{$\left\liv\set{x}\in\set{C}^+\right\riv$}]{nci}{$(cr) + (1.0,-1)$};
    \draw[edge] (br) -- (c);
    \draw[edge] (c) -- (cr); \draw[edge] (cl) -- (ci);
    \draw[edge] (c) -- (cl); \draw[edge] (cr) -- (nci);
    \newGaussNode[label=below:{$\Leaf_1$},fill=boxpink!30]{l1}{$(cl) + (0,-1)$};
    \newGaussNode[label=below:{$\Leaf_2$},fill=boxgoldenrod!40]{l2}{$(cr) + (0,-1)$};
    \draw[edge] (cl) -- (l1);
    \draw[edge] (cr) -- (l2);

    %\newSumNode[fill=boxteal]{r}{$(a) + (0.0,-7.5)$};
    %\newGaussNode[fill=boxpink!30,label=below:{$\Leaf_1\cdot\liv\set{x}\in\Leaf_1\riv$}]{l1}{$(r) + (-1.5,-1)$};
    %\newGaussNode[fill=boxgoldenrod!40,label=below:{$\Leaf_2\cdot\liv\set{x}\in\Leaf_2\riv$}]{l2}{$(r) + (1.5,-1)$};
    %\node at ($(r) + (0,-1)$) {$\cdots$};
    %\draw[edge] (r) -- node[above left,pos=0.5] {$w_{\Leaf_1}$} (l1);
    %\draw[edge] (r) -- node[above right,pos=0.5] {$w_{\Leaf_2}$} (l2);
  \end{tikzpicture}
\end{example}

What \Cref{thm:linevi}'s proof on \cpageref{proof:linevi} tells us is that, algorithmically, the
only difference between \evi{}, \mar{} and \con{} is what is done on the input nodes. This can be
seen in \Cref{alg:mar}, which shows how to compute marginals in PCs. With \mar{} available to us,
querying for \con{} reduces to a simple two-pass evaluation over the circuit: the first computes
the numerator as the marginal $p(\set{Y},\set{X})$, and the second the denominator marginal
$p(\set{X})$.

Besides smoothness and decomposability, another structural constraint of interest is
\emph{determinism}. Together with the first two, determinism provides sufficient conditions for
tractably computing the \map{}.

\begin{algorithm}[t]
  \caption{\textproc{MaxProduct}}\label{alg:map}
  \begin{algorithmic}[1]
    \Require A smooth, decomposable and deterministic PC $\mathcal{C}$ and evidence assignment $\set{x}$
    \Ensure Value $\max_{\set{y}}\mathcal{C}(\set{y}|\set{x})$
    \State Let $v$ be a hash function mapping a node to its probability
    \For{each $\Node$ in reverse topological order}
      \IIf{$\Node$ is an input}{$v(\Node)\gets\max_{\set{y}}\Node(\set{y},\set{x})$}
      \IElseIf{$\Node$ is a sum}{$v(\Node)\gets\max_{\Child\in\Ch(\Node)}w_{\Node,\Child}v_{\Child}$}
      \IElseIf{$\Node$ is a product}{$v(\Node)\gets\prod_{\Child\in\Ch(\Node)}v_{\Child}$}%
    \EndFor%
    \State \textbf{return} $v_{\normalfont{R}}/\mathcal{C}(\set{x})$, where $\textsf{R}$ is
    $\mathcal{C}$'s root and $\mathcal{C}(\set{x})$ is the \evi{} on $\set{x}$
  \end{algorithmic}
\end{algorithm}

\begin{definition}[Determinism]
  A probabilistic circuit $\mathcal{C}$ is said to be \emph{deterministic} if for every sum node
  $\Sum\in\mathcal{C}$ at most one child of $\Sum$ has nonnegative value for any complete assignment.
\end{definition}

\begin{restatable}[\cite{peharz16}]{theorem}{det}
  \label{thm:det}
  Let $\mathcal{C}$ be a smooth, decomposable and \emph{deterministic} PC.
  \textup{\textproc{MaxProduct}} computes the \map{} in $\mathcal{C}$ in linear time (on the size
  of $\mathcal{C}$).
\end{restatable}

\begin{figure}[t]
  \begin{subfigure}[t]{0.32\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newSumNode[fill=boxgreen]{r}{0,0};
      \newProdNode[fill=boxred!70]{p1}{$(r) + (-0.75,-1.25)$};
      \newProdNode[fill=boxred!70]{p2}{$(r) + (0.75,-1.25)$};
      \newLeafNode[fill=boxbrown!60,label=below:{\scriptsize$\left\liv x\leq 0\right\riv$}]{x1}{$(p1) + (-1.5,-1.25)$};
      \newLeafNode[fill=boxbrown!60,label=below:{\scriptsize$\left\liv x>0\right\riv$}]{x2}{$(p2) + (1.5,-1.25)$};
      \newSumNode[fill=boxpurple!70]{s1}{$(p1) + (0,-1.25)$};
      \newSumNode[fill=boxpurple!70]{s2}{$(p2) + (0,-1.25)$};
      \newProdNode[fill=boxteal]{q1}{$(s1) + (0,-1.25)$};
      \newProdNode[fill=boxteal]{q2}{$(s2) + (0,-1.25)$};
      \newLeafNode[fill=boxpink!50,label=below:{\scriptsize$\left\liv y\leq 0\right\riv$}]{y1}{$(q1) + (-1.5,-1.25)$};
      \newLeafNode[fill=boxpink!50,label=below:{\scriptsize$\left\liv y>0\right\riv$}]{y2}{$(q2) + (1.5,-1.25)$};
      \newGaussNode[label=below:{\scriptsize$\mathcal{N}_A(1,0.7)$},fill=boxgoldenrod!70]{a}{$(q1) + (0,-1.25)$};
      \newGaussNode[label=below:{\scriptsize$\mathcal{N}_B(3,0.5)$},fill=boxorange!70]{b}{$(q2) + (0,-1.25)$};
      \draw[edge] (r)  edge[bend left=5] node[above left,pos=0.5] {\scriptsize$0.3$} (p1);
      \draw[edge] (r)  edge[bend left=5] node[above right,pos=0.5] {\scriptsize$0.7$} (p2);
      \draw[edge] (p1) edge[bend left=5] (x1);
      \draw[edge] (p1) edge[bend left=5] (s1);
      \draw[edge] (p2) edge[bend left=5] (x2);
      \draw[edge] (p2) edge[bend left=5] (s2);
      \draw[edge] (s1) edge[bend left=5] node[left,pos=0.5] {\scriptsize$0.2$} (q1);
      \draw[edge] (s1) edge[bend left=5] node[right,pos=0.05,yshift=0.1] {\scriptsize$0.8$} (q2);
      \draw[edge] (s2) edge[bend right=5] node[left,pos=0.05,yshift=0.1] {\scriptsize$0.6$} (q1);
      \draw[edge] (s2) edge[bend left=5] node[right,pos=0.5] {\scriptsize$0.4$} (q2);
      \draw[edge] (q1) edge[bend left=5] (y1);
      \draw[edge] (q1) edge[bend left=5] (a);
      \draw[edge] (q1) edge[bend left=5] (b);
      \draw[edge] (q2) edge[bend left=5] (y2);
      \draw[edge] (q2) edge[bend left=5] (a);
      \draw[edge] (q2) edge[bend left=5] (b);
      \draw[edge,boxdgray] (p1) edge[bend right=-5] (r);
      \draw[edge,boxdgray] (p2) edge[bend right=-5] (r);
      \draw[edge,boxdgray] (x1) edge[bend right=-5] (p1);
      \draw[edge,boxdgray] (s1) edge[bend right=-5] (p1);
      \draw[edge,boxdgray] (x2) edge[bend right=-5] (p2);
      \draw[edge,boxdgray] (s2) edge[bend right=-5] (p2);
      \draw[edge,boxdgray] (q1) edge[bend right=-5] (s1);
      \draw[edge,boxdgray] (q2) edge[bend right=-5] (s1);
      \draw[edge,boxdgray] (q1) edge[bend left=-5] (s2);
      \draw[edge,boxdgray] (q2) edge[bend right=-5] (s2);
      \draw[edge,boxdgray] (y1) edge[bend right=-5] (q1);
      \draw[edge,boxdgray] (a) edge[bend right=-5] (q1);
      \draw[edge,boxdgray] (b) edge[bend right=-5] (q1);
      \draw[edge,boxdgray] (y2) edge[bend right=-5] (q2);
      \draw[edge,boxdgray] (a) edge[bend right=-5] (q2);
      \draw[edge,boxdgray] (b) edge[bend right=-5] (q2);
      \node (inx1) at ($(x1) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut x=1$}};
      \node (inx2) at ($(x2) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut x=1$}};
      \node (iny1) at ($(y1) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut y=-1$}};
      \node (iny2) at ($(y2) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut y=-1$}};
      \node (ina) at ($(a) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut a=1.2$}};
      \node (inb) at ($(b) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut b=3.6$}};
      \node (out) at ($(r) + (0,0.9)$) {\scriptsize\colorbox{boxgreen}{\color{white}$.089$}};
      \draw[edge,boxdgray] (inx1) -- ($(x1) + (0,-0.75)$);
      \draw[edge,boxdgray] (inx2) -- ($(x2) + (0,-0.75)$);
      \draw[edge,boxdgray] (iny1) -- ($(y1) + (0,-0.75)$);
      \draw[edge,boxdgray] (iny2) -- ($(y2) + (0,-0.75)$);
      \draw[edge,boxdgray] (ina) -- ($(a) + (0,-0.75)$);
      \draw[edge,boxdgray] (inb) -- ($(b) + (0,-0.75)$);
      \draw[edge,boxdgray] (r) -- (out);
      \node at ($(a) + (-0.6,0.1)$) {\scriptsize\colorbox{boxgoldenrod}{\color{white}$.54$}};
      \node at ($(b) + (0.6,0.1)$) {\scriptsize\colorbox{boxorange}{\color{white}$.38$}};
      \node at ($(y1) + (-0.4,0.4)$) {\scriptsize\colorbox{boxpink}{\color{white}$1$}};
      \node at ($(y2) + (0.4,0.4)$) {\scriptsize\colorbox{boxpink}{\color{white}$0$}};
      \node at ($(q1) + (-0.6,0.2)$) {\scriptsize\colorbox{boxteal}{\color{white}$.21$}};
      \node at ($(q2) + (0.6,0.2)$) {\scriptsize\colorbox{boxteal}{\color{white}$0$}};
      \node at ($(s1) + (-0.6,0.2)$) {\scriptsize\colorbox{boxpurple}{\color{white}$.04$}};
      \node at ($(s2) + (0.6,0.2)$) {\scriptsize\colorbox{boxpurple}{\color{white}$.12$}};
      \node at ($(x1) + (-0.4,0.4)$) {\scriptsize\colorbox{boxbrown}{\color{white}$0$}};
      \node at ($(x2) + (0.4,0.4)$) {\scriptsize\colorbox{boxbrown}{\color{white}$1$}};
      \node at ($(p1) + (-0.5,0.4)$) {\scriptsize\colorbox{boxred}{\color{white}$0$}};
      \node at ($(p2) + (0.5,0.4)$) {\scriptsize\colorbox{boxred}{\color{white}$.12$}};
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newMaxNode[fill=boxgreen]{r}{0,0};
      \newProdNode[fill=boxred!70]{p1}{$(r) + (-0.75,-1.25)$};
      \newProdNode[fill=boxred!70]{p2}{$(r) + (0.75,-1.25)$};
      \newLeafNode[fill=boxbrown!60,label=below:{\scriptsize$\left\liv x\leq 0\right\riv$}]{x1}{$(p1) + (-1.5,-1.25)$};
      \newLeafNode[fill=boxbrown!60,label=below:{\scriptsize$\left\liv x>0\right\riv$}]{x2}{$(p2) + (1.5,-1.25)$};
      \newMaxNode[fill=boxpurple!70]{s1}{$(p1) + (0,-1.25)$};
      \newMaxNode[fill=boxpurple!70]{s2}{$(p2) + (0,-1.25)$};
      \newProdNode[fill=boxteal]{q1}{$(s1) + (0,-1.25)$};
      \newProdNode[fill=boxteal]{q2}{$(s2) + (0,-1.25)$};
      \newLeafNode[fill=boxpink!50,label=below:{\scriptsize$\left\liv y\leq 0\right\riv$}]{y1}{$(q1) + (-1.5,-1.25)$};
      \newLeafNode[fill=boxpink!50,label=below:{\scriptsize$\left\liv y>0\right\riv$}]{y2}{$(q2) + (1.5,-1.25)$};
      \newGaussNode[label=below:{\scriptsize$\mathcal{N}_A(1,0.7)$},fill=boxgoldenrod!70]{a}{$(q1) + (0,-1.25)$};
      \newGaussNode[label=below:{\scriptsize$\mathcal{N}_B(3,0.5)$},fill=boxorange!70]{b}{$(q2) + (0,-1.25)$};
      \draw[edge] (r)  edge[bend left=5] node[above left,pos=0.5] {\scriptsize$0.3$} (p1);
      \draw[edge] (r)  edge[bend left=5] node[above right,pos=0.5] {\scriptsize$0.7$} (p2);
      \draw[edge] (p1) edge[bend left=5] (x1);
      \draw[edge] (p1) edge[bend left=5] (s1);
      \draw[edge] (p2) edge[bend left=5] (x2);
      \draw[edge] (p2) edge[bend left=5] (s2);
      \draw[edge] (s1) edge[bend left=5] node[left,pos=0.5] {\scriptsize$0.2$} (q1);
      \draw[edge] (s1) edge[bend left=5] node[right,pos=0.05,yshift=0.1] {\scriptsize$0.8$} (q2);
      \draw[edge] (s2) edge[bend right=5] node[left,pos=0.05,yshift=0.1] {\scriptsize$0.6$} (q1);
      \draw[edge] (s2) edge[bend left=5] node[right,pos=0.5] {\scriptsize$0.4$} (q2);
      \draw[edge] (q1) edge[bend left=5] (y1);
      \draw[edge] (q1) edge[bend left=5] (a);
      \draw[edge] (q1) edge[bend left=5] (b);
      \draw[edge] (q2) edge[bend left=5] (y2);
      \draw[edge] (q2) edge[bend left=5] (a);
      \draw[edge] (q2) edge[bend left=5] (b);
      \draw[edge,boxdgray] (p1) edge[bend right=-5] (r);
      \draw[edge,boxdgray] (p2) edge[bend right=-5] (r);
      \draw[edge,boxdgray] (x1) edge[bend right=-5] (p1);
      \draw[edge,boxdgray] (s1) edge[bend right=-5] (p1);
      \draw[edge,boxdgray] (x2) edge[bend right=-5] (p2);
      \draw[edge,boxdgray] (s2) edge[bend right=-5] (p2);
      \draw[edge,boxdgray] (q1) edge[bend right=-5] (s1);
      \draw[edge,boxdgray] (q2) edge[bend right=-5] (s1);
      \draw[edge,boxdgray] (q1) edge[bend left=-5] (s2);
      \draw[edge,boxdgray] (q2) edge[bend right=-5] (s2);
      \draw[edge,boxdgray] (y1) edge[bend right=-5] (q1);
      \draw[edge,boxdgray] (a) edge[bend right=-5] (q1);
      \draw[edge,boxdgray] (b) edge[bend right=-5] (q1);
      \draw[edge,boxdgray] (y2) edge[bend right=-5] (q2);
      \draw[edge,boxdgray] (a) edge[bend right=-5] (q2);
      \draw[edge,boxdgray] (b) edge[bend right=-5] (q2);
      \node (inx1) at ($(x1) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut x=1$}};
      \node (inx2) at ($(x2) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut x=1$}};
      \node (iny1) at ($(y1) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut \max y$}};
      \node (iny2) at ($(y2) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut \max y$}};
      \node (ina) at ($(a) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut a=1.2$}};
      \node (inb) at ($(b) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut \max b$}};
      \node (out) at ($(r) + (0,0.9)$) {\scriptsize\colorbox{boxgreen}{\color{white}$.183$}};
      \draw[edge,boxdgray] (inx1) -- ($(x1) + (0,-0.75)$);
      \draw[edge,boxdgray] (inx2) -- ($(x2) + (0,-0.75)$);
      \draw[edge,boxdgray] (iny1) -- ($(y1) + (0,-0.75)$);
      \draw[edge,boxdgray] (iny2) -- ($(y2) + (0,-0.75)$);
      \draw[edge,boxdgray] (ina) -- ($(a) + (0,-0.75)$);
      \draw[edge,boxdgray] (inb) -- ($(b) + (0,-0.75)$);
      \draw[edge,boxdgray] (r) -- (out);
      \node at ($(a) + (-0.6,0.1)$) {\scriptsize\colorbox{boxgoldenrod}{\color{white}$.54$}};
      \node at ($(b) + (0.6,0.1)$) {\scriptsize\colorbox{boxorange}{\color{white}$.79$}};
      \node at ($(y1) + (-0.4,0.4)$) {\scriptsize\colorbox{boxpink}{\color{white}$1$}};
      \node at ($(y2) + (0.4,0.4)$) {\scriptsize\colorbox{boxpink}{\color{white}$1$}};
      \node at ($(q1) + (-0.6,0.2)$) {\scriptsize\colorbox{boxteal}{\color{white}$.43$}};
      \node at ($(q2) + (0.6,0.2)$) {\scriptsize\colorbox{boxteal}{\color{white}$.43$}};
      \node at ($(s1) + (-0.6,0.2)$) {\scriptsize\colorbox{boxpurple}{\color{white}$.35$}};
      \node at ($(s2) + (0.6,0.2)$) {\scriptsize\colorbox{boxpurple}{\color{white}$.26$}};
      \node at ($(x1) + (-0.4,0.4)$) {\scriptsize\colorbox{boxbrown}{\color{white}$0$}};
      \node at ($(x2) + (0.4,0.4)$) {\scriptsize\colorbox{boxbrown}{\color{white}$1$}};
      \node at ($(p1) + (-0.5,0.4)$) {\scriptsize\colorbox{boxred}{\color{white}$0$}};
      \node at ($(p2) + (0.5,0.4)$) {\scriptsize\colorbox{boxred}{\color{white}$.26$}};
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newMaxNode[fill=boxgreen]{r}{0,0};
      \newProdNode[fill=boxred!70]{p1}{$(r) + (-0.75,-1.25)$};
      \newProdNode[fill=boxred!70]{p2}{$(r) + (0.75,-1.25)$};
      \newLeafNode[fill=boxbrown!60,label=below:{\scriptsize$\left\liv x\leq 0\right\riv$}]{x1}{$(p1) + (-1.5,-1.25)$};
      \newLeafNode[fill=boxbrown!60,label=below:{\scriptsize$\left\liv x>0\right\riv$}]{x2}{$(p2) + (1.5,-1.25)$};
      \newMaxNode[fill=boxpurple!70]{s1}{$(p1) + (0,-1.25)$};
      \newMaxNode[fill=boxpurple!70]{s2}{$(p2) + (0,-1.25)$};
      \newProdNode[fill=boxteal]{q1}{$(s1) + (0,-1.25)$};
      \newProdNode[fill=boxteal]{q2}{$(s2) + (0,-1.25)$};
      \newLeafNode[fill=boxpink!50,label=below:{\scriptsize$\left\liv y\leq 0\right\riv$}]{y1}{$(q1) + (-1.5,-1.25)$};
      \newLeafNode[fill=boxpink!50,label=below:{\scriptsize$\left\liv y>0\right\riv$}]{y2}{$(q2) + (1.5,-1.25)$};
      \newGaussNode[label=below:{\scriptsize$\mathcal{N}_A(1,0.7)$},fill=boxgoldenrod!70]{a}{$(q1) + (0,-1.25)$};
      \newGaussNode[label=below:{\scriptsize$\mathcal{N}_B(3,0.5)$},fill=boxorange!70]{b}{$(q2) + (0,-1.25)$};
      \draw[edge] (r)  edge node[above left,pos=0.5] {\scriptsize$0.3$} (p1);
      \draw[edge] (r)  edge[bend left=5] node[above right,pos=0.5] {\scriptsize$0.7$} (p2);
      \draw[edge] (p1) edge (x1);
      \draw[edge] (p1) edge (s1);
      \draw[edge] (p2) edge[bend left=5] (x2);
      \draw[edge] (p2) edge[bend left=5] (s2);
      \draw[edge] (s1) edge node[left,pos=0.5] {\scriptsize$0.2$} (q1);
      \draw[edge] (s1) edge node[right,pos=0.05,yshift=0.1] {\scriptsize$0.8$} (q2);
      \draw[edge] (s2) edge[bend right=5] node[left,pos=0.05,yshift=0.1] {\scriptsize$0.6$} (q1);
      \draw[edge] (s2) edge node[right,pos=0.5] {\scriptsize$0.4$} (q2);
      \draw[edge] (q1) edge[bend left=5] (y1);
      \draw[edge] (q1) edge[bend left=5] (a);
      \draw[edge] (q1) edge[bend left=5] (b);
      \draw[edge] (q2) edge (y2);
      \draw[edge] (q2) edge (a);
      \draw[edge] (q2) edge (b);
      \draw[edge,boxdgray] (r) edge[bend left=-5] (p2);
      \draw[edge,boxdgray] (p2) edge[bend left=-5] (x2);
      \draw[edge,boxdgray] (p2) edge[bend left=-5] (s2);
      \draw[edge,boxdgray] (s2) edge[bend left=5] (q1);
      \draw[edge,boxdgray] (q1) edge[bend left=-5] (y1);
      \draw[edge,boxdgray] (q1) edge[bend left=-5] (a);
      \draw[edge,boxdgray] (q1) edge[bend left=-5] (b);
      \node (inx2) at ($(x2) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut x=1$}};
      \node (iny1) at ($(y1) + (0,-1.5)$) {\scriptsize\colorbox{boxgreen}{\color{white}$\mathstrut \max y\leq 0$}};
      \node (ina) at ($(a) + (0,-1.5)$) {\scriptsize\colorbox{boxblue}{\color{white}$\mathstrut a=1.2$}};
      \node (inb) at ($(b) + (0,-1.5)$) {\scriptsize\colorbox{boxgreen}{\color{white}$\mathstrut \max b=3$}};
      \node (out) at ($(r) + (0,0.9)$) {};
      \draw[edge,boxdgray] ($(x2) + (0,-0.75)$) -- (inx2);
      \draw[edge,boxdgray] ($(y1) + (0,-0.75)$) -- (iny1);
      \draw[edge,boxdgray] ($(a) + (0,-0.75)$)  -- (ina);
      \draw[edge,boxdgray] ($(b) + (0,-0.75)$)  -- (inb);
      \draw[edge,boxdgray] (out) -- (r);
      \node at ($(a) + (-0.6,0.1)$) {\scriptsize\colorbox{boxgoldenrod}{\color{white}$.54$}};
      \node at ($(b) + (0.6,0.1)$) {\scriptsize\colorbox{boxorange}{\color{white}$.79$}};
      \node at ($(y1) + (-0.4,0.4)$) {\scriptsize\colorbox{boxpink}{\color{white}$1$}};
      \node at ($(q1) + (-0.6,0.2)$) {\scriptsize\colorbox{boxteal}{\color{white}$.43$}};
      \node at ($(s2) + (0.6,0.2)$) {\scriptsize\colorbox{boxpurple}{\color{white}$.26$}};
      \node at ($(x2) + (0.4,0.4)$) {\scriptsize\colorbox{boxbrown}{\color{white}$1$}};
      \node at ($(p2) + (0.5,0.4)$) {\scriptsize\colorbox{boxred}{\color{white}$.26$}};
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \caption{The computation, on a smooth, decomposable and deterministic probabilistic circuit, of
    \evi{} $p(a=1.2,b=3.6,x=1,y=-1)\approx 0.089$ \uncaption{(a)}, \map{}
    $\max_{b,y}p(b,y|a=1.2,x=1)\approx 0.183$ via \textup{\textproc{MaxProduct}} \uncaption{(b)},
    and $\argmax_{b,y}p(b,y|a=1.2,x=1)=\{b=3,y\leq 0\}$ by backtracking the values set by
    \textup{\textproc{MaxProduct}} \uncaption{(c)}. \inode{\newMaxNode} nodes signal the
    replacement of sums with maximizations in \textup{\textproc{MaxProduct}}. The backtracking in
    \uncaption{(c)} is done from the root down, finding a max induced tree by propagating through
    all product children and only the highest valued child in \inode{\newMaxNode} nodes.}
  \label{fig:map}
\end{figure}

As \Cref{thm:det}'s proof on \cpageref{proof:det} suggests, the \map{} on a smooth, decomposable
and deterministic PC can be easily computed by simply replacing sum nodes with a max operation and
performing a bottom-up \evi{} pass. This is commonly called the Max-Product algorithm, shown more
formally in \Cref{alg:map} and visually exemplified in \Cref{fig:map}. To find the assignment
$\set{y}$ that maximizes \Cref{eq:map} in a given circuit $\mathcal{C}$, we first compute the
\map{} probabilities through the usual bottom-up pass, and then find the maximum (in terms of
probability) induced tree $\mathcal{M}$ rooted at $\mathcal{C}$ by backtracking the graph to the
most probable assignments. This maximum induced tree can be retrieved by a top-down pass selecting
the most probable sum child nodes according to the probabilities set by \map{}. Since $\mathcal{C}$
is decomposable, there cannot exist a node in $\mathcal{M}$ with more than one parent, meaning it
is by construction a tree whose leaves are input nodes with scopes whose union is the scope of
$\mathcal{C}$. This reduces the problem to a divide-and-conquer approach where each input node is
individually maximized.

Many known probabilistic models can be subsumed as smooth, decomposable and deterministic PCs, such
Markov networks \citep{lowd13a}, nave Bayes, thin junction trees \citep{bach01}, and more
generally low-treewidth Bayesian networks \citep{darwiche03}. \Cref{eg:nbayes} shows a Gaussian
nave Bayes model as a probabilistic circuit.

\begin{example}[sidebyside,lefthand width=0.55\textwidth]{Nave Bayes as probabilistic circuits}{nbayes}
  Suppose we have samples of per capita census measurements on three different features, say age
  $A$, body mass index $B$ and average amount of cheese consumed daily $C$ from three different
  cities $Y$.  Assuming $A$, $B$ and $C$ are independent, given a sample $x=(a,b,c)$ we can use
  Gaussian nave Bayes to predict $x$'s city
  \begin{equation}
    p(y|a,b,c)=p(y)p(a|y)p(b|y)p(c|y).
  \end{equation}
  In PC terms, $p(y)$ are the prior probabilities, i.e. sum weights, for each class and $p(z|y)$
  are Gaussian input nodes corresponding to the distributions of each feature in each city. To make
  sure that these are in fact conditional distributions, we introduce indicator variables
  ``selecting'' $Y$'s state. Since the resulting PC is deterministic, we can compute the \map{} for
  classification in linear time by simply replacing the root node with a max, which is exactly
  equivalent to finding the highest value of $x$ for each city $y$.
  \tcblower
  \begin{center}
  \begin{tikzpicture}
    \node[label=above:{\scriptsize$p(Y)$},inner sep=0pt,thick,minimum size=12pt,draw,circle,fill=boxteal] (y) at (0,0) {$Y$};
    \node[label=below:{\scriptsize$p(A|Y)$},inner sep=0pt,thick,minimum size=12pt,draw,circle,fill=boxorange!80] (a) at ($(y) + (-1,-1)$) {$A$};
    \node[label=below:{\scriptsize$p(B|Y)$},inner sep=0pt,thick,minimum size=12pt,draw,circle,fill=boxpink!50] (b) at ($(y) + (0,-1)$) {$B$};
    \node[label=below:{\scriptsize$p(C|Y)$},inner sep=0pt,thick,minimum size=12pt,draw,circle,fill=boxgoldenrod!70] (c) at ($(y) + (1,-1)$) {$C$};
    \draw[edge] (y) -- (a);
    \draw[edge] (y) -- (b);
    \draw[edge] (y) -- (c);
  \end{tikzpicture}

  \vskip -0.25cm
  \small%
  Gaussian nave Bayes

  \begin{tikzpicture}
    \newSumNode[fill=boxgreen]{r}{0,0};
    \newProdNode[fill=boxred!70]{p1}{$(r) + (1,-2.25)$};
    \newProdNode[fill=boxred!70]{p2}{$(r) + (1,0)$};
    \newProdNode[fill=boxred!70]{p3}{$(r) + (1,2.25)$};
    \newLeafNode[fill=boxteal,label={[label distance=-0.1cm]below right:{\scriptsize$\liv y=1\riv$}}]{i1}{$(p1) + (0.5,-1.5)$};
    \newLeafNode[fill=boxteal,label={[label distance=-0.1cm]below right:{\scriptsize$\liv y=2\riv$}}]{i2}{$(p2) + (0.5,-1.5)$};
    \newLeafNode[fill=boxteal,label={[label distance=-0.1cm]below right:{\scriptsize$\liv y=3\riv$}}]{i3}{$(p3) + (0.5,-1.5)$};
    \newGaussNode[label={[label distance=-0.1cm]below right:{$A$}},fill=boxorange!80]{g11}{$(p1) + (1,-1)$};
    \newGaussNode[label={[label distance=-0.1cm]below right:{$B$}},fill=boxpink!50]{g12}{$(p1) + (1.5,-0.5)$};
    \newGaussNode[label={[label distance=-0.1cm]below right:{$C$}},fill=boxgoldenrod!70]{g13}{$(p1) + (2,0)$};
    \newGaussNode[label={[label distance=-0.1cm]below right:{$A$}},fill=boxorange!80]{g21}{$(p2) + (1,-1)$};
    \newGaussNode[label={[label distance=-0.1cm]below right:{$B$}},fill=boxpink!50]{g22}{$(p2) + (1.5,-0.5)$};
    \newGaussNode[label={[label distance=-0.1cm]below right:{$C$}},fill=boxgoldenrod!70]{g23}{$(p2) + (2,0)$};
    \newGaussNode[label={[label distance=-0.1cm]below right:{$A$}},fill=boxorange!80]{g31}{$(p3) + (1,-1)$};
    \newGaussNode[label={[label distance=-0.1cm]below right:{$B$}},fill=boxpink!50]{g32}{$(p3) + (1.5,-0.5)$};
    \newGaussNode[label={[label distance=-0.1cm]below right:{$A$}},fill=boxgoldenrod!70]{g33}{$(p3) + (2,0)$};
    \draw[edge] (r) edge node[midway,below left] {\scriptsize$p(y=1)$} (p1);
    \draw[edge] (r) edge node[midway,above,xshift=0.2cm,yshift=0.1cm] {\scriptsize$p(y=2)$} (p2);
    \draw[edge] (r) edge node[midway,above left] {\scriptsize$p(y=3)$} (p3);
    \draw[edge] (p1) edge (i1);
    \draw[edge] (p2) edge (i2);
    \draw[edge] (p3) edge (i3);
    \draw[edge] (p1) -- (g11);
    \draw[edge] (p1) -- (g12);
    \draw[edge] (p1) -- (g13);
    \draw[edge] (p2) -- (g21);
    \draw[edge] (p2) -- (g22);
    \draw[edge] (p2) -- (g23);
    \draw[edge] (p3) -- (g31);
    \draw[edge] (p3) -- (g32);
    \draw[edge] (p3) -- (g33);
  \end{tikzpicture}

  \vskip -0.25cm
  Equivalent PC
  \end{center}
\end{example}

Although we have only covered the most basic queries so far, more complex tasks involving
information-theoretic measures, logical queries or distributional divergences are also (tractably)
computable in PCs under the right conditions. Particularly, we are interested in a key component
for tractability in all these tasks: the notion of \emph{vtrees} and \emph{structured
decomposability}, a stronger variant of decomposability where variable partitionings on product
nodes follow a hierarchy. This hierarchy is easily visualized through a \emph{vtree} (variable
tree), a data structure that defines a (partial) ordering of variables.

\begin{definition}[Vtree]
  A variable tree $\vtree=(\mathcal{B},\phi)$, or \emph{vtree}, over a set of variables $\set{X}$
  is a pair made out of a binary tree $\mathcal{B}$ whose number of leaf nodes is $|\set{X}|$, and
  a one-to-one and onto mapping $\phi$ of leaves of $\mathcal{B}$ with variables $\set{X}$.
\end{definition}

We shall adopt the same scope definition and notation $\Sc(\cdot)$ for vtrees as in PCs. Let $v$ be
a vtree node from a vtree $\vtree$. If $v$ is a leaf node of $\vtree$, its scope is
$\phi_{\vtree}(v)$, i.e.\ the leaf's assigned variable; otherwise its scope is the union of the
scope of its children. For an inner node $v$, we shall call its left child $\lch{v}$ and right
child $\rch{v}$. Every inner node $v$ of a vtree $\vtree$ defines a variable \emph{partitioning} of
the scope $(\Sc(\lch{v}), \Sc(\rch{v}))$, while the leaves of $\vtree$ define a partial ordering of
$\Sc(\vtree)$. We are especially interested in the scope partitioning aspect of vtrees.

\begin{definition}
  A product node $\Prod$ \emph{respects} a vtree node $v$ if $\Prod$ contains only two children
  $\Ch(\Prod)=\{\Child_1,\Child_2\}$, and $\Sc(\Child_1)=\Sc(\lch{v})$ and $\Sc(\Child_2)=
  \Sc(\rch{v})$.
\end{definition}

Obviously, the above definition is vague with regards to which child (e.g.\ graphically, left or
right) of $\Prod$ should respect the scope of which $v$ child. We therefore assume a fixed order
for $\Prod$'s children and say that \textbf{the (graphically) left child is called the \emph{prime}
and (graphically) right child the \emph{sub}}, and refer to $\Prod$ as an \emph{element}. This
ultimately means that the scope of the prime (resp. sub) of $\Prod$ must be the same as the scope
of the left (resp. right) child of $v$. Although the graphical concept of left and right is needed
for easily visualizing the scope partitioning of a product with respect to a vtree node, we do not
use it strictly. In fact, when the situation is unambiguous, we compactly represent the
computational graph without adhering to the left-right convention in favor of readability.

\begin{figure}[t]
  \begin{subfigure}[t]{0.25\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newVtreeNode{r}{0,0}{1};
      \newVtreeNode{ab}{$(r) + (-1.0,-1.5)$}{2};
      \newVtreeNode{cd}{$(r) + (1.0,-1.5)$}{3};
      \newVtreeNode[fill=boxteal]{a}{$(ab) + (-0.5,-1.5)$}{$A$};
      \newVtreeNode[fill=boxorange!80]{b}{$(ab) + (0.5,-1.5)$}{$B$};
      \newVtreeNode[fill=boxpink!50]{c}{$(cd) + (-0.5,-1.5)$}{$C$};
      \newVtreeNode[fill=boxgoldenrod!70]{d}{$(cd) + (0.5,-1.5)$}{$D$};
      \draw (r) -- (ab); \draw (r) -- (cd);
      \draw (ab) -- (a); \draw (ab) -- (b);
      \draw (cd) -- (c); \draw (cd) -- (d);
      \node at ($(b) + (0,-0.5)$) {};
    \end{tikzpicture}
    }
    \caption{}
    \label{fig:vtree}
  \end{subfigure}
  \begin{subfigure}[t]{0.35\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newSumNode[fill=boxgreen]{r}{0,0};
      \newProdNode[fill=boxred!70]{p1}{$(r) + (-1.5,-1)$};
      \newProdNode[fill=boxred!70]{p2}{$(r) + (1.5,-1)$};
      \newSumNode[fill=boxpurple!60]{s1}{$(r) + (-2,-2)$};
      \newSumNode[fill=boxpurple!60]{s2}{$(r) + (0,-2)$};
      \newSumNode[fill=boxpurple!60]{s3}{$(r) + (2,-2)$};
      \newProdNode[fill=boxbrown!60]{q1}{$(s1) + (-0.5,-1)$};
      \newProdNode[fill=boxbrown!60]{q2}{$(s1) + (0.5,-1)$};
      \newProdNode[fill=boxbrown!60]{q3}{$(s2) + (-0.5,-1)$};
      \newProdNode[fill=boxbrown!60]{q4}{$(s2) + (0.5,-1)$};
      \newProdNode[fill=boxbrown!60]{q5}{$(s3) + (-0.5,-1)$};
      \newProdNode[fill=boxbrown!60]{q6}{$(s3) + (0.5,-1)$};
      \newGaussNode[label=below:{$A$},fill=boxteal]{a1}{$(q1) + (0,-1)$};
      \newGaussNode[label=below:{$B$},fill=boxorange!80]{b1}{$(q2) + (0,-1)$};
      \newGaussNode[label=below:{$C$},fill=boxpink!50]{c}{$(q3) + (0,-1)$};
      \newGaussNode[label=below:{$D$},fill=boxgoldenrod!70]{d}{$(q4) + (0,-1)$};
      \newGaussNode[label=below:{$A$},fill=boxteal]{a2}{$(q5) + (0,-1)$};
      \newGaussNode[label=below:{$B$},fill=boxorange!80]{b2}{$(q6) + (0,-1)$};
      \draw[edge] (r) -- (p1); \draw[edge] (r) -- (p2);
      \draw[edge] (p1) -- (s1); \draw[edge] (p1) -- (s2); \draw[edge] (p2) -- (s2); \draw[edge] (p2) -- (s3);
      \draw[edge] (s1) -- (q1); \draw[edge] (s1) -- (q2);
      \draw[edge] (s2) -- (q3); \draw[edge] (s2) -- (q4);
      \draw[edge] (s3) -- (q5); \draw[edge] (s3) -- (q6);
      \draw[edge] (q1) -- (a1); \draw[edge] (q1) -- (b1);
      \draw[edge] (q2) -- (a1); \draw[edge] (q2) -- (b1);
      \draw[edge] (q3) -- (c); \draw[edge] (q3) -- (d);
      \draw[edge] (q4) -- (c); \draw[edge] (q4) -- (d);
      \draw[edge] (q5) -- (a2); \draw[edge] (q5) -- (b2);
      \draw[edge] (q6) -- (a2); \draw[edge] (q6) -- (b2);
    \end{tikzpicture}
    }
    \caption{}
    \label{fig:respect}
  \end{subfigure}
  \begin{subfigure}[t]{0.35\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newSumNode[fill=boxgreen]{r}{0,0};
      \newProdNode[fill=boxred!70]{p1}{$(r) + (-1.5,-0.8)$};
      \newProdNode[fill=boxred!70]{p2}{$(r) + (1.5,-0.8)$};
      \newGaussNode[label=below:{$A$},fill=boxteal]{a1}{$(p1) + (-1,-0.8)$};
      \newSumNode[fill=boxpurple!60]{s1}{$(p1) + (0.5,-0.8)$};
      \newSumNode[fill=boxpurple!60]{s2}{$(p2) + (-0.5,-0.8)$};
      \newGaussNode[label=below:{$D$},fill=boxgoldenrod!70]{d1}{$(p2) + (1,-0.8)$};
      \newProdNode[fill=boxbrown!60]{q1}{$(s1) + (-0.75,-0.8)$};
      \newProdNode[fill=boxbrown!60]{q2}{$(s1) + (0.5,-0.8)$};
      \newProdNode[fill=boxbrown!60]{q3}{$(s2) + (-0.5,-0.8)$};
      \newProdNode[fill=boxbrown!60]{q4}{$(s2) + (0.75,-0.8)$};
      \newGaussNode[label=below:{$C$},fill=boxpink!50]{c1}{$(q1) + (-0.65,-0.8)$};
      \newGaussNode[label=below:{$C$},fill=boxpink!50]{c2}{$(q2) + (0.6,-0.8)$};
      \newGaussNode[label=below:{$C$},fill=boxpink!50]{c3}{$(q4) + (0.65,-0.8)$};
      \newSumNode[fill=boxblue!80]{z1}{$(q1) + (0.75,-0.8)$};
      \newSumNode[fill=boxblue!80]{z2}{$(q3) + (0.75,-0.8)$};
      \newProdNode[fill=boxgray!80]{t1}{$(z1) + (-0.5,-0.8)$};
      \newProdNode[fill=boxgray!80]{t2}{$(z1) + (0.5,-0.8)$};
      \newProdNode[fill=boxgray!80]{t3}{$(z2) + (-0.25,-0.8)$};
      \newProdNode[fill=boxgray!80]{t4}{$(z2) + (0.5,-0.8)$};
      \newGaussNode[label=below:{$D$},fill=boxgoldenrod!70]{d2}{$(t1) + (0,-0.8)$};
      \newGaussNode[label=below:{$B$},fill=boxorange!80]{b}{$(c2) + (0,-1.6)$};
      \newGaussNode[label=below:{$A$},fill=boxteal]{a2}{$(t4) + (0,-0.8)$};
      \draw[edge] (r) -- (p1); \draw[edge] (r) -- (p2);
      \draw[edge] (p1) -- (a1); \draw[edge] (p1) -- (s1);
      \draw[edge] (p2) -- (s2); \draw[edge] (p2) -- (d1);
      \draw[edge] (s1) -- (q1); \draw[edge] (s1) -- (q2);
      \draw[edge] (s2) -- (q3); \draw[edge] (s2) -- (q4);
      \draw[edge] (q1) -- (c1); \draw[edge] (q1) -- (z1);
      \draw[edge] (q2) -- (z1); \draw[edge] (q2) -- (c2);
      \draw[edge] (q3) -- (c2); \draw[edge] (q3) -- (z2);
      \draw[edge] (q4) -- (z2); \draw[edge] (q4) -- (c3);
      \draw[edge] (z1) -- (t1); \draw[edge] (z1) -- (t2);
      \draw[edge] (z2) -- (t3); \draw[edge] (z2) -- (t4);
      \draw[edge] (t1) -- (d2); \draw[edge] (t1) -- (b);
      \draw[edge] (t2) -- (d2); \draw[edge] (t2) -- (b);
      \draw[edge] (t3) -- (b); \draw[edge] (t3) -- (a2);
      \draw[edge] (t4) -- (b); \draw[edge] (t4) -- (a2);
    \end{tikzpicture}
    }
    \caption{}
    \label{fig:norespect}
  \end{subfigure}
  \caption{A vtree \uncaption{(a)} defining an order $(A,B,C,D)$, a 2-standard structured
    decomposable probabilistic circuit that respects the vtree \uncaption{(b)}, and a 2-standard
    decomposable probabilistic circuit that does not \uncaption{(c)}.}
  \label{fig:vtreeresp}
\end{figure}

We say that a vtree is linear, if either it is left-linear or right-linear. A left- (resp. right)
linear vtree is a vtree whose inner nodes all have leaf nodes on their right (resp. left) child.
Similarly, a vtree is said to be left- (resp. right) leaning if the number of leaf nodes as right
(resp. left) children is much higher then left (resp. right) children. Otherwise, it is a balanced
vtree. The variable order of a vtree is the sequence of leaf nodes (i.e.\ variables) read from left
to right. \Cref{fig:vtree} shows a balanced vtree with order $(A,B,C,D)$.

Now that we understand what a vtree is, we can properly introduce \emph{structured decomposability},
a stronger variant of decomposability. We say that a PC is \emph{2-standard} if it is standard and
all of its product nodes have exactly two children. Further, we call the $i$-th layer of a PC or a
vtree as the set of all nodes that are at depth $i$ (i.e.\ the shortest connected path from the
root to the node has size $i$).

\begin{definition}[Structured decomposability]
  \label{def:sdec}%
  Let $\mathcal{C}$ be a 2-standard probabilistic circuit and $\vtree$ a vtree with same scope as
  $\mathcal{C}$. $\mathcal{C}$ is said to be \emph{structured decomposable} if every $i$-th product
  layer of $\mathcal{C}$ respects every $i$-th inner node layer of $\vtree$.
\end{definition}

Although we assume a 2-standard PC in \Cref{def:sdec}, this assumption was only for convenience,
and does not imply in a loss of expressivity; as a matter of fact, any PC can be 2-standardized
(see \Cref{thm:2standard}). Intuitively, structured decomposability merely states that every two
product nodes whose scopes are the same must partition their scopes (between their two children)
exactly the same (and according to their corresponding vtree node). Semantically speaking, a
vtree's inner node $v$ defines a context-specific independence relationship between $\Sc(v^\gets)$
and $\Sc(v^\to)$ under the distribution encoded by its PC.

\Cref{fig:vtreeresp} shows a vtree $\vtree$ and two probabilistic circuits, say
$\mathcal{C}_1$ for the one in the middle and $\mathcal{C}_2$ for the one on the right. Notice how
$\mathcal{C}_1$ respects $\vtree$, as each \inode[fill=boxred!70]{\newProdNode} respects the split
at vtree node $1$ (namely $\{A,B\}$). The primes are then \inode[fill=boxpurple!60]{\newSumNode}
whose scopes are $\{A,B\}$, while the sub is the one with two parents and scope $\{C,D\}$. For each
of these, their children \inode[fill=boxbrown!60]{\newProdNode} also respect $\vtree$: they either
encode the same split as $2$ or as $3$, depending on whether they are descendants from the sub or
prime of $1$. Although $\mathcal{C}_2$ is decomposable, it does \emph{not} respect $\vtree$, as
\inode[fill=boxred!70]{\newProdNode} encode different variable partionings: $(\{A\},\{B,C,D\})$ and
$(\{A,B,C\},\{D\})$. In fact, it is not structured decomposable, as it does not respect any vtree.
\Cref{eg:hmm} shows a Hidden Markov model as a smooth, deterministic and structured decomposable PC
whose sums describe the latent variables and products partition observable variables according to
the vtree.

Despite our structured decomposability definition relying on a vtree, there is at least one
alternative definition that defines it in terms of \emph{circuit compatibility}. Essentially, a
circuit $\mathcal{C}_1$ is \emph{compatible} with $\mathcal{C}_2$ if they can be 2-standardized (in
polynomial time) in such a way that any two products with same scope, one from $\mathcal{C}_1$ and
the other $\mathcal{C}_2$, partition the scope into the same decompositions \citep{vergari21}. A
structured decomposable PC is then defined as a PC that is compatible with a copy of itself. In
summary, the two definitions of structured decomposability are equivalent, except compatibility
implicitly assumes an arrangement of product scopes that is analogous to a vtree.

Probabilistic circuits appear in literature under many names. A PC which is both smooth and
decomposable is often referred to as a Sum-Product Network (SPN, \cite{poon11}), although
definitions vary around the presence of the two structural constraints as a requirement, with SPNs
sometimes used as a synonym for probabilistic circuits. Smooth, decomposable and deterministic PCs
often appear as Arithmetic Circuits (ACs, \cite{darwiche03}) or Cutset Networks (CNets,
\cite{rahman14}). Probabilistic Sentential Decision Diagrams (PSDDs, \cite{kisa14}), Probabilistic
Decision Graphs (PDGs, \cite{jaeger04}) and And/Or-Graphs (AOGs, \cite{dechter07}) can all be
described as smooth, deterministic and structured decomposable PCs.

\begin{example}[sidebyside,lefthand width=0.55\textwidth]{Hidden Markov models as probabilistic circuits}{hmm}
  Say we wish to model a sequential structured dependence between three latent binary variables, for
  example the presence of a subject $X_1$, verb $X_2$ and object $X_3$ in a natural language
  phrase. Each observation $Y_i$ is a fragment (of $X_i$) taken from a complete sentence
  $\set{y}=(y_1,y_2,y_3)$. The first-order Hidden Markov Model (HMM) (on the right) models the
  joint probability of sentences
  \begin{equation}
    p(X_{1..3},Y_{1..3})=p(X_1)\prod_{i=2}^3 p(X_i|X_{i-1})\prod_{i=1}^3 p(Y_i|X_i).
  \end{equation}
  This is computationally equivalent to the PC on the right. Each input node $p(Y_i|X_i)$ is a
  conditional distribution model (possibly another PC) for each assignment (here two) of $X_i$,
  meaning that if $p(Y_i|X_i=0)>0$, then $p(Y_i|X_i=1)=0$ and vice-versa. Root weights are exactly
  $p(X_1)$, and each $p(X_i|X_{i-1})$ translates into the other matched color sum weights. Further,
  every product follows the partitionings imposed by the vtree, with
  \inode[fill=boxgray!80]{\newProdNode} decomposing into $(\emptyset,\{Y_3\})$. This means that
  this PC is not only smooth, but structured decomposable and deterministic.
  \tcblower
  \begin{center}
    \begin{tikzpicture}
      \node[inner sep=0pt,thick,minimum size=12pt,draw,circle,fill=boxgreen] (x1) at (0,0) {$X_1$};
      \node[inner sep=0pt,thick,minimum size=12pt,draw,circle,fill=boxpurple!60] (x2) at ($(x1) + (1,0)$) {$X_2$};
      \node[inner sep=0pt,thick,minimum size=12pt,draw,circle,fill=boxblue!80] (x3) at ($(x2) + (1,0)$) {$X_3$};
      \node[inner sep=0pt,thick,minimum size=12pt,draw,circle,fill=boxorange!80] (y1) at ($(x1) + (0,-1)$) {$Y_1$};
      \node[inner sep=0pt,thick,minimum size=12pt,draw,circle,fill=boxpink!50] (y2) at ($(x2) + (0,-1)$) {$Y_2$};
      \node[inner sep=0pt,thick,minimum size=12pt,draw,circle,fill=boxgoldenrod!70] (y3) at ($(x3) + (0,-1)$) {$Y_3$};
      \draw[edge] (x1) -- (y1); \draw[edge] (x1) -- (x2);
      \draw[edge] (x2) -- (y2); \draw[edge] (x2) -- (x3);
      \draw[edge] (x3) -- (y3);
    \end{tikzpicture}

    \vskip -0.25cm
    \small%
    Hidden Markov Model

    \begin{tikzpicture}
      \newSumNode[fill=boxgreen]{r}{0,0};
      \newProdNode[fill=boxred!70]{p1}{$(r) + (-0.5,-1)$};
      \newProdNode[fill=boxred!70]{p2}{$(r) + (0.5,-1)$};
      \newGaussNode[label=right:{$p(Y_1|X_1=0)$},fill=boxorange!80]{x11}{$(p2) + (1,0.75)$};
      \newGaussNode[label=right:{$p(Y_1|X_1=1)$},fill=boxorange!80]{x12}{$(p2) + (1,0.0)$};
      \newSumNode[fill=boxpurple!60]{s1}{$(p1) + (0,-1)$};
      \newSumNode[fill=boxpurple!60]{s2}{$(p2) + (0,-1)$};
      \newProdNode[fill=boxbrown!60]{q1}{$(s1) + (0,-0.8)$};
      \newProdNode[fill=boxbrown!60]{q2}{$(s2) + (0,-0.8)$};
      \newGaussNode[label=right:{$p(Y_2|X_2=0)$},fill=boxpink!50]{x21}{$(q2) + (1,0.75)$};
      \newGaussNode[label=right:{$p(Y_2|X_2=1)$},fill=boxpink!50]{x22}{$(q2) + (1,0.0)$};
      \newSumNode[fill=boxblue!80]{z1}{$(q1) + (0,-1)$};
      \newSumNode[fill=boxblue!80]{z2}{$(q2) + (0,-1)$};
      \newProdNode[fill=boxgray!80]{t1}{$(z1) + (0,-0.8)$};
      \newProdNode[fill=boxgray!80]{t2}{$(z2) + (0,-0.8)$};
      \newGaussNode[label=right:{$p(Y_3|X_3=0)$},fill=boxgoldenrod!70]{x31}{$(t2) + (1,0.75)$};
      \newGaussNode[label=right:{$p(Y_3|X_3=1)$},fill=boxgoldenrod!70]{x32}{$(t2) + (1,0.0)$};
      \draw[edge] (r) -- (p1); \draw[edge] (r) -- (p2);
      \draw[edge] (p1) -- (x11); \draw[edge] (p2) -- (x12);
      \draw[edge] (p1) -- (s1); \draw[edge] (p2) -- (s2);
      \draw[edge] (s1) -- (q1); \draw[edge] (s1) -- (q2);
      \draw[edge] (s2) -- (q1); \draw[edge] (s2) -- (q2);
      \draw[edge] (q1) -- (x21); \draw[edge] (q2) -- (x22);
      \draw[edge] (q1) -- (z1); \draw[edge] (q2) -- (z2);
      \draw[edge] (z1) -- (t1); \draw[edge] (z1) -- (t2);
      \draw[edge] (z2) -- (t1); \draw[edge] (z2) -- (t2);
      \draw[edge] (t1) -- (x31); \draw[edge] (t2) -- (x32);
    \end{tikzpicture}

    \vskip -0.25cm
    Equivalent PC

    \begin{tikzpicture}
      \newVtreeNode[fill=boxred!70]{r}{0,0}{1};
      \newVtreeNode[fill=boxbrown!60]{l}{$(r) + (-0.75,-1)$}{2};
      \newVtreeNode[fill=boxorange!80]{x1}{$(r) + (0.75,-1)$}{$Y_1$};
      \newVtreeNode[fill=boxpink!50]{x2}{$(l) + (-0.75,-1)$}{$Y_2$};
      \newVtreeNode[fill=boxgoldenrod!70]{x3}{$(l) + (0.75,-1)$}{$Y_3$};
      \draw (r) -- (l); \draw (r) -- (x1);
      \draw (l) -- (x2); \draw (l) -- (x3);
    \end{tikzpicture}

    \vskip -0.25cm
    Vtree
  \end{center}
\end{example}

The notion of structured decomposability (or compatibility for that matter) is key to more complex
queries. For instance, given two probabilistic circuits $\mathcal{C}_1$ and $\mathcal{C}_2$,
computing cross entropy between the two is $\bigo\left(|\mathcal{C}_1||\mathcal{C}_2|\right)$ as
long as both have the same vtree and the circuit that needs to come inside the $\log$ is also
deterministic. Likewise, computing the Kullback-Leibler (KL) divergence between $\mathcal{C}_1$ and
$\mathcal{C}_2$ requires that the two share the same vtree and both be deterministic. Mutual
Information (MI), in turn, calls for the circuit to be smooth, structured decomposable and an even
stronger version of determinism known as \emph{marginal determinism} where sums can only have one
nonnegative valued child for any \emph{partial} assignment at a time. In fact, when a PC is smooth,
decomposable and marginal deterministic, marginal \map{}, i.e. \map{} over partial assignments
becomes linear time computable. For a more detailed insight on the tractability of these (and
other) queries, as well as proofs on these results, we point to the comprehensive study of
\citet{vergari21}.

A particularly interesting class of queries that becomes tractable when circuits are structured
decomposable is the expectation (\expc{}) of a circuit with respect to another \citep{choi20},
defined as
\begin{equation}
  \mathbb{E}_\mathcal{C}\left[\mathcal{S}\right]=\int_{\set{x}}\mathcal{C}(\set{x})\mathcal{S}(\set{x})\dif\set{x}.
  \label{eq:exp}
\end{equation}
One notable example from this class is computing the probability of logical events, which in terms
of a \expc{} query amounts to computing the probability of $\mathcal{C}$ when circuit $\mathcal{S}$
represents a given logical query. This leads us to logic circuits, a parallel version of
probabilistic circuits for logical reasoning which we shall see next.

\section{Probabilistic Circuits as Knowledge Bases}
\label{sec:pckb}

We superficially mentioned in \Cref{rem:optract} that PCs under a Boolean semiring with
conjunctions and disjunctions as operators are known as Logic Circuits (LCs). In this section, we
formally yet briefly define LCs and more precisely show the connection between PCs and LCs.

\subsection{From Certainty...}
\label{sec:fromcertainty}

Logic circuits are computational graphs just like PCs, but whose input are always Booleans (and as
such the scope is over propositional variables) and computational units define either a
conjunction, disjunction or literal of their inputs. While the computational graph in PCs encodes
uncertainty as a probability distribution, in LCs their computational graph encodes certain
knowledge as a propositional language. Similar to PCs, computing the satisfiability of an
assignment is done by a bottom-up feedforward evaluation of the circuit. In terms of notation, we
shall use \inode{\newOrNode} for disjunction nodes, \inode{\newAndNode} for conjunction nodes, and
$X$ and $\neg X$ for literal nodes.

\begin{figure}[t]
  \begin{subfigure}[t]{0.45\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newNamedOrNode[inputs=nn,fill=boxgreen]{r}{0,0}{1};
      \newNamedAndNode[inputs=nn,fill=boxred!70]{p1}{$(r) + (-1,-1)$}{0};
      \newNamedAndNode[inputs=nn,fill=boxred!70]{p2}{$(r) + (1,-1)$}{1};
      \newNamedOrNode[inputs=nn,fill=boxpurple!60]{s1}{$(p1) + (-1.5,-1)$}{1};
      \newNamedOrNode[inputs=nn,fill=boxpurple!60]{s2}{$(p2) + (1.5,-1)$}{1};
      \newNamedAndNode[inputs=nn,fill=boxblue!80]{q1}{$(s1) + (-0.5,-1)$}{0};
      \newNamedAndNode[inputs=nn,fill=boxblue!80]{q2}{$(s1) + (0.5,-1)$}{1};
      \newNamedAndNode[inputs=nn,fill=boxblue!80]{q3}{$(s1) + (1.75,-1)$}{1};
      \newNamedAndNode[inputs=nn,fill=boxblue!80]{q4}{$(s2) + (-1.75,-1)$}{0};
      \newNamedAndNode[inputs=nn,fill=boxblue!80]{q5}{$(s2) + (-0.5,-1)$}{1};
      \newNamedAndNode[inputs=nn,fill=boxblue!80]{q6}{$(s2) + (0.5,-1)$}{0};
      \node (c) at ($(q1) + (-0.5,-1)$) {$C$};
      \newNamedOrNode[inputs=nn,fill=boxorange!80]{z1}{$(q1) + (0.5,-1.5)$}{1};
      \node (nc1) at ($(q2) + (0.5,-1)$) {$\neg C$};
      \node (nc2) at ($(q3) + (0.5,-1)$) {$\neg C$};
      \node (a1) at ($(q4) + (-0.5,-1)$) {$A$};
      \node (a2) at ($(q5) + (-0.5,-1)$) {$A$};
      \node (nb1) at ($(q5) + (0.3,-1)$) {$\neg B$};
      \newNamedOrNode[inputs=nn,fill=boxorange!80]{z2}{$(q6.input 1) + (0,-1.25)$}{1};
      \node (na) at ($(q6) + (0.75,-1)$) {$\neg A$};
      \node (nd) at ($(z1) + (-0.5,-1)$) {$\neg D$};
      \node (d) at ($(z1) + (1.5,-1)$) {$D$};
      \node (b) at ($(z2) + (-2.0,-1)$) {$B$};
      \node (nb2) at ($(z2) + (0.5,-1)$) {$\neg B$};
      \draw[edge] (r.input 1) -- ++(0,-0.2) -| (p1);
      \draw[edge] (r.input 2) -- ++(0,-0.2) -| (p2);
      \draw[edge] (p1.input 1) -- ++(0,-0.2) -| (s1.east);
      \draw[edge] (p2.input 2) -- ++(0,-0.2) -| (s2.east);
      \draw[edge] (p1.input 2) -- ++(0,-0.4) -| (q4);
      \draw[edge] (p2.input 1) -- ++(0,-0.7) -| (q3);
      \draw[edge] (s1.input 1) -- ++(0,-0.2) -| (q1);
      \draw[edge] (s1.input 2) -- ++(0,-0.2) -| (q2);
      \draw[edge] (s2.input 1) -- ++(0,-0.2) -| (q5);
      \draw[edge] (s2.input 2) -- ++(0,-0.2) -| (q6);
      \draw[edge] (q1.input 1) -- ++(0,-0.2) -| (c);
      \draw[edge] (q1.input 2) -- ++(0,-0.2) -| (z1.east);
      \draw[edge] (q2.input 1) -- ++(0,-0.2) -| (z1.east);
      \draw[edge] (q2.input 2) -- ++(0,-0.2) -| (nc1);
      \draw[edge] (q3.input 1) -- ++(0,-0.2) -| (d);
      \draw[edge] (q3.input 2) -- ++(0,-0.2) -| (nc2);
      \draw[edge] (q4.input 1) -- ++(0,-0.2) -| (a1);
      \draw[edge] (q4.input 2) -- ++(0,-0.2) -| (b);
      \draw[edge] (q5.input 1) -- ++(0,-0.2) -| (a2);
      \draw[edge] (q5.input 2) -- ++(0,-0.2) -| (nb1);
      \draw[edge] (q6.input 1) -- (z2.east);
      \draw[edge] (q6.input 2) -- ++(0,-0.2) -| (na);
      \draw[edge] (z1.input 2) -- ++(0,-0.2) -| (d);
      \draw[edge] (z1.input 1) -- ++(0,-0.2) -| (nd);
      \draw[edge] (z2.input 1) -- ++(0,-0.2) -| (b);
      \draw[edge] (z2.input 2) -- ++(0,-0.2) -| (nb2);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (r.input 1) -- ++(0,-0.15) -| (p1);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (r.input 2) -- ++(0,-0.15) -| (p2);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (p1.input 1) -- ++(0,-0.15) -| (s1.east);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (p2.input 2) -- ++(0,-0.15) -| (s2.east);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (p1.input 2) -- ++(0,-0.35) -| (q4);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (p2.input 1) -- ++(0,-0.65) -| (q3);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (s1.input 1) -- ++(0,-0.15) -| (q1);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (s1.input 2) -- ++(0,-0.15) -| (q2);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (s2.input 1) -- ++(0,-0.15) -| (q5);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (s2.input 2) -- ++(0,-0.15) -| (q6);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (q1.input 1) -- ++(0,-0.15) -| (c);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (q1.input 2) -- ++(0,-0.15) -| (z1.east);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (q2.input 1) -- ++(0,-0.15) -| (z1.east);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (q2.input 2) -- ++(0,-0.15) -| (nc1);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (q3.input 1) -- ++(0,-0.15) -| (d);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (q3.input 2) -- ++(0,-0.15) -| (nc2);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (q4.input 1) -- ++(0,-0.15) -| (a1);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (q4.input 2) -- ++(0,-0.15) -| (b);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (q5.input 1) -- ++(0,-0.15) -| (a2);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (q5.input 2) -- ++(0,-0.15) -| (nb1);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (q6.input 1) -- (z2.east);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (q6.input 2) -- ++(0,-0.15) -| (na);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (z1.input 2) -- ++(0,-0.15) -| (d);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (z1.input 1) -- ++(0,-0.15) -| (nd);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=-2pt}] (z2.input 1) -- ++(0,-0.15) -| (b);
      \draw[<-,>=latex',boxdgray,transform canvas={xshift=2pt}]  (z2.input 2) -- ++(0,-0.15) -| (nb2);
      \node at ($(r) + (0,-6.5)$) {\colorbox{boxpink!50}{\color{white}$\mathbf{A=1\quad B=0\quad C=0\quad D=1}$}};
      \node (out) at ($(r) + (0,1.5)$) {\colorbox{boxgreen}{\color{white}$\mathbf{\bm{\phi}(x)=1}$}};
      \draw[edge,boxdgray] (r.east) -- (out);
    \end{tikzpicture}
    }
    \caption{}
    \label{fig:lc1}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newOrNode[inputs=nn,fill=boxgreen]{r}{0,0};
      \newAndNode[inputs=nn,fill=boxred!70]{p1}{$(r) + (-1.3,-1)$};
      \newAndNode[inputs=nn,fill=boxred!70]{p2}{$(r) + (1.3,-1.5)$};
      \node (a) at ($(p1) + (-1,-1)$) {$A$};
      \newOrNode[inputs=nn,fill=boxpurple!60]{s1}{$(p1.input 2) + (0,-0.75)$};
      \node (na) at ($(p2) + (-0.5,-1.5)$) {$\neg A$};
      \newAndNode[inputs=nn,fill=boxpurple!60]{s2}{$(p2) + (0.5,-1.5)$};
      \newAndNode[inputs=nn,fill=boxblue!80]{q1}{$(s1) + (-0.75,-1)$};
      \newAndNode[inputs=nn,fill=boxblue!80]{q2}{$(s1) + (0.75,-1)$};
      \newAndNode[inputs=nn,fill=boxblue!80]{q3}{$(s2) + (-0.75,-1.5)$};
      \newOrNode[inputs=nn,fill=boxblue!80]{q4}{$(s2) + (0.75,-1.5)$};
      \newOrNode[inputs=nn,fill=boxorange!80]{z1}{$(q1.input 1) + (0,-0.75)$};
      \node (b1) at ($(q1) + (0.5,-1)$) {$B$};
      \node (nb1) at ($(q2) + (-0.5,-1)$) {$\neg B$};
      \newAndNode[inputs=nn,fill=boxorange!80]{z2}{$(q2) + (0.5,-1.0)$};
      \node (nc1) at ($(q3) + (-0.5,-1.5)$) {$\neg C$};
      \node (d1) at ($(q3) + (0.5,-1.5)$) {$D$};
      \node (b2) at ($(q4) + (-0.5,-1.5)$) {$B$};
      \node (nb2) at ($(q4) + (0.5,-1.5)$) {$\neg B$};
      \newAndNode[inputs=nn,fill=boxteal]{t1}{$(z1) + (-0.5,-1)$};
      \newAndNode[inputs=nn,fill=boxteal]{t2}{$(z1) + (0.5,-1)$};
      \node (d2) at ($(z2) + (-0.5,-1)$) {$D$};
      \node (nc2) at ($(t1) + (-0.5,-1)$) {$\neg C$};
      \newOrNode[inputs=nn,fill=boxpink!50]{r1}{$(t1) + (0.5,-1)$};
      \node (c) at ($(t2) + (0.5,-1)$) {$C$};
      \node (d3) at ($(r1) + (-0.5,-1)$) {$D$};
      \node (nd) at ($(r1) + (0.5,-1)$) {$\neg D$};
      \draw[edge] (r.input 1) -- ++(0,-0.2) -| (p1);
      \draw[edge] (r.input 2) -- ++(0,-0.2) -| (p2);
      \draw[edge] (p1.input 1) -- ++(0,-0.2) -| (a);
      \draw[edge] (p1.input 2) -- ++(0,-0.2) -| (s1);
      \draw[edge] (p2.input 1) -- ++(0,-0.4) -| (na);
      \draw[edge] (p2.input 2) -- ++(0,-0.4) -| (s2);
      \draw[edge] (s1.input 1) -- ++(0,-0.2) -| (q1);
      \draw[edge] (s1.input 2) -- ++(0,-0.2) -| (q2);
      \draw[edge] (s2.input 1) -- ++(0,-0.4) -| (q3);
      \draw[edge] (s2.input 2) -- ++(0,-0.4) -| (q4);
      \draw[edge] (q1.input 1) -- ++(0,-0.2) -| (z1);
      \draw[edge] (q1.input 2) -- ++(0,-0.2) -| (b1);
      \draw[edge] (q2.input 1) -- ++(0,-0.2) -| (nb1);
      \draw[edge] (q2.input 2) -- ++(0,-0.2) -| (z2);
      \draw[edge] (q3.input 1) -- ++(0,-0.4) -| (nc1);
      \draw[edge] (q3.input 2) -- ++(0,-0.4) -| (d1);
      \draw[edge] (q4.input 1) -- ++(0,-0.4) -| (b2);
      \draw[edge] (q4.input 2) -- ++(0,-0.4) -| (nb2);
      \draw[edge] (z1.input 1) -- ++(0,-0.2) -| (t1);
      \draw[edge] (z1.input 2) -- ++(0,-0.2) -| (t2);
      \draw[edge] (t1.input 1) -- ++(0,-0.2) -| (nc2);
      \draw[edge] (t1.input 2) -- ++(0,-0.2) -| (r1);
      \draw[edge] (t2.input 1) -- ++(0,-0.2) -| (r1);
      \draw[edge] (t2.input 2) -- ++(0,-0.2) -| (c);
      \draw[edge] (r1.input 1) -- ++(0,-0.2) -| (d3);
      \draw[edge] (r1.input 2) -- ++(0,-0.2) -| (nd);
      \draw[edge] (z2.input 1) -- ++(0,-0.2) -| (d2);
      \draw[edge] (z2.input 2) -- ++(0,-0.9) -| (nc1);
    \end{tikzpicture}
    }
    \caption{}
    \label{fig:lc2}
  \end{subfigure}
  \caption{Two smooth, structured decomposable and deterministic logic circuits encoding the same
    logic constraint $\phi\equiv(A\wedge B)\vee(\neg C\wedge D)$ for a balanced \uncaption{(a)} and a
    right-linear \uncaption{(b)} vtree. In \uncaption{(a)}, a circuit evaluation for an assignment,
    with each node value in the bottom-up evaluation pass shown inside nodes.}
  \label{fig:logic}
\end{figure}

\begin{definition}[Logic circuit]
  A logic circuit $\mathcal{L}$ is a rooted connected DAG whose inner nodes compute either a
  conjunction or a disjunction of their children. Nodes with no outgoing edges, i.e.\
  \emph{literal} nodes, are indicator functions of either a positive (true) or negative (false)
  assignment. Computing the satisfiability of a world $\set{x}$ according to $\mathcal{L}$ amounts
  to a bottom-up pass where literal nodes are assigned values consistent with $\set{x}$ and values
  are propagated up to the root.
\end{definition}

Evidently, logic circuits are closely related to probabilistic circuits. The strikingly similar
definitions are not coincidence: much of the literature on PCs have their origins on LCs. In fact,
most structural constraints in PCs are the exact same (up to even their names\footnote{There are
some exceptions. Smoothness is sometimes referred to as \emph{completeness} in PCs, while
determinism has the alternative name of \emph{selectivity}.}) as their LC analogues. In this
dissertation, we are particularly interested in the specific subset of smooth, structured
decomposable and deterministic LCs, known as Sentential Decision Diagrams (SDDs,
\cite{darwiche11}). \Cref{fig:logic} shows two SDDs encoding the same knowledge base but under
different vtrees. The one on the left respects a balanced vtree and the one on the right a
right-linear vtree.

Logic circuits have appeared in computer science literature under many names, more closely to the
knowledge compilation community under the title of Negated Normal Form (NNF)
\citep{darwiche01b,darwiche99}, a superset of other propositional compilation languages such as
Binary Decision Diagrams (BDDs, \cite{bryant86}), Propositional DAGs (PDAGs, \cite{wachter06}),
Sentential Decision Diagrams (SDDs, \cite{darwiche11}), DNFs and CNFs. \Cref{eg:bdd} shows a
smooth, structured decomposable and deterministic logic circuit as a BDD. Although no structural
constraint is required for model verification in logic circuits, the same properties defined in the
past section have come up in LCs to enable other more complex queries like equivalence,
implication, sentential entailment and model counting, as well as transformations such as closed
conditionings, forgetting, and conjunctions and disjunctions of circuits such that some structural
constraint is preserved during transformation \citep{darwiche02}. The succinctness (i.e. expressive
efficiency) of LCs are also impacted by these structural restrictions
\citep{gogic95,papadimitriou94,darwiche02}.  See \citet{darwiche02} and \citet{darwiche20} for more
on logic circuits.

\begin{example}[sidebyside,lefthand width=0.55\textwidth]{BDDs as logic circuits}{bdd}
  A Binary Decision Diagram (BDD, \cite{bryant86}) defines a Boolean function over binary variables
  as a rooted DAG. BDDs are a subset of smooth, structured decomposable and deterministic LCs (i.e.\
  SDDs) whose vtrees are always right-linear.
  \begin{center}
    \begin{tikzpicture}
      \node[draw,inner sep=2mm] (a) at (0,0) {$A$};
      \node[draw,inner sep=2mm] (b1) at (-1.5, -1.0) {$B$};
      \node[draw,inner sep=2mm] (b2) at (1.5, -1.0) {$B$};
      \node[draw,inner sep=2mm] (c) at (0.0, -1.25) {$C$};
      \node (F) at (-1.5, -3.0) {\large$\bot$};
      \node (T) at (1.5, -3.0) {\large$\top$};
      \draw[dashed] (a) -- (b1);
      \draw (a) -- (b2);
      \draw[dashed] (b1) -- (T);
      \draw (b1) -- (F);
      \draw[dashed] (b2) -- (T);
      \draw (b2) -- (c);
      \draw[dashed] (c) -- (F);
      \draw (c) -- (T);
    \end{tikzpicture}
  \end{center}
  In their usual notation, inner nodes are variables and leaves are constants $\bot$ and $\top$.
  Evaluating an assignment $\set{x}\in\{0,1\}^n$ is equivalent to a path from the root to a
  leaf where each variable $X$ determines a decision to go through the dashed line when $x=0$ or
  solid line when $x=1$. If the path ends at a $\top$, the function returns 1, otherwise it must
  end at a $\bot$ and therefore returns 0. The BDD above encodes the following logic formula
  \begin{equation}
    \phi(A,B,C)=(A\vee\neg B)\wedge(\neg B\vee C),
  \end{equation}
  also shown as a truth table on the right, together with a logic circuit that encodes the same
  truth table and its vtree. Conjunctions take the role of products, with each conjunction node
  determining a vtree node's scope partition.
  \tcblower
  \small%
  \begin{center}
    \begin{tabular}{ccc|c}
      \hline
      $A$ & $B$ & $C$ & $\phi(\set{x})$\\
      \hline
      0 & 0 & 0 & 1\\
      1 & 0 & 0 & 1\\
      0 & 1 & 0 & 0\\
      1 & 1 & 0 & 0\\
      0 & 0 & 1 & 1\\
      1 & 0 & 1 & 1\\
      0 & 1 & 1 & 0\\
      1 & 1 & 1 & 1\\
      \hline
    \end{tabular}

    Truth Table

    \begin{tikzpicture}
      \newOrNode[inputs=nn,fill=boxgreen]{r}{0,0};
      \newAndNode[inputs=nn,fill=boxred!70]{p1}{$(r) + (-1,-1)$};
      \newAndNode[inputs=nn,fill=boxred!70]{p2}{$(r) + (1,-1)$};
      \node (a) at ($(p1) + (-0.5,-1)$) {$A$};
      \newOrNode[inputs=nn,fill=boxpurple!60]{s1}{$(p1) + (0.5,-1)$};
      \node (na) at ($(p2) + (0.5,-1)$) {$\neg A$};
      \newAndNode[inputs=nn,fill=boxblue!80]{q1}{$(s1) + (-0.75,-1)$};
      \newAndNode[inputs=nn,fill=boxblue!80]{q2}{$(s1) + (0.75,-1)$};
      \newAndNode[inputs=nn,fill=boxblue!80]{q3}{$(p2.input 1) + (0,-1.75)$};
      \node (b) at ($(q1) + (-0.5,-1)$) {$B$};
      \node (c1) at ($(q1) + (0.5,-1)$) {$C$};
      \newOrNode[inputs=nn,fill=boxorange!80]{z1}{$(q2.input 1) + (0,-0.75)$};
      \node (nb) at ($(q3.input 2) + (0,-0.75)$) {$\neg B$};
      \node (c) at ($(z1) + (-0.5,-1)$) {$C$};
      \node (nc) at ($(z1) + (0.5,-1)$) {$\neg C$};
      \draw[edge] (r.input 1) -- ++(0,-0.2) -| (p1);
      \draw[edge] (r.input 2) -- ++(0,-0.2) -| (p2);
      \draw[edge] (s1.input 1) -- ++(0,-0.2) -| (q1);
      \draw[edge] (s1.input 2) -- ++(0,-0.2) -| (q2);
      \draw[edge] (p1.input 1) -- ++(0,-0.2) -| (a);
      \draw[edge] (p1.input 2) -- ++(0,-0.2) -| (s1);
      \draw[edge] (p2.input 1) -- (q3);
      \draw[edge] (p2.input 2) -- ++(0,-0.2) -| (na);
      \draw[edge] (q1.input 1) -- ++(0,-0.2) -| (b);
      \draw[edge] (q1.input 2) -- ++(0,-0.2) -| (c1);
      \draw[edge] (q2.input 1) -- (z1.east);
      \draw[edge] (q2.input 2) -- (nb.north);
      \draw[edge] (q3.input 1) -- (z1.east);
      \draw[edge] (q3.input 2) -- (nb.north);
      \draw[edge] (z1.input 1) -- ++(0,-0.2) -| (c);
      \draw[edge] (z1.input 2) -- ++(0,-0.2) -| (nc);
    \end{tikzpicture}

    \vskip -0.25cm
    Equivalent LC

    \begin{tikzpicture}
      \newVtreeNode{r}{0,0}{1};
      \newVtreeNode{l}{$(r) + (0.75,-1)$}{2};
      \newVtreeNode{x1}{$(r) + (-0.75,-1)$}{$A$};
      \newVtreeNode{x2}{$(l) + (-0.75,-1)$}{$B$};
      \newVtreeNode{x3}{$(l) + (0.75,-1)$}{$C$};
      \draw (r) -- (l); \draw (r) -- (x1);
      \draw (l) -- (x2); \draw (l) -- (x3);
    \end{tikzpicture}

    \skip -0.35cm
    Vtree
  \end{center}
\end{example}

\subsection{...to Uncertainty}
\label{sec:touncertainty}

Logic circuits are easily extensible to probabilistic circuits. In fact, if we think of an LC as
the support of a PC the connections between the two come naturally. Suppose a 2-standard smooth,
structured decomposable and deterministic probabilistic circuit $\mathcal{C}$ over binary variables.
We can construct an identically structured logic circuit (up to input nodes) $\mathcal{L}$ with
same vtree as $\mathcal{C}$ whose underlying Boolean function encodes $\phi(\set{x})=
\left\liv\mathcal{C}(\set{x})>0\right\riv$. Since sums act exactly like disjunctions and products
like conjunctions under the Boolean semiring, products in $\mathcal{C}$ are replaced with
conjunctions in $\mathcal{L}$, and sums with disjunctions. Input nodes from $\mathcal{C}$ are
replaced with a literal node if the function is degenerate, or with a disjunction over positive and
negative literals otherwise. This makes sure $\mathcal{L}$ acts as the support of $\mathcal{C}$, as
each disjunction node $\Sum_{\mathcal{L}}$ of $\mathcal{L}$ defines
\begin{equation}
  \Sum_\mathcal{L}(\set{x})=\bigvee_{\Child\in\Ch_{\mathcal{C}}(\Sum_\mathcal{L})}\left\liv
  \Child_p(\set{x})>0\right\riv\wedge\left\liv\Child_s(\set{x})>0\right\riv,
\end{equation}
where $\Ch_{\mathcal{C}}(\Sum)$ retrieves the children of $\Sum$'s corresponding sum node in
$\mathcal{C}$, with $\Child_p(\set{x})$ and $\Child_s(\set{x})$ the probabilities of $\Child$'s
prime and sub respectively. The corresponding sum node $\Sum_{\mathcal{C}}$ in $\mathcal{C}$ then
only attributes a weight (i.e.\ probability) to each positive element as usual
\begin{equation}
  \Sum_\mathcal{C}(\set{x},\set{y})=\sum_{\Child\in\Ch(\Sum_\mathcal{C})}w_{\Sum_\mathcal{C},\Child}\cdot
  \Child_p(\set{x})\cdot\Child_s(\set{y}).
\end{equation}
When a deterministic sum (resp. disjunction) node has the above form, then this composition of a
weighted sum (resp. disjunction) is known in PC and LC literature as a \emph{partition}.

\begin{example}[sidebyside,lefthand width=0.55\textwidth]{Embedding certain knowledge in probabilistic circuits}{pcsupp}

  Recall the logic circuit $\mathcal{L}$ from \Cref{eg:bdd}. Imagine we wish to model the
  uncertainty coming from all assignments where $\mathcal{L}(\set{x})=1$. In other words, we want
  to assign a positive probability to all true entries in the previous example's truth table,
  turning it into a probability table. The table on the right shows the chosen probabilities for
  each instance. Naturally, they all sum to one, with logically impossible assignments set to zero.

  Compiling an LC into a PC is straightforward: replace conjunctions with product nodes and
  disjunctions with sum nodes. Input nodes are left untouched, as literal nodes are just degenerate
  probability distributions. Sum weights are what ultimately define the probabilities in the
  probability table. The PC on the right is the result of the compilation of $\mathcal{L}$ into a
  probabilistic circuit whose distribution is defined by the probability table above it. When we
  mean to say that a PC has its support defined by its underlying LC, then we use the logic gate
  notation with the added weights on edges coming out from \inode{\newOrNode} nodes.

  \tcblower
  \small%
  \begin{center}
    \begin{tabular}{ccc|cc}
      \hline
      $A$ & $B$ & $C$ & $\phi(\set{x})$ & $p(\set{x})$\\
      \hline
      0 & 0 & 0 & 1 & 0.140\\
      1 & 0 & 0 & 1 & 0.024\\
      0 & 1 & 0 & 0 & \textcolor{gray}{0.000}\\
      1 & 1 & 0 & 0 & \textcolor{gray}{0.000}\\
      0 & 0 & 1 & 1 & 0.560\\
      1 & 0 & 1 & 1 & 0.096\\
      0 & 1 & 1 & 0 & \textcolor{gray}{0.000}\\
      1 & 1 & 1 & 1 & 0.180\\
      \hline
    \end{tabular}

    Probability Table

    \begin{tikzpicture}
      \newOrNode[inputs=nn,fill=boxgreen]{r}{0,0};
      \newAndNode[inputs=nn,fill=boxred!70]{p1}{$(r) + (-1,-1)$};
      \newAndNode[inputs=nn,fill=boxred!70]{p2}{$(r) + (1.5,-1)$};
      \node (a) at ($(p1) + (-0.5,-1)$) {$A$};
      \newOrNode[inputs=nn,fill=boxpurple!60]{s1}{$(p1) + (0.5,-1.25)$};
      \node (na) at ($(p2) + (0.5,-1)$) {$\neg A$};
      \newAndNode[inputs=nn,fill=boxblue!80]{q1}{$(s1) + (-0.75,-1)$};
      \newAndNode[inputs=nn,fill=boxblue!80]{q2}{$(s1) + (0.75,-1)$};
      \newAndNode[inputs=nn,fill=boxblue!80]{q3}{$(p2.input 1) + (0,-2.0)$};
      \node (b) at ($(q1) + (-0.5,-1)$) {$B$};
      \node (c1) at ($(q1) + (0.5,-1)$) {$C$};
      \newOrNode[inputs=nn,fill=boxorange!80]{z1}{$(q2.input 1) + (0,-1.0)$};
      \node (nb) at ($(q3.input 2) + (0,-1.0)$) {$\neg B$};
      \node (c) at ($(z1) + (-0.5,-1)$) {$C$};
      \node (nc) at ($(z1) + (0.5,-1)$) {$\neg C$};
      \draw[edge] (r.input 1) -- ++(0,-0.2) -| node[near start,above left] {$0.3$} (p1);
      \draw[edge] (r.input 2) -- ++(0,-0.2) -| node[near start,above right] {$0.7$} (p2);
      \draw[edge] (s1.input 1) -- ++(0,-0.2) -| node[near start,above left] {$0.6$} (q1);
      \draw[edge] (s1.input 2) -- ++(0,-0.2) -| node[near start,above right] {$0.4$} (q2);
      \draw[edge] (p1.input 1) -- ++(0,-0.2) -| (a);
      \draw[edge] (p1.input 2) -- ++(0,-0.2) -| (s1);
      \draw[edge] (p2.input 1) -- (q3);
      \draw[edge] (p2.input 2) -- ++(0,-0.2) -| (na);
      \draw[edge] (q1.input 1) -- ++(0,-0.2) -| (b);
      \draw[edge] (q1.input 2) -- ++(0,-0.2) -| (c1);
      \draw[edge] (q2.input 1) -- (z1.east);
      \draw[edge] (q2.input 2) -- (nb.north);
      \draw[edge] (q3.input 1) -- (z1.east);
      \draw[edge] (q3.input 2) -- (nb.north);
      \draw[edge] (z1.input 1) -- ++(0,-0.2) -| node[near start,above left] {$0.8$} (c);
      \draw[edge] (z1.input 2) -- ++(0,-0.2) -| node[near start,above right] {$0.2$} (nc);
    \end{tikzpicture}

    \vskip -0.25cm
    Probabilistic Circuit
  \end{center}
\end{example}

This compatibility between logic and probabilistic circuits allows certain knowledge to be embedded
into an uncertain model by constructing a computational graph whose underlying logic circuit
correctly attributes positive values only to the support of the distribution. When such a
computational graph is also smooth, structured decomposable and deterministic, then it belongs to a
special subclass of PCs called Probabilistic Sentential Decision Diagrams (PSDDs, \cite{kisa14}).
An alternative use case for logic circuits within the context of probabilistic reasoning is
querying for the probability of logical events, i.e.\ the expectation of a logic query with respect
to a distribution, a special case of the previously defined \Cref{eq:exp}, where the circuit to be
queried $\mathcal{L}$ is a logic circuit representing any logic query and $\mathcal{C}$ is the
probabilistic interpretation
\begin{equation}
  \mathbb{E}_{\mathcal{C}}\left[\mathcal{L}\right]=\int_{\set{x}}\mathcal{C}(\set{x})\mathcal{L}(\set{x})\dif\set{x}.
\end{equation}
This computation is known to be tractable when $\mathcal{C}$ and $\mathcal{L}$ both have the same
vtree and $\mathcal{C}$ is smooth, as \Cref{thm:expc} shows.

\begin{algorithm}[t]
  \caption{\expc}\label{alg:expc}
  \begin{algorithmic}[1]
    \Require A smooth, structured decomposable PC $\mathcal{C}$ and LC $\mathcal{L}$, both with vtree
      $\vtree$
      \Ensure The expectation $\mathbb{E}_\mathcal{C}\left[\mathcal{L}\right]=\int_\set{x}\mathcal{C}(\set{x})
      \mathcal{L}(\set{x})\dif\set{x}$
    \State Let $\mathcal{H}$ be a hash table mapping a pair of nodes to an expectation
    \Function{Traverse}{$\Node_{\mathcal{C}}$, $\Node_{\mathcal{L}}$}
      \IIf{$(\Node_\mathcal{C},\Node_\mathcal{L})\in\mathcal{H}$}{\textbf{return} $\mathcal{H}(\Node_\mathcal{C},\Node_\mathcal{L})$}
      \IIf{$\Node_\mathcal{C}$ is an input}{$\mathcal{H}(\Node_\mathcal{C},\Node_\mathcal{L})\gets\mathbb{E}_{\Node_\mathcal{C}}\left[\Node_\mathcal{L}\right]$}
      \NIElseIf{$\Node_\mathcal{C}$ is a product}
        \State $v\gets 0$
        \For{each $i$-th children $\Child_\mathcal{C}^{(i)}$ and $\Child_\mathcal{L}^{(i)}$ of
          $\Node_\mathcal{C}$ and $\Node_\mathcal{L}$ respectively}
          \State $v\gets v\cdot\Call{Traverse}{\Child_\mathcal{C}^{(i)},\Child_\mathcal{L}^{(i)}}$
        \EndFor%
        \State $\mathcal{H}(\Node_\mathcal{C},\Node_\mathcal{L})\gets v$
      \EndNIElseIf%
      \NIElseIf{$\Node_\mathcal{C}$ is a sum}
        \State $v\gets 0$
        \For{each child $\Child_\mathcal{C}\in\Ch(\Node_\mathcal{C})$}
          \For{each child $\Child_\mathcal{L}\in\Ch(\Node_\mathcal{L})$}
            \State $v\gets v + w_{\Node_\mathcal{C},\Child_\mathcal{C}}\cdot\Call{Traverse}{\Child_\mathcal{C},\Child_\mathcal{L}}$
          \EndFor%
        \EndFor%
        \State $\mathcal{H}(\Node_\mathcal{C},\Node_\mathcal{L})\gets v$
      \EndNIElseIf
      \State \textbf{return} $\mathcal{H}(\Node_\mathcal{C},\Node_\mathcal{L})$
    \EndFunction%
    \State \textbf{return} \Call{Traverse}{$\mathcal{C},\mathcal{L}$}
  \end{algorithmic}
\end{algorithm}

\begin{restatable}[\cite{choi20}]{theorem}{expcthm}\label{thm:expc}
  If $\mathcal{C}$ is a smooth and structured decomposable probabilistic circuit with vtree
  $\vtree$, and $\mathcal{L}$ a structured decomposable logic circuit also respecting $\vtree$, then
  $\mathbb{E}_\mathcal{C}\left[\mathcal{L}\right]$ is polynomial time computable (in the number of
  edges).
\end{restatable}

Let $\mathcal{C}$ be a smooth and structured decomposable circuit with vtree $\vtree$, and
$\mathcal{L}$ a logic circuit representing a logical query whose vtree is also $\vtree$. Computing
the probability of $\mathcal{L}$ with respect to the distribution encoded by $\mathcal{C}$ is done
by a bottom-up evaluation over both circuits at the same time. \Cref{alg:expc} shows the procedure
algorithmically. Importantly, the procedure relies on evaluating the expectation of a node with
respect to another, with one coming from the \emph{probabilistic}, and the other from the
\emph{logical} side. The algorithm runs in polynomial time by caching expectation values to avoid
recomputing already visited nodes. Starting with inputs, where the expectation is delegated to the
input's distribution, we go up to inner nodes, where both computation and node pairing is distinct
depending on the node's computational unit. For product nodes, primes are paired with primes, and
subs with subs; for sums, each combination of PC child with LC child is paired up.

\begin{example}[sidebyside,lefthand width=0.55\textwidth]{Computing the probability of logical events}{problog}
  \begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newOrNode[inputs=nn,fill=boxgreen]{r}{0,0};
      \newAndNode[inputs=nn,fill=boxred!70]{p1}{$(r) + (-1,-1)$};
      \newAndNode[inputs=nn,fill=boxred!70]{p2}{$(r) + (1,-1)$};
      \node (a) at ($(p1) + (-1,-1)$) {$A$};
      \newOrNode[inputs=nn,fill=boxpurple!60]{s1}{$(p1.input 2) + (0,-0.75)$};
      \node (na) at ($(p2) + (-1,-1)$) {$\neg A$};
      \newOrNode[inputs=nn,fill=boxpurple!60]{s2}{$(p2.input 2) + (0,-0.75)$};
      \newAndNode[inputs=nn,fill=boxblue!80]{q1}{$(s1) + (0,-0.75)$};
      \newAndNode[inputs=nn,fill=boxblue!80]{q2}{$(s2) + (0,-0.75)$};
      \newOrNode[inputs=nn,fill=orange!80]{bb}{$(q1) + (-1,-1)$};
      \newOrNode[inputs=nn,fill=orange!80]{bc1}{$(q1) + (1,-1)$};
      \node (nb) at ($(q2.input 1) + (0,-0.75)$) {$\neg B$};
      \newOrNode[inputs=nn,fill=orange!80]{bc2}{$(q2) + (1,-1)$};
      \node (b1) at ($(bb) + (-0.5,-1)$) {$\neg B$};
      \node (nb1) at ($(bb) + (0.5,-1)$) {$B$};
      \node (c1) at ($(bc1) + (-0.5,-1)$) {$\neg C$};
      \node (nc1) at ($(bc1) + (0.5,-1)$) {$C$};
      \node (c2) at ($(bc2) + (-0.5,-1)$) {$\neg C$};
      \node (nc2) at ($(bc2) + (0.5,-1)$) {$C$};
      \draw[edge] (r.input 1) -- ++(0,-0.2) -| (p1);
      \draw[edge] (r.input 2) -- ++(0,-0.2) -| (p2);
      \draw[edge] (p1.input 1) -- ++(0,-0.2) -| (a);
      \draw[edge] (p2.input 1) -- ++(0,-0.2) -| (na);
      \draw[edge] (p1.input 2) -- (s1);
      \draw[edge] (p2.input 2) -- (s2);
      \draw[edge] (s1) -- (q1);
      \draw[edge] (s2) -- (q2);
      \draw[edge] (q1.input 1) -- ++(0,-0.2) -| (bb);
      \draw[edge] (q1.input 2) -- ++(0,-0.2) -| (bc1);
      \draw[edge] (q2.input 1) -- (nb);
      \draw[edge] (q2.input 2) -- ++(0,-0.2) -| (bc2);
      \draw[edge] (bb.input 1) -- ++(0,-0.2) -| (b1);
      \draw[edge] (bb.input 2) -- ++(0,-0.2) -| (nb1);
      \draw[edge] (bc1.input 1) -- ++(0,-0.2) -| (c1);
      \draw[edge] (bc1.input 2) -- ++(0,-0.2) -| (nc1);
      \draw[edge] (bc2.input 1) -- ++(0,-0.2) -| (c2);
      \draw[edge] (bc2.input 2) -- ++(0,-0.2) -| (nc2);
    \end{tikzpicture}
    }
  \end{wrapfigure}
  Say we have a PC encoding the distribution shown in the table on the right. Suppose we wish to
  compute the probability of a logical event, say $\phi\equiv A\vee\neg B$. A nave approach would
  be to go over each assignment $\set{x}$ where $\phi(\set{x})=1$, and compute their sum. This is
  obviously exponential on the number of variables. Instead, we may compile $\phi$ into the logic
  circuit above and run the \expc{} algorithm to (in polynomial time) compute this otherwise
  intractable marginalization. Running \expc{} gives us a probability of
  $0.6415=1.0-(0.1365+0.2220)$, which is exactly the desired probability.
\tcblower
\begin{center}
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    \newOrNode[inputs=nn,fill=boxgreen]{r}{0,0};
    \newAndNode[inputs=nn,fill=boxred!70]{p1}{$(r) + (-1,-1)$};
    \newAndNode[inputs=nn,fill=boxred!70]{p2}{$(r) + (1,-1)$};
    \newOrNode[inputs=nn,fill=boxpurple!60]{ba1}{$(p1) + (-1.5,-1)$};
    \newOrNode[inputs=nn,fill=boxpurple!60]{s1}{$(p1.input 2) + (0.0,-0.75)$};
    \newOrNode[inputs=nn,fill=boxpurple!60]{s2}{$(p2.input 1) + (0.0,-0.75)$};
    \newOrNode[inputs=nn,fill=boxpurple!60]{ba2}{$(p2) + (1.5,-1)$};
    \node (a1) at ($(ba1) + (-0.75,-1)$) {$\neg A$};
    \node (na1) at ($(ba1.input 2) + (0.0,-0.75)$) {$A$};
    \newAndNode[inputs=nn,fill=boxblue!80]{q1}{$(s1.input 1) + (0,-0.75)$};
    \newAndNode[inputs=nn,fill=boxblue!80]{q2}{$(s2.input 2) + (0,-0.75)$};
    \node (a2) at ($(ba2.input 1) + (0.0,-0.75)$) {$\neg A$};
    \node (na2) at ($(ba2) + (0.75,-1)$) {$A$};
    \newOrNode[inputs=nn,fill=orange!80]{bb1}{$(q1) + (-1.5,-1)$};
    \newOrNode[inputs=nn,fill=orange!80]{bc1}{$(q1.input 2) + (0,-0.75)$};
    \newOrNode[inputs=nn,fill=orange!80]{bb2}{$(q2.input 1) + (0,-0.75)$};
    \newOrNode[inputs=nn,fill=orange!80]{bc2}{$(q2) + (1.5,-1)$};
    \node (b1) at ($(bb1) + (-0.75,-1)$) {$\neg B$};
    \node (nb1) at ($(bb1.input 2) + (0.0,-0.75)$) {$B$};
    \node (c1) at ($(bc1) + (-0.5,-1)$) {$\neg C$};
    \node (nc1) at ($(bc1) + (0.5,-1)$) {$C$};
    \node (b2) at ($(bb2) + (-0.5,-1)$) {$\neg B$};
    \node (nb2) at ($(bb2) + (0.5,-1)$) {$B$};
    \node (c2) at ($(bc2.input 1) + (0.0,-0.75)$) {$\neg C$};
    \node (nc2) at ($(bc2) + (0.75,-1)$) {$C$};
    \draw[edge] (r.input 1) -- ++(0,-0.2) -| node[near start,above left] {$.45$} (p1);
    \draw[edge] (r.input 2) -- ++(0,-0.2) -| node[near start,above right] {$.55$} (p2);
    \draw[edge] (p1.input 1) -- ++(0,-0.2) -| (ba1);
    \draw[edge] (ba1.input 1) -- ++(0,-0.2) -| node[near start, above left] {$.7$} (a1);
    \draw[edge] (ba1.input 2) -- node[midway, right] {$.3$} (na1);
    \draw[edge] (p2.input 2) -- ++(0,-0.2) -| (ba2);
    \draw[edge] (ba2.input 1) -- node[midway, left] {$.6$} (a2);
    \draw[edge] (ba2.input 2) -- ++(0,-0.2) -| node[near start, above right] {$.4$} (na2);
    \draw[edge] (p1.input 2) -- ++(0,-0.2) -| (s1);
    \draw[edge] (p2.input 1) -- ++(0,-0.2) -| (s2);
    \draw[edge] (s1.input 1) -- node[midway,left] {$.2$} (q1.east);
    \draw[edge] (s1.input 2) -- node[near start,above right,xshift=-0.1cm] {$.8$} (q2.east);
    \draw[edge] (s2.input 1) -- node[near start,above left,xshift=0.1cm] {$.9$} (q1.east);
    \draw[edge] (s2.input 2) -- node[midway,right] {$.1$} (q2.east);
    \draw[edge] (q1.input 1) -- ++(0,-0.2) -| (bb1);
    \draw[edge] (q1.input 2) -- ++(0,-0.2) -| (bc1);
    \draw[edge] (q2.input 1) -- ++(0,-0.2) -| (bb2);
    \draw[edge] (q2.input 2) -- ++(0,-0.2) -| (bc2);
    \draw[edge] (bb1.input 1) -- ++(0,-0.2) -| node[near start,above left] {$.4$} (b1);
    \draw[edge] (bb1.input 2) -- node[midway,right] {$.6$} (nb1);
    \draw[edge] (bc1.input 1) -- ++(0,-0.2) -| node[near start,above left] {$.5$} (c1);
    \draw[edge] (bc1.input 2) -- ++(0,-0.2) -| node[near start,above right] {$.5$} (nc1);
    \draw[edge] (bb2.input 1) -- ++(0,-0.2) -| node[near start,above left] {$.5$} (b2);
    \draw[edge] (bb2.input 2) -- ++(0,-0.2) -| node[near start,above right] {$.5$} (nb2);
    \draw[edge] (bc2.input 1) -- node[midway, left] {$.2$} (c2);
    \draw[edge] (bc2.input 2) -- ++(0,-0.2) -| node[near start,above right] {$.8$} (nc2);
  \end{tikzpicture}
  }

  \begin{tabular}{ccc|cc}
    \hline
    $A$ & $B$ & $C$ & $\phi(\set{x})$ & $p(\set{x})$\\
    \hline
    0 & 0 & 0 & 1 & .1000\\
    1 & 0 & 0 & 1 & .0580\\
    0 & 1 & 0 & 0 & \textcolor{gray}{.1365}\\
    1 & 1 & 0 & 1 & .0805\\
    0 & 0 & 1 & 1 & .1860\\
    1 & 0 & 1 & 1 & .0970\\
    0 & 1 & 1 & 0 & \textcolor{gray}{.2220}\\
    1 & 1 & 1 & 1 & .1195\\
    \hline
  \end{tabular}
\end{center}
\end{example}

\begin{remark}[breakable]{On applications of probabilistic circuits}{applications}
  So far, we have not yet addressed real-world applications of probabilistic circuits. Although
  their usage has not yet gained popularity among the data science crowd, they have been
  successfuly employed in a multitude of interdisciplinary tasks. Here, we give a brief survey on
  the different use cases of probabilistic circuits present in literature.

  Computer vision is perhaps the most popular application for deep learning, and this could not be
  different for probabilistic circuits. PCs have been used for image classification
  \citep{gens12,sguerra16,llerena17,geh19,peharz20a}, image reconstruction and sampling
  \citep{poon11,dennis17,peharz20b,butz19}, image segmentation and scene understanding
  \citep{friesen17,yuan16,friesen18,rathke17}, and activity recognition
  \citep{wang18,amer12,nourani20,amer16}.

  Probabilistic circuits have also been used for sequential data \citep{melibari16b}, such as
  speech recognition and reconstruction \citep{peharz14b,ratajczak14,ratajczak18} and natural
  language processing \citep{cheng14}.

  Remarkably, probabilistic circuits have seen a recent boom in hardware-aware research
  \citep{shah19,olascoaga19} and dedicated hardware for PCs in embedded systems
  \citep{sommer18,shah20,shah21}.

  Some other applications include robotics \citep{sguerra16,geh19,zheng18,pronobis17}, biology
  \citep{butz18,friesen15}, probabilistic programming \citep{stuhlmuller12,saad21}, and fault
  localization \citep{nath16}.
\end{remark}

