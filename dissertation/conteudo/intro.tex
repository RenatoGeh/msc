\chapter{Introduction}

When reasoning about the world, rarely can we find a realistic model that perfectly subsumes all of
the needed relationships for flawless prediction. As such, the presence of a reliable quantifier on
the aleatoric uncertainty of intelligent systems is essential in developing performant yet
diagnostible agents. This is made explicitly clear in the case of high-risk settings, such as
autonomous vehicles or automated power plant systems, where a wrong prediction could cause
disastrous consequences. A well known example in the case of the former is obstacle avoidance:
while the agent should be capable of accurately identifying obstructions in its way during normal
conditions, so should it be able to identify its own lack of confidence in high uncertainty
situations like ones brought by environmental factors, such as severe blizzard or heavy rain. In
these and other situations where a system faces unusual and perhaps never before seen evidence,
predictions might prove unreliable, and the safest option might be for the agent to first identify
its uncertainty, and second to reach out for human help. Conversely, in a case of anomaly
detection, the agent might identify a human's driving as abnormally irregular (due to inebriation,
infirmity, etc.) and appropriately take control of the vehicle before a potential accident takes
place.

One popular approach to quantifying uncertainty is through probability theory. By abstracting the
world as a probability distribution with a finite number of observable random variables that encode
a possibly incomplete knowledge of the environment, we are, in theory, able to answer a diverse set
of complex queries as long as we have access to the (approximate) true data distribution. Practice
is far different from theory, however, as most machine learning models either lie within a very
limited range in the tractability spectrum in terms of inference \citep{pclec} or are too
simplistic for complex real-world problems. Besides, although the majority of recent advances in
deep learning claim some probabilistic meaning from the model's output, they are often uncalibrated
distributions, a result of focusing on maximizing predictive accuracy at the expense of predictive
(data) uncertainty \citep{guo17,yaniv19,chernikova19}, ultimately producing overconfident and
peculiar results \citep{szegedy13,wei18,su19,chernikova19}. Crucially, mainstream deep models
(i.e.\ standard neural networks) usually optimize a conditional distribution over the
to-be-predicted random variables -- and are thus often called \emph{discriminative} models -- and
do not model the actual joint distribution of the data, limiting inference capabilities.

In contrast, \emph{generative} models seek to extract information from the joint (in varying
capacities), and have lately seen a sharp rise in interest within deep learning. Despite this, most
popular models do not admit either exact or tractable querying of key inference scenarios. For
instance, although Generative Adversarial Networks (GANs) allow for efficient sampling
\citep{goodfellow14}, basic queries such as likelihood or marginals are outside of their
capabilities.  Similarly, Normalizing Flows (NF) also permit access to efficient sampling, with the
added feature of computing likelihoods, but are severely limited by the base distribution when it
comes to discrete data \citep{rezende15,papamakarios21} albeit recent works on discretizing NFs
have shown empirically good results \citep{lippe21,ziegler19}. Variational Auto-Encoders (VAEs) are
(under certain conditions) a generalization of NFs \citep{yu20,gritsenko19} with known extensions
for categorical data \citep{rolfe17,vahdat18a,vahdat18b}, but only permit access to sampling and an
upper-bound on the likelihood, with the latter available only after solving a complex optimization
task \citep{kingma14}.

Despite the impressive achievements of the aforementioned generative models on realistically
producing samples consistent with evidence, in none of the previous models are complex queries like
structured prediction under partial observations, \emph{maximum a posteriori} (MAP), conditional or
marginal probabilities tractable. An obvious alternative would be Probabilistic Graphical Models
(PGMs), although they too suffer from intractability when dealing with high treewidth networks
\citep{dechter98,koller09}, severely limiting expressivity. Instead, we draw our attention to an
expressive class of models that subsumes several families of probabilistic models with tractable
inference capabilities.

Probabilistic Circuits (PCs) define a superclass of probabilistic models distincly specified by
recursive compositions of distributions through graphical formalisms. Vaguely speaking, PCs are
computational graphs akin to neural networks, but whose network structure and computational units
abide by special constraints. Within these specific conditions span a wide range of subclasses,
each establishing a distinct set of restrictions on their structure in order to enable different
segments within the tractability spectrum. As an example, Sum-Product Networks (SPNs,
\cite{poon11}) are usually loosely defined over a couple of constraints: namely \emph{smoothness}
and \emph{decomposability}, which in turn enables likelihood, marginal and conditional
computations. Arithmetic Circuits (ACs, \cite{darwiche03}) add \emph{determinism} to the mix,
allowing for tractable computation of MAP probabilities. Similarly, Cutset Networks (CNets,
\cite{rahman14}) employ the same constraints as ACs, but accept more expressive distributions as
part of their computational units. Probabilistic Sentential Decision Diagrams (PSDDs,
\cite{kisa14}), Probabilistic Decision Graphs (PDGs, \cite{jaeger04}) and And/Or-Graphs (AOGs,
\cite{dechter07}) all require \emph{smoothness} and \emph{determinism}, but also call for a
stronger version of \emph{decomposability}, permitting all queries previously mentioned as well as
computation of the Kullback-Leibler divergence and expectation between two circuits \citep{choi20}.
Usually, PCs represent the joint distribution of the data, although they are sufficiently
expressive for generative \emph{and} discriminative modeling
\citep{khosravi19,rashwan18a,rooshenas16,gens12,shao20}. In this dissertation though, we shall
focus on the generative side of PCs.

While inference is usually straightforward, as we shall see in \Cref{sec:pc}, learning the
structure of PCs so that they obey the needed structural restrictions requires either careful
handcrafted architectures \citep{poon11,cheng14,nath16} or usually involves running costly
(in)dependence tests over most (if not all) variables \citep{gens13,jaini18a,vergari15,dimauro17a},
which can become prohibitive in higher dimension data. Alternatively, some learning algorithms
resort to structure preserving iterative methods to grow a PC that already initially satisfies
desired constraints, adding complexity to the underlying distribution at each iteration
\citep{liang17,dang20}. However, these can take several iterations until visible improvement and
often take several minutes for each iteration when the circuit is big. Common techniques used in
deep learning for generating scalable architectures for neural network also pose a problem, as the
nature of the needed structural constraints make for sparse computational graphs. To circumvent
these issues, work on scaling PCs to higher dimensions has focused mainly on random architectures,
with competitive results \citep{peharz20a,dimauro21,geh21a,peharz20b}. Apart from the scalability
side of random structure generation, usual structure learning algorithms often require grid-search
for hyperparameter tuning to achieve top quality performance, which is usually not the case for
random algorithms. For the usual data scientist or machine learning practicioner, hyperparameter
tuning can become exhaustive, especially if the goal is to analyze and infer from large data, and
not to achieve top tier performance on benchmark datasets.

In this dissertation, we propose two scalable structural learning algorithms for probabilistic
circuits that are especially suited for large data and fast deployment. They both take advantage of
random network generation to quickly construct PCs with little to no need for hyperparameters. The
first is effective for constructing PCs from binary data with a highly constrained structure, and
thus appropriate when complex querying is needed. The second builds less constrained random PCs,
but supports both discrete and continuous data.

\section{Contributions and Dissertation Outline}

We organize this dissertation as follows. We begin \Cref{ch:pc} by formally defining probabilistic
circuits (\Cref{sec:pc}), conducting a review of some of the structural constraints that we might
impose on PCs, as well as what we may gain from them in terms of tractability.
We then list existing formalisms that may be viewed as instances of PCs, and what their structure
entail in terms of inference power. We finish the chapter by looking at a particular class of PCs
which allow the embedding of logical formulae within their support. In \Cref{ch:learning}, we address existing PC structure
learning algorithms dividing them into three classes: divide-and-conquer learning
(\Cref{sec:divconq}), incremental learning (\Cref{sec:incremental}) and random learning
(\Cref{sec:random}). For each, we give a brief analysis on their complexity and discuss their
advantages and disadvantages. We cover the two new structure learners in \Cref{ch:scalable},
providing empirical results on their performance. The final chapter (\Cref{ch:conclusion}) is
dedicated to summarizing our research contributions and pointing to potential future work in
learning PCs.

Our contributions in this dissertation address the following research topics.

\newCTitle{boxgray}{Scalably learning PCs directly from background knowledge}

In \citet{geh21a}, we provide a learning algorithm for PSDDs that learns a PC directly from
background knowledge in the form of logical constraints. The algorithm samples a structure from a
distribution of possible PSDDs that are weakly consistent with the logical formula. How weak
consistency is depends on a parameter that trades permission of false statements as non zero
probability events with circuit complexity. We provide the algorithm and empirical results in
\Cref{sec:logical}.

\newCTitle{boxgray}{Using ensembles to strengthen consistency}

The PC sampler given by \citet{geh21a} produces competitive probabilistic models (in terms of
likelihood), albeit weak logical models in the sense that it possibly assigns non-zero probability
to false variable assignments -- as we discuss in \Cref{sec:logical}, it never assigns zero
probability to true statements. By producing many weak models, we not only gain in terms of data
fitness, but also consistency: if any one component in the ensemble returns an assignment to be
impossible, the whole model should return false.

\newCTitle{boxgray}{Random projections to efficiently learn PCs}

Usual methods often employ clustering algorithms for constructing convex combinations of
computational units. These can take many iterations to converge or require space quadratic in the
number of data points. Instead, in \Cref{sec:data} we present linear alternatives based on random
projections \citep{dasgupta08a,dasgupta08b}.

