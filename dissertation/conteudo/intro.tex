\chapter{Introduction}

Machine learning has become, without a doubt, ubiquitous in today's technology. From self-driving
vehicles to bioinformatics, data-driven models have taken the center stage when it comes to
state-of-the-art prediction and modeling of real-world tasks \citep{grigorescu20,lan18,li19,khan18,
sezer20}. In many of these scenarios, such as system diagnosis of automated power plants or
real-time translation, timely decisions are crucial to the well-functioning of often critical
systems \citep{enshaei19,niehues18}. In others, such as in medical diagnostic systems or credit
assessment, conclusions produced by the system must be reliable or come with some measure of error
of its estimates \citep{lou19,xolani20}. Even more crucially, these predictive models often need to
perform complex reasoning tasks over hundreds if not thousands of variables, sometimes under known
constraints dependent on domain \citep{xu18,marin20,wong12,lu13}; and yet prove expressive enough
to learn from the possibly enormous quantity of data available.

Fulfilling these requirements are Probabilistic Circuits (PCs), a class of probabilistic models
distinctly specified by recursive compositions of distributions through graphical formalisms.
Vaguely speaking, PCs are computational graphs akin to neural networks, but whose network structure
and computational units abide by special constraints. More concretely, these structural and
operational constraints lead to sufficient conditions for the polynomial time computation of
complex exact queries, providing a powerful toolbox for probabilistic reasoning. Within these
specific conditions span a wide range of subclasses, each establishing a distinct set of
restrictions on their structure in order to enable different segments within the tractability
spectrum. These specific families of PCs have been known throughout literature by different names:
Arithmetic Circuits (ACs, \cite{darwiche03}), Sum-Product Networks (SPNs, \cite{poon11}), Cutset
Networks (CNets, \cite{rahman14}), Probabilistic Sentential Decision Diagrams (PSDDs,
\cite{kisa14}), Probabilistic Decision Graphs (PDGs, \cite{jaeger04}) and And/Or-Graphs (AOGs,
\cite{dechter07}) are some of the more well-known formalisms caught under the PC framework.

While inference in PCs is usually straightforward, learning their structure so that they both
obey the needed structural restrictions and prove expressive enough for the task at hand has proven
to be a challenging process. Even so, many techniques have been proposed in the last decade, with
encouraging results. These techniques however, often do not scale up well when faced with higher
dimensional data, as they require either carefully handcrafted architectures
\citep{poon11,cheng14,nath16} or usually involve running costly (in)dependence tests over most (if
not all) variables \citep{gens13,jaini18a,vergari15,dimauro17a}. Alternatively, some learning
algorithms resort to structure preserving iterative methods to grow a PC that already initially
satisfies desired constraints, adding complexity to the underlying distribution at each iteration
\citep{liang17,dang20}. However, these can take several iterations until visible improvement and
often take several minutes for each iteration when the circuit is big. Common techniques used in
deep learning for generating scalable architectures for neural network also pose a problem, as the
nature of the needed structural constraints make for sparse computational graphs. To circumvent
these issues, work on scaling PCs to higher dimensions has focused mainly on random architectures,
with competitive results \citep{peharz20a,dimauro21,geh21a,peharz20b}. Apart from the scalability
side of random structure generation, usual structure learning algorithms often require an extensive
grid-search for hyperparameter tuning to achieve top quality performance, which is usually not the
case for random algorithms. For the usual data scientist or machine learning practicioner,
hyperparameter tuning can become time consuming and tedious, especially if the goal is to analyze
and infer from large data, and not to achieve top tier performance on benchmark datasets.

The objectives of this research are two-fold. First, we seek to provide a concise literature
review on structure learning algorithms for probabilistic circuits, describing a few of the most
popular techniques for learning PCs. This particular contribution comes from a need for a
systematic comparison of the large body of techniques that have been developed so far, each with
different trade-offs of computational cost, accuracy and structural properties. To this end, we
compare and categorize structure learning algorithms with respect to time and memory requirements,
efficient queries enabled by the learned model and the overall pros and cons brought by each.

Second, we present two new structural learning algorithms for PCs whose focus are on efficiently
learning from complex domains; we show that our approaches are scalable and competitive against the
state-of-the-art. They both take advantage of random circuit generation to quickly construct PCs
with little to no need for hyperparameter tuning. The first approaches the problem of circuit
construction through the lenses of symbolic machine learning, effective for constructing PCs from
highly structured binary data where prior knowledge of the domain can be embedded into the model as
a means to define its support. We describe a randomized algorithm capable of taking background
knowledge in the form of logical constraints and produce PC samples whose support are relaxations
of the given logical formula. By taking advantage of known expert knowledge, the resulting PC
attributes mass to the more relevant portions of the sample space, resulting in a more performant
model, especially in data scarce regimes with abundant certain knowledge. We show how this approach
scales well to complex formulae and large amounts of data. From a pure data point of view, we
present a simple yet effective way to learn the structure of probabilistic circuits by exploiting a
recently discovered connection between PCs and random forests \citep{correia20}. We revisit a
well-known inductive method of hierarchically stacking oblique projections to learn decision trees
\citep{dasgupta08a,dasgupta08b} and transplant them into the context of probabilistic circuits,
adapting one of the more popular techniques for constructing PCs and proposing a simpler and faster
random version based on random oblique projections. We found that our approach produced fairly
competitive PC structures in a fraction of the time.

\section{Dissertation Outline}

We begin \Cref{ch:pc} by formally defining probabilistic circuits (\Cref{sec:pc}), conducting a
review of some of the structural constraints that we might impose on PCs, as well as what we may
gain from them in terms of tractability. This is followed by a description on how to
algorithmically compute inference queries in PCs (\Cref{sec:inf}). We then list existing formalisms
that may be viewed as instances of PCs, and what their structure entail in terms of inference
power. We finish the chapter by looking at a particular class of PCs which allow the embedding of
logical formulae within their support (\Cref{sec:pckb}).

In \Cref{ch:learning}, we address existing PC structure learning algorithms dividing them into
three classes: divide-and-conquer learning (\Cref{sec:divconq}), incremental learning
(\Cref{sec:incremental}) and random learning (\Cref{sec:random}). For each, we give a brief
analysis on their complexity and discuss their advantages and disadvantages. We finish the chapter
with with a summary of all algorithms in \Cref{sec:summary}.

In \Cref{ch:logical} and \Cref{ch:data}, we propose the two scalable structural learning algorithms
for probabilistic circuits that are especially suited for large data and fast deployment.  The
final chapter (\Cref{ch:conclusion}) is dedicated to summarizing our research contributions and
pointing to potential future work in learning PCs.

\section{Contributions}

Overall, our contributions throughout this dissertation address the following research topics.

\newCTitle{boxgray}{A concise review of literature on structure learning of PCs}

\Cref{ch:learning} is dedicated to an extensive review of some of the existing techniques for
learning the structure of probabilistic circuits. We categorize them into three different classes
and analyze each in terms of their time requirements. We describe them in detail and list insights
on what seems to work and what could potentially be improved. Perhaps more importantly, we provide
a birds-eye view of what each learning algorithm guarantees in terms of structural constraints,
time requirement and number of hyperparameters needed during learning.

\newCTitle{boxgray}{Scalably learning PCs directly from background knowledge}

In \citet{geh21a}, we provide a learning algorithm for PSDDs that learns a PC directly from
background knowledge in the form of logical constraints. The algorithm samples a structure from a
distribution of possible PSDDs that are weakly consistent with the logical formula. How weak
consistency is depends on a parameter that trades permission of false statements as non zero
probability events with circuit complexity. We provide the algorithm and empirical results in
\Cref{ch:logical}.

\newCTitle{boxgray}{Using ensembles to strengthen consistency}

The PC sampler given by \citet{geh21a} produces competitive probabilistic models (in terms of
likelihood), albeit weak logical models in the sense that it possibly assigns non-zero probability
to false variable assignments -- as we discuss in \Cref{ch:logical}, it never assigns zero
probability to true statements. By producing many weak models, we not only gain in terms of data
fitness, but also consistency: if any one component in the ensemble returns an assignment to be
impossible, the whole model should return false.

\newCTitle{boxgray}{Random projections to efficiently learn PCs}

Usual methods often employ clustering algorithms for constructing convex combinations of
computational units. These can take many iterations to converge or require space quadratic in the
number of data points. Instead, in \Cref{ch:data} we present linear alternatives based on random
projections \citep{dasgupta08a,dasgupta08b}.
