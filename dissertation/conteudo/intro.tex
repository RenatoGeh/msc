\chapter{Introduction}

Machine learning has become, without a doubt, ubiquitous in tech. From self-driving vehicles to
bioinformatics, data-driven models have taken the center stage when it comes to state-of-the-art
prediction and modeling of real-world tasks. In many of these scenarios, such as system diagnosis
of automated power plants or real-time translation, timely decisions are crucial to the
well-functioning of often critical systems. In others, agents must be either precise or come with
approximation guarantees of their estimation, as is the case of decision making in healthcare or
finance. Even more crucially, these predictive models often need to perform complex reasoning tasks
over hundreds if not thousands of variables, sometimes under known constraints dependent on
domain; and yet prove expressive enough to learn from the possibly enormous quantity of data
available.

Fulfilling these requirements are Probabilistic Circuits (PCs), a class of probabilistic models
distinctly specified by recursive compositions of distributions through graphical formalisms.
Vaguely speaking, PCs are computational graphs akin to neural networks, but whose network structure
and computational units abide by special constraints. More concretely, as we shall see in
\Cref{sec:inf}, these structural and operational constraints lead to sufficient conditions for the
polynomial time computation of complex exact queries, providing a powerful toolbox for
probabilistic reasoning. Within these specific restrictions span a wide range of subclasses, each
establishing a distinct set of conditions on their structure in order to enable different segments
within the tractability spectrum. These specific families of PCs have been known throughout
literature by different names: Arithmetic Circuits (ACs, \cite{darwiche03}), Sum-Product Networks
(SPNs, \cite{poon11}), Cutset Networks (CNets, \cite{rahman14}), Probabilistic Sentential Decision
Diagrams (PSDDs, \cite{kisa14}), Probabilistic Decision Graphs (PDGs, \cite{jaeger04}) and
And/Or-Graphs (AOGs, \cite{dechter07}) are all some of the more well-known formalisms caught under
the PC framework.

While inference in PCs is usually straightforward, learning the structure of PCs so that they both
obey the needed structural restrictions and prove expressive enough for the task at hand has proven
to be a challenging process. Even so, many techniques have been proposed in the last decade, with
encouraging results. These techniques however, often do not scale up well when faced with higher
dimensional data, as they require either carefully handcrafted architectures
\citep{poon11,cheng14,nath16} or usually involve running costly (in)dependence tests over most (if
not all) variables \citep{gens13,jaini18a,vergari15,dimauro17a}. Alternatively, some learning
algorithms resort to structure preserving iterative methods to grow a PC that already initially
satisfies desired constraints, adding complexity to the underlying distribution at each iteration
\citep{liang17,dang20}. However, these can take several iterations until visible improvement and
often take several minutes for each iteration when the circuit is big. Common techniques used in
deep learning for generating scalable architectures for neural network also pose a problem, as the
nature of the needed structural constraints make for sparse computational graphs. To circumvent
these issues, work on scaling PCs to higher dimensions has focused mainly on random architectures,
with competitive results \citep{peharz20a,dimauro21,geh21a,peharz20b}. Apart from the scalability
side of random structure generation, usual structure learning algorithms often require an extensive
grid-search for hyperparameter tuning to achieve top quality performance, which is usually not the
case for random algorithms. For the usual data scientist or machine learning practicioner,
hyperparameter tuning can become exhaustive, especially if the goal is to analyze and infer from
large data, and not to achieve top tier performance on benchmark datasets.

\section{Contributions and Dissertation Outline}

In this dissertation, we propose two scalable structural learning algorithms for probabilistic
circuits that are especially suited for large data and fast deployment. They both take advantage of
random network generation to quickly construct PCs with little to no need for hyperparameters. The
first approaches this problem through the lenses of symbolic machine learning, effective for
constructing PCs from highly structured binary data where prior knowledge of the domain can be
embedded into the model as a means to define its support. We describe a randomized algorithm
capable of taking background knowledge in the form of logical constraints and produce PC samples
whose support are relaxations of the given logical formula. We show how this approach scales well
to complex formulae and large amounts of data.

From a pure data point of view, we present a simple and effective way to learn the structure of
probabilistic circuits by exploiting a recently discovered connection between PCs and random
forests \citep{correia20}. We explore a well-known inductive method of hierarchically stacking
oblique projections to learn decision trees \citep{dasgupta08a,dasgupta08b} and transplant them
into the context of probabilistic circuits. We found that our approach produced fairly competitive
PCs in a fraction of the time. Overall, our contributions in this dissertation address the
following research topics.

\newCTitle{boxgray}{Scalably learning PCs directly from background knowledge}

In \citet{geh21a}, we provide a learning algorithm for PSDDs that learns a PC directly from
background knowledge in the form of logical constraints. The algorithm samples a structure from a
distribution of possible PSDDs that are weakly consistent with the logical formula. How weak
consistency is depends on a parameter that trades permission of false statements as non zero
probability events with circuit complexity. We provide the algorithm and empirical results in
\Cref{sec:logical}.

\newCTitle{boxgray}{Using ensembles to strengthen consistency}

The PC sampler given by \citet{geh21a} produces competitive probabilistic models (in terms of
likelihood), albeit weak logical models in the sense that it possibly assigns non-zero probability
to false variable assignments -- as we discuss in \Cref{sec:logical}, it never assigns zero
probability to true statements. By producing many weak models, we not only gain in terms of data
fitness, but also consistency: if any one component in the ensemble returns an assignment to be
impossible, the whole model should return false.

\newCTitle{boxgray}{Random projections to efficiently learn PCs}

Usual methods often employ clustering algorithms for constructing convex combinations of
computational units. These can take many iterations to converge or require space quadratic in the
number of data points. Instead, in \Cref{sec:data} we present linear alternatives based on random
projections \citep{dasgupta08a,dasgupta08b}.

We organize this dissertation as follows. We begin \Cref{ch:pc} by formally defining probabilistic
circuits (\Cref{sec:pc}), conducting a review of some of the structural constraints that we might
impose on PCs, as well as what we may gain from them in terms of tractability.
We then list existing formalisms that may be viewed as instances of PCs, and what their structure
entail in terms of inference power. We finish the chapter by looking at a particular class of PCs
which allow the embedding of logical formulae within their support. In \Cref{ch:learning}, we address existing PC structure
learning algorithms dividing them into three classes: divide-and-conquer learning
(\Cref{sec:divconq}), incremental learning (\Cref{sec:incremental}) and random learning
(\Cref{sec:random}). For each, we give a brief analysis on their complexity and discuss their
advantages and disadvantages. We cover the two new structure learners in \Cref{ch:scalable},
providing empirical results on their performance. The final chapter (\Cref{ch:conclusion}) is
dedicated to summarizing our research contributions and pointing to potential future work in
learning PCs.
