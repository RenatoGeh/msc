\chapter{Learning Probabilistic Circuits}
\label{ch:learning}

As we have seen in \Cref{ch:pc}, inference in probabilistic circuits is, for the most part,
straightforward.  This is not so much the case when \emph{learning} PCs. Despite the uncomplicated
syntax, learning sufficiently expressive PCs in a principled way is comparatively harder to, say
the usual neural network. For a start, we are usually required to comply with smoothness and
decomposability to ensure marginalization at the least. This restriction excludes the possibility
of adopting any of the most popular neural network patterns or architectures used in deep learning
today. To make matters worse, constructing a PC graph more often than not involves costly
statistical tests that make learning their structure a challenge for high dimensional data.

In this chapter, we review the most popular PC structure learning algorithms, their pros and cons,
and more importantly, what can we learn from them to efficiently build scalable probabilistic
circuits. We broadly divide existing structure learners into three main categories:
divide-and-conquer (\divclass{}, \Cref{sec:divconq}), incremental methods (\incrclass{},
\Cref{sec:incremental}) and random approaches (\randclass{}, \Cref{sec:random}).

\section{Divide-and-Conquer Learning}
\label{sec:divconq}

Arguably the most popular approach to learning the structure of probabilistic circuits are
algorithms that follow a \emph{divide-and-conquer} scheme. This class of PC learning algorithms,
which here we denote by \divclass{}, are characterized by recursive calls over (usually mutually
exclusive) subsets of data in true divide-and-conquer fashion. This kind of procedure is more
clearly visualized by \textproc{LearnSPN}, the first, most well-known, and perhaps most archetypal
of its class.

Before we start however, we must first address how we denote data. Data is commonly represented as
a matrix where rows are assignments (of all variables), and columns are the values that each variable
takes at each assignment. Let $\set{D}\in\mathbb{R}^{m \times n}$ a matrix with $m$ rows and $n$
columns. We use $\set{D}_{i,j}$ to access an element of $\set{D}$ at the $i$-th row, $j$-th column
of matrix $\set{D}$. We denote by $\set{D}_{\set{i},\set{j}}$, where $\set{i}\subseteq
\left[1..n\right]$ and $\set{j}\subseteq\left[1..m\right]$ are sets of indices, a submatrix from
the extraction of the $\set{i}$ rows and $\set{j}$ columns of $\set{D}$. We use a colon as a
shorthand for selecting all rows or columns, e.g.\ $\set{D}_{:,:}=\set{D}$, $\set{D}_{:,j}$ is the
$j$-th column and $\set{D}_{i,:}$ is the $i$-th row.

\subsection{\textproc{LearnSPN}}

\begin{figure}[t]
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{subfigure}{0.45\textwidth}
      \resizebox{\textwidth}{!}{
      \begin{tabular}{ccccc}
        \hline
        $A$ & $B$ & $C$ & $D$ & $E$\\
        \hline
        \rowcolor{boxgreen!70}
        0 & 1 & 0 & 0 & 1\\
        \rowcolor{boxgreen!70}
        1 & 0 & 1 & 1 & 1\\
        \rowcolor{boxblue!50}
        1 & 1 & 0 & 1 & 1\\
        \rowcolor{boxblue!50}
        0 & 0 & 1 & 0 & 0\\
        \rowcolor{boxgreen!70}
        1 & 1 & 0 & 1 & 0\\
        \rowcolor{boxblue!50}
        0 & 1 & 1 & 0 & 1\\
        \rowcolor{boxorange!60}
        1 & 0 & 1 & 1 & 1\\
        \rowcolor{boxorange!60}
        1 & 1 & 0 & 0 & 0\\
        \rowcolor{boxblue!50}
        0 & 1 & 1 & 0 & 1\\
        \hline
      \end{tabular}
      }
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \resizebox{\textwidth}{!}{
      \begin{tikzpicture}
        \newSumNode[fill=boxpink!50]{r}{0,0};
        \newProdNode[fill=boxgreen!70]{p1}{$(r) + (-1.5,-1.5)$};
        \newProdNode[fill=boxorange!60]{p2}{$(r) + (0,-1.5)$};
        \newProdNode[fill=boxblue!50]{p3}{$(r) + (1.5,-1.5)$};
        \draw[edge] (r) -- node[midway,above left] {$\frac{3}{9}$} (p1);
        \draw[edge] (r) -- node[midway,left] {$\frac{2}{9}$} (p2);
        \draw[edge] (r) -- node[midway,above right] {$\frac{4}{9}$} (p3);
      \end{tikzpicture}
      }
    \end{subfigure}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{subfigure}{0.45\textwidth}
      \newcolumntype{x}{>{\columncolor{boxgreen!70}}c}
      \newcolumntype{y}{>{\columncolor{boxorange!60}}c}
      \newcolumntype{z}{>{\columncolor{boxblue!50}}c}
      \resizebox{\textwidth}{!}{
      \begin{tabular}{xyyzx}
        \hline
        \multicolumn{1}{c}{$A$} & \multicolumn{1}{c}{$B$} & \multicolumn{1}{c}{$C$} &
        \multicolumn{1}{c}{$D$} & \multicolumn{1}{c}{$E$}\\
        \hline
        0 & 1 & 0 & 0 & 1\\
        1 & 0 & 1 & 1 & 1\\
        1 & 1 & 0 & 1 & 1\\
        0 & 0 & 1 & 0 & 0\\
        1 & 1 & 0 & 1 & 0\\
        0 & 1 & 1 & 0 & 1\\
        1 & 0 & 1 & 1 & 1\\
        1 & 1 & 0 & 0 & 0\\
        0 & 1 & 1 & 0 & 1\\
        \hline
      \end{tabular}
      }
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \resizebox{\textwidth}{!}{
      \begin{tikzpicture}
        \newProdNode[fill=boxpink!50]{r}{0,0};
        \newSumNode[label=below:{$\{A,E\}$},fill=boxgreen!70]{s1}{$(r) + (-1.5,-1.5)$};
        \newSumNode[label=below:{$\{B,C\}$},fill=boxorange!60]{s2}{$(r) + (0,-1.5)$};
        \newSumNode[label=below:{$\{D\}$},fill=boxblue!50]{s3}{$(r) + (1.5,-1.5)$};
        \draw[edge] (r) -- (s1); \draw[edge] (r) -- (s2); \draw[edge] (r) -- (s3);
      \end{tikzpicture}
      }
    \end{subfigure}
    \caption{}
  \end{subfigure}
  \caption{\textproc{LearnSPN} assigns either rows (a) or columns (b) for sum and product nodes
    respectively. For sums, their edge weights are set proportionally to the assignments. For
    product children, scopes are defined by which columns are assigned to them.}
  \label{fig:learnspn}
\end{figure}

Recall the semantics of sum and product nodes in a smooth and decomposable probabilistic circuit.
A sum is a mixture of distributions $p(\set{X})=\sum_{i=1}^m w_i\cdot p_i(\set{X})$ whose children
scopes are all the same. A product is a factorization $p(\set{X}_1,\ldots,\set{X}_n)=\prod_{i=1}^n
p(\set{X}_i)$, implying that $\set{X}_i\indep\set{X}_j$ for $i,j\in [n]$ and $i\neq j$.
\textproc{LearnSPN} \citep{gens13} exploits these semantics in an intuitive and uncomplicated
manner: sum children are defined by sub-PCs learned from similar (by some arbitrary metric)
assignments, and product children are sub-PCs learned from data conditioned on the variables
defined by their scope. In practice, this means that, for a dataset $\set{D}\in\mathbb{R}^{m\times
n}$, sums assign rows to their children, while product children are assigned columns. This
procedures continues recursively until data are reduced to a $k\times 1$ matrix, in which case a
univariate distribution acting as input node is learned from it. This recursive procedure is shown
more formally in \Cref{alg:learnspn}.

\begin{algorithm}[t]
  \caption{\textproc{LearnSPN}}\label{alg:learnspn}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \IIf{$|\set{X}|=1$}{\textbf{return} an input node learned from $\set{D}$}
    \NIElse
      \State Find scope partitions $\set{X}_1,\ldots,\set{X}_t\subseteq\set{X}$ st
        $\set{X}_i\indep\set{X}_j$ for $i\neq j$
      \IIf{$k>1$}{\textbf{return} $\prod_{j=1}^t \textproc{LearnSPN}(\set{D}_{:,\set{X}_j},
        \set{X}_j)$}
      \NIElse
        \State Find subsets of data $\set{x}_1,\ldots,\set{x}_k\subseteq\set{D}$ st all assignments
          within $\set{x}_i$ are all similar
        \State \textbf{return} $\sum_{i=1}^k \frac{|\set{x}_i|}{|\set{D}|}\cdot
          \textproc{LearnSPN}(\set{x}_i,\set{X})$
      \EndNIElse
    \EndNIElse
  \end{algorithmic}
\end{algorithm}

Notably, \citep{gens13} purposely does not strictly specify which techniques should be used for
assigning rows and columns, although they do provide empirical results on a particular form of
\textproc{LearnSPN} where row assignments are computed through EM clustering and products by
pairwise G-testing. Instead, they call the algorithm a \emph{schema} that incorporates several
actual learning algorithms whose concrete form depends on the choice of how to split data.

\subsubsection{Complexity}

To be able to analyze the complexity of \textproc{LearnSPN}, we assume a common implementation
where sums are learned from $k$-means clustering, and products through pairwise G-testing. We know
learning sums is efficient: $k$-means takes $\bigo(n\cdot k\cdot m\cdot c)$ time, where $k$ is the
number of clusters and $c$ the number of iterations to be run. Products, on the other hand, are
much more costly. The naÃ¯ve approach would be to compute whether $X_i\indep X_j$ for every possible
combination. This is clearly quadratic on the number of variables
$\bigo\left(\binom{m}{2}=\frac{m!}{2(m-2)!}\right)$ assuming an $\bigo(1)$ oracle for independence
testing. In reality, G-test takes $\bigo(n\cdot m)$ time, as we must compute a ratio of observed
versus expected values for each cell in the contingency table. This brings the total runtime for
products to a whopping $\bigo\left(n\cdot m^3\right)$, prohibitive to any reasonably large dataset.
In terms of space, independence tests most commonly used require either a correlation (for
continuous data) or contingency (for discrete data) matrix that takes up $\bigo(m^2)$ space,
another barrier for scaling up to high dimensional data.

Alternatively, instead of computing the G-test for every possible combination of variables,
\citep{gens13} constructs an independence graph $\mathcal{G}$ whose nodes are variables and edges
indicate whether two variables are statistically dependent. Within this context, the variable
partitions we attribute to product children are exactly the connected components of $\mathcal{G}$,
meaning it suffices testing only some combinations. Even so, this heuristic is still quadratic
worst case. \Cref{fig:indepgraph} shows $\mathcal{G}$, the spanning forest resulted from the
connected component heuristic, and the equivalent product node from this decomposition.

\begin{figure}[t]
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=8,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxgreen] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxred!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
      \node[circle,fill=boxgray] (p6) at (p.corner 6) {$F$};
      \node[circle,fill=boxgoldenrod!70] (p7) at (p.corner 7) {$G$};
      \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};
      \draw[thick] (p1) -- (p2) -- (p3); \draw[thick] (p1) -- (p3);
      \draw[thick] (p3) -- (p4); \draw[thick] (p1) -- (p4);
      \draw[thick] (p5) -- (p6) -- (p7); \draw[thick] (p5) -- (p7);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=8,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxgreen] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxred!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
      \node[circle,fill=boxgray] (p6) at (p.corner 6) {$F$};
      \node[circle,fill=boxgoldenrod!70] (p7) at (p.corner 7) {$G$};
      \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};
      \draw[thick] (p1) -- (p2); \draw[thick] (p1) -- (p3);
      \draw[thick] (p1) -- (p4); \draw[thick] (p1) -- (p4);
      \draw[thick] (p5) -- (p6); \draw[thick] (p5) -- (p7);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \begin{tikzpicture}
      \newProdNode[fill=boxgreen]{r}{0,0};
      \newSumNode[label=below:{$\{A,B,C,D\}$},fill=boxorange!80]{s1}{$(r) + (-1.25,-2.5)$};
      \newSumNode[label=below:{$\{E,F,G\}$},fill=boxpink!50]{s2}{$(r) + (0,-1.75)$};
      \newSumNode[label=below:{$\{H\}$},fill=boxblue!50]{s3}{$(r) + (1.25,-1.0)$};
      \draw[edge] (r) -- (s1); \draw[edge] (r) -- (s2); \draw[edge] (r) -- (s3);
    \end{tikzpicture}
    \caption{}
  \end{subfigure}
  \caption{The pairwise (in)dependence graph where each node is a variable. In (a) we show the full
    graph, computing independence tests for each pair of variables in $\bigo(m^2)$. However, it
    suffices to compute for only the connected components (b), saving up pairwise computation time
    for reachable nodes.  The resulting product node and scope partitioning is shown in (c).}
  \label{fig:indepgraph}
\end{figure}

\subsubsection{Pros and cons}

\paragraph{Pros.} Perhaps the main factor for \textproc{LearnSPN}'s popularity is how easily
implementable, intuitive and modular it is. Even more remarkably, it is an empirically competitive
PC learning algorithm despite its age, serving as a baseline for most subsequent works in PC
literature. Lastly, the fact that each recursive call from \textproc{LearnSPN} is completely
independent from each the other makes it an attractive candidate for CPU parallelization.

\paragraph{Cons.} Debatably, one of the key weakness of \textproc{LearnSPN} is its tree-shaped
computational graph, meaning that they are strictly less succint compared to non-tree DAG PCs
\citep{martens14}. In terms of runtime efficiency, the algorithm struggles on high dimensional
data due to the complexity involved in computing costly statistical tests. Despite
\Cref{alg:learnspn} giving the impression that no hyperparameter tuning is needed for
\textproc{LearnSPN}, in practice the modules for learning sums and products often take many
parameters, most of which (if not all) are exactly the same for every recursive call. This can have
a negative impact on the algorithm's performance, since the same parameters are repeatedly used
even under completely different data.

\subsection{\textproc{ID-SPN}}

A subtle yet effective way of improving the performance of \textproc{LearnSPN} is to consider
tractable probabilistic models over many variables as input nodes instead of univariate
distributions. \textproc{ID-SPN} \citep{rooshenas14} does so by assuming that input nodes are
Markov networks. Further, instead of blindly applying the recursion over subsequent sub-data, it
attempts to compute some metric of quality from each node. The worst scored node is then replaced
with a \textproc{LearnSPN}-like tree. This is repeated until no significant increase in likelihood
is observed. \Cref{alg:idspn} shows the \textproc{ID-SPN} pipeline, where \textproc{ExtendID} is
used in line \ref{alg:idspn:line:extend} to grow the circuit in a divide-and-conquer fashion. The
name \textproc{ID-SPN} comes from \emph{direct} variable interactions, meaning the relationships
modeled through the Markov networks as input nodes; and \emph{indirect} interactions brought from
the latent variable interpretation of sum nodes.

\begin{algorithm}[t]
  \caption{\textproc{ExtendID}}\label{alg:extendid}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$, and memoization
      function $\mathcal{M}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \State Find scope partitions $\set{X}_1,\ldots,\set{X}_t\subseteq\set{X}$ st
    \If{$k>1$}
      \For{each $j\in\left[t\right]$}
        \State $\Node_j\gets\textproc{LearnMarkov}(\set{D}_{:,\set{X}_j},\set{X}_j)$
        \State Associate $\mathcal{M}(\Node_j)$ with $\set{D}_{:,\set{X}_j}$ and $\set{X}_j$
      \EndFor
      \State \textbf{return} $\prod_{j=1}^t \Node_j$
    \Else
      \State Find subsets of data $\set{x}_1,\ldots,\set{x}_k\subseteq\set{D}$ st all assignments
        within $\set{x}_i$ are all similar
      \For{each $i\in\left[k\right]$}
        \State $\Node_i\gets\textproc{LearnMarkov}(\set{x}_i,\set{X})$
        \State Associate $\mathcal{M}(\Node_i)$ with $\set{x}_i$ and $\set{X}$
      \EndFor
      \State \textbf{return} $\sum_{i=1}^k \frac{|\set{x}_i|}{|\set{D}|}\cdot\Node_i$
    \EndIf
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
  \caption{\textproc{ID-SPN}}\label{alg:idspn}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \State Create a single-node PC: $\mathcal{C}\gets\textproc{LearnMarkov}(\set{D},\set{X})$
    \State Let $\mathcal{M}$ a memoization function associating a node with a dataset and scope
    \State Call $\mathcal{C}'$ a copy of $\mathcal{C}$
    \While{improving $\mathcal{C}$ yields better likelihood}
      \State Pick worse node $\Node$ from $\mathcal{C}'$
      \State Extract sub-data $\set{D}'$ and sub-scope $\set{X}'$ from $\mathcal{M}(\Node)$
      \State Replace $\Node$ with $\textproc{ExtendID}(\set{D}',\set{X}',\mathcal{M})$\label{alg:idspn:line:extend}
      \IIf{$\mathcal{C}'$ has better likelihood than $\mathcal{C}$}{$\mathcal{C}\gets\mathcal{C}$}
    \EndWhile
    \State \textbf{return} $\mathcal{C}$
  \end{algorithmic}
\end{algorithm}

With respect to its implementation, \textproc{ID-SPN} is as modular as \textproc{LearnSPN} in the
sense that the data partitioning is left as a subroutine. Indeed, even the choice of input
distributions is customizable: although \citeauthor{rooshenas14} recommend Markov networks, any
tractable distribution will do. Despite this seemingly small change compared to the original
\textproc{LearnSPN} algorithm, \textproc{ID-SPN} seems to perform better compared to its
counterpart most of the time \citep{rooshenas14,jaini18a}, although at a cost to learning speed.
Further, because of the enormous parameter space brought by having to learn Markov networks as
inputs \emph{and} perform the optimizations from sums and products, grid search hyperparameter
tuning is infeasible. \citep{rooshenas14} recommend random search \citep{bergstra12a} as an
alternative.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    % Markov 1
    \node[fill=boxteal] (a) at (0,0) {$A$};
    \node[fill=boxorange!80] (b) at ($(a) + (1,-1)$) {$B$};
    \node[fill=boxpink!50] (c) at ($(a) + (1,0)$) {$C$};
    \node[fill=boxgoldenrod!70] (d) at ($(a) + (0,-1)$) {$D$};
    \node[fill=boxred!70] (e) at ($(d) + (0,-1)$) {$E$};
    \node[fill=boxpurple!60] (f) at ($(b) + (0,-1)$) {$F$};
    \draw (a) -- (b); \draw (b) -- (c);
    \draw (a) -- (c); \draw (a) -- (d);
    \draw (e) -- (f); \draw (f) -- (d);
    \draw[red,very thick,dashed] (-0.5,0.5) rectangle ($(f) + (0.5,-0.5)$);
    \node at ($(e) + (0.5,-1)$) {Initial Markov network};

    \draw[edge,line width=0.2cm,red] (2,-1) -- (3.5,-1);

    \newProdNode[fill=boxgreen]{r}{6.25,0};

    % Markov 2
    \node[fill=boxpink!50] (a) at ($(r) + (0.75,-1)$) {$A$};
    \node[fill=boxteal] (b) at ($(a) + (1,0)$) {$B$};
    \node[fill=boxorange!80] (c) at ($(b) + (0,-1)$) {$C$};
    \draw (a) -- (b); \draw (b) -- (c);
    \draw[edge] (r) -- ($(a) + (0.5,0.5)$);
    \draw[red,very thick,dashed] ($(a) + (-0.5,0.5)$) rectangle ($(c) + (0.5,-0.5)$);

    % Markov 3
    \node[fill=boxgoldenrod!70] (f) at ($(r) + (-0.75,-1)$) {$F$};
    \node[fill=boxred!70] (d) at ($(f) + (-1,0)$) {$D$};
    \node[fill=boxpurple!60] (e) at ($(d) + (1,-1)$) {$E$};
    \draw (d) -- (f); \draw (e) -- (d);
    \draw[edge] (r) -- ($(d) + (0.5,0.5)$);
    \draw[boxdgray,thick,dashed] ($(d) + (-0.5,0.5)$) rectangle ($(e) + (0.5,-0.5)$);

    \node (i1) at ($(r) + (0,-3)$) {Iteration 1};

    \draw[edge,line width=0.2cm,red] ($(b) + (1,0)$) -- ($(b) + (2.5,0)$);

    \newProdNode[fill=boxgreen]{r}{14.5,0.5};
    \newSumNode[fill=boxbrown!60]{s}{$(r) + (2.0,-0.5)$};
    \draw[edge] (r) -- (s);

    \begin{scope}[local bounding box=m1]
      \node[fill=boxorange!80] (b) at ($(s) + (-1.5,-1)$) {$B$};
      \node[fill=boxteal] (a) at ($(b) + (-1,0)$) {$A$};
      \node[fill=boxpink!50] (c) at ($(b) + (0,-1)$) {$C$};
      \draw (a) -- (b); \draw (a) -- (c);
      \draw[boxdgray,thick,dashed] ($(a) + (-0.5,0.5)$) rectangle ($(c) + (0.5,-0.5)$);
    \end{scope}
    \draw[edge] (s) -- (m1.north);

    \begin{scope}[local bounding box=m2]
      \node[fill=boxteal] (a) at ($(s) + (0.0,-1.5)$) {$A$};
      \node[fill=boxorange!80] (b) at ($(a) + (1,0)$) {$B$};
      \node[fill=boxpink!50] (c) at ($(b) + (0,-1)$) {$C$};
      \draw (a) -- (c); \draw (b) -- (c); \draw (a) -- (b);
      \draw[boxdgray,thick,dashed] ($(a) + (-0.5,0.5)$) rectangle ($(c) + (0.5,-0.5)$);
    \end{scope}
    \draw[edge] (s) -- (m2.north);

    % Markov 3
    \node[fill=boxgoldenrod!70] (f) at ($(r) + (-2,-1)$) {$F$};
    \node[fill=boxred!70] (d) at ($(f) + (-1,0)$) {$D$};
    \node[fill=boxpurple!60] (e) at ($(d) + (1,-1)$) {$E$};
    \draw (d) -- (f); \draw (e) -- (d);
    \draw[edge] (r) -- ($(d) + (0.5,0.5)$);
    \draw[boxdgray,thick,dashed] ($(d) + (-0.5,0.5)$) rectangle ($(e) + (0.5,-0.5)$);

    \node at ($(r) + (0,-3.5)$) {Iteration 2};
  \end{tikzpicture}
  }
  \caption{Two iterations of \textproc{ID-SPN}, where the contents inside the dashed line are
  Markov networks. The red color indicates that a node has been chosen as the best candidate for an
  extension with \textproc{ExtendID}. Although here we only extend input nodes, inner nodes can in
  fact be extended as well.}
\end{figure}

\subsubsection{Complexity}

As \textproc{ID-SPN} is a special case of \textproc{LearnSPN}, the analysis for the sums and
products subroutines holds. The only difference is on the runtime complexity for learning input
nodes and the convergence rate for \textproc{ID-SPN}. Assuming input nodes are learned from the
method suggested by \citet{rooshenas14}, which involves learning a probabilistic circuit from a
Markov network \citep{lowd13a}, then each ``input'' node takes time $\bigo(i\cdot c(k\cdot n+m))$,
where $i$ is the number of iterations to run, $c$ is the size of the generated PC, and constant $k$
is a bound on the number of candidate improvements to the circuit, which can grow exponentially for
multi-valued variables. Importantly, opposite from \textproc{LearnSPN} where we only learn input
nodes once per call \emph{if} data is univariate, \textproc{ID-SPN} requires learning multiple
multivariate inputs for \emph{every} \textproc{ExtendID} call.

\subsubsection{Pros and Cons}

\paragraph{Pros.} If we assume any multivariate distribution in place of Markov networks, PCs
learned from \textproc{ID-SPN} are strictly more expressive than ones learned from
\textproc{LearnSPN}, as input nodes could potentially be replaced with \textproc{LearnSPN}
distributions. Additionally, the modularity inherited from \textproc{LearnSPN} allows
\textproc{ID-SPN} to adapt to data according to expert knowledge, bringing some flexibility to the
algorithm.

\paragraph{Cons.} Unfortunately, most of the disadvantages from \textproc{LearnSPN} also apply to
\textproc{ID-SPN}. Just like \textproc{LearnSPN}, independence tests are more often than not a
bottleneck for most executions with resonably large number of variables. However, \textproc{ID-SPN}
relies on a likelihood improvement for the computational graph to be extended, which ends up
curbing the easy parallelization aspect of \textproc{LearnSPN}. Besides, the complexity involved in
learning Markov networks (or any other complex multivariate distribution as input node) carries a
heavy weight during learning. This, coupled with the fact that hyperparameter tuning in the huge
parameter space of \textproc{ID-SPN} must be done by a random search method, can take a heavy price
in terms of learning time.

\subsection{\textproc{Prometheus}}

So far, we have only considered structure learning algorithms that produce tree-shaped circuits.
Even though \textproc{ID-SPN} \emph{might} produce non-tree graphs at the input nodes depending on
the choice of families of multivariate distributions, it does not do so as a rule. We now turn our
attention to a PC learner that \emph{does} generate non-tree computational graphs in a
divide-and-conquer manner.

Recall that in both \textproc{LearnSPN} and \textproc{ID-SPN} the scope partitioning is done
greedily; we define a graph encoding the pairwise (in)dependencies of variables and greedily search
for connected components by comparing independence test results with some correlation threshold,
adding an edge if the correlation is sufficiently high. The choice of this threshold is often
arbitrary and subject to hyperparameter tuning during learning, which is especially worrying when
dealing with high dimensional data. In this section we review \textproc{Prometheus}
\citep{jaini18a}, a divide-and-conquer \textproc{LearnSPN}-like PC learning algorithm with two main
features that stand out compared to the last two methods we have seen so far: (1) it requires no
hyperparameter tuning for variable partitionings, and (2) accepts a more scalable alternative to
computing all pairwise correlations.

\begin{figure}[t]
  \begin{subfigure}[t]{0.25\textwidth}
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=5,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxred!70] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxpink!50] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxgoldenrod!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxblue!50] (p5) at (p.corner 5) {$E$};
      \draw[thick] (p1) -- node[midway,above left] {$0.8$} (p2);
      \draw[thick] (p1) -- node[midway,left,yshift=0.1cm] {$0.2$} (p3);
      \draw[thick] (p1) -- node[midway,right,yshift=0.1cm] {$0.4$} (p4);
      \draw[thick] (p1) -- node[midway,above right] {$0.3$} (p5);
      \draw[thick] (p2) -- node[midway,below left] {$0.6$} (p3);
      \draw[thick] (p2) -- node[midway,below left,xshift=0.2cm] {$0.4$} (p4);
      \draw[thick] (p2) -- node[midway,above] {$0.1$} (p5);
      \draw[thick] (p3) -- node[midway,below] {$0.9$} (p4);
      \draw[thick] (p3) -- node[midway,below right,xshift=-0.2cm] {$0.5$} (p5);
      \draw[thick] (p4) -- node[midway,right] {$0.7$} (p5);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.25\textwidth}
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=5,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxred!70] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxpink!50] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxgoldenrod!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxblue!50] (p5) at (p.corner 5) {$E$};
      \draw[very thick,blue] (p1) -- node[midway,above left,xshift=-0.1cm,yshift=0.1cm] {$\mathbf{0.8}$} node[fill=white,inner sep=1pt] {$e_2$} (p2);
      \draw[very thick,red] (p2) -- node[midway,left,xshift=-0.1cm,yshift=-0.1cm] {$\mathbf{0.6}$} node[fill=white,inner sep=1pt] {$e_4$} (p3);
      \draw[very thick,boxdgray] (p3) -- node[midway,below,yshift=-0.1cm] {$\mathbf{0.9}$} node[fill=white,inner sep=1pt] {$e_1$} (p4);
      \draw[very thick,orange!80] (p4) -- node[midway,right,xshift=0.1cm,yshift=-0.1cm] {$\mathbf{0.7}$} node[fill=white,inner sep=1pt] {$e_3$} (p5);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newSumNode[fill=boxgreen!75]{r}{0,0};
      \newProdNode[fill=boxdgray!80]{p1}{$(r) + (-3,-1)$};
      \newProdNode[fill=blue!50]{p2}{$(r) + (-1,-1)$};
      \newProdNode[fill=orange!80]{p3}{$(r) + (1,-1)$};
      \newProdNode[fill=red!60]{p4}{$(r) + (3,-1)$};
      \newSumNode[label=below:{$\{A,B,C\}$},fill=boxgreen!75]{s1}{$(r) + (-3.5,-3.0)$};
      \newSumNode[label=below:{$\{D,E\}$},fill=boxgreen!75]{s2}{$(r) + (-2.5,-2.5)$};
      \newSumNode[label=below:{$\{B,C\}$},fill=boxgreen!75]{s3}{$(r) + (-1.5,-2.0)$};
      \newSumNode[label=below:{$\{A\}$},fill=boxgreen!75]{s4}{$(r) + (-0.5,-2.0)$};
      \newSumNode[label=below:{$\{D\}$},fill=boxgreen!75]{s5}{$(r) + (0.5,-2.0)$};
      \newSumNode[label=below:{$\{E\}$},fill=boxgreen!75]{s6}{$(r) + (1.5,-2.0)$};
      \newSumNode[label=below:{$\{B\}$},fill=boxgreen!75]{s7}{$(r) + (2.5,-2.0)$};
      \newSumNode[label=below:{$\{C\}$},fill=boxgreen!75]{s8}{$(r) + (3.5,-2.0)$};
      \draw[edge] (r) -- (p1.north);
      \draw[edge] (r) -- (p2.north);
      \draw[edge] (r) -- (p3.north);
      \draw[edge] (r) -- (p4.north);
      \draw[edge,blue!80] (p1) -- (s1.north);
      \draw[edge,blue!80] (p1) -- (s2.north);
      \draw[edge,red!80] (p2) -- (s2.north);
      \draw[edge,red!80] (p2) -- (s3.north);
      \draw[edge,red!80] (p2) -- (s4.north);
      \draw[edge,boxdgray!80] (p3) -- (s3.north);
      \draw[edge,boxdgray!80] (p3) -- (s4.north);
      \draw[edge,boxdgray!80] (p3) -- (s5.north);
      \draw[edge,boxdgray!80] (p3) -- (s6.north);
      \draw[edge,orange!80] (p4) -- (s4.north);
      \draw[edge,orange!80] (p4) -- (s5.north);
      \draw[edge,orange!80] (p4) -- (s6.north);
      \draw[edge,orange!80] (p4) -- (s7.north);
      \draw[edge,orange!80] (p4) -- (s8.north);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \caption{The fully connected correlation graph (a) with weights as the pairwise correlation
    measurements for each pair of variables; the maximum spanning tree for determining
    decompositions (b); and the mixture of decompositions (c). Colors in (b) match their
    partitionings in (c).}
  \label{fig:prometheus}
\end{figure}

Let $\mathcal{G}$ the independence graph for scope $\set{X}=\{X_1,X_2,\ldots,X_m\}$. Remember that
$\mathcal{G}$'s vertices are $\set{X}$ and each (undirected) edge $\overline{X_i X_j}$ coming from
$X_i$ to $X_j$ means that $X_i\notindep X_j$. Previously, we constructed $\mathcal{G}$ by comparing
the output of an independence test (such as the G-test) against a threshold (e.g.\ a sufficiently
low $p$-value). Instead, suppose $\mathcal{G}$ is fully connected and that we attribute weights
corresponding to a correlation metric of $X_i$ against $X_j$ for each edge (e.g.\ Pearson's
correlation coefficient). The \emph{maximum spanning tree} (MST) of $\mathcal{G}$, here denoted by
$\mathcal{T}$, defines a graph where the removal of any edge in $\mathcal{T}$ partitions the
component into two subcomponents. Let $e_i$ the $i$-th lowest (weight) valued edge;
\textproc{Prometheus} obtains a set of decompositions by iteratively removing edges from $e_1$ to
$e_{|\set{X}|-1}$. In other words, the algorithm constructs a product node for each decomposition,
assigning the scope of each child as the scope of each component at each edge removal. These
products are then joined together by a parent sum node that acts as a mixture of decompositions.
\Cref{fig:prometheus} shows an example of $\mathcal{T}$, the subsequent decompositions, and the
resulting mixture of decompositions.

\begin{algorithm}[t]
  \caption{\textproc{Prometheus}}\label{alg:prometheus}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \IIf{$|\set{X}|$ is sufficiently small}{\textbf{return} an input node learned from $\set{D}$}
    \NIElse
      \State Find subsets of data $\set{x}_1,\ldots,\set{x}_k\subseteq\set{D}$ st all assignments
        within $\set{x}_i$ are all similar
      \State Create a sum node $\Sum$ with initially no children and uniform weights
      \For{each $\set{x}_i$}
        \State $\mathcal{T}\gets\textproc{CorrelationMST}(\set{x}_i,\set{X})$
        \For{each weighted edge $e_j$ in $\mathcal{T}$ in decreasing order}
          \State Remove edge $e_i$ from $\mathcal{T}$
          \State Call $\set{S}_1,\ldots,\set{S}_t$ the scopes of each component in $\mathcal{T}$
          \State Create product node $\Prod_j$ and associate it with $\set{S}_1,\ldots,\set{S}_t$
          \State Associate $\Prod_j$ with dataset $\set{x}_i$
          \State Add $\Prod_j$ as a child of $\Sum$
        \EndFor
      \EndFor
      \State Let $\mathcal{H}$ a hash table (initially empty) associating scopes to sum nodes
      \For{each $\Prod\in\Ch(\Sum)$}
        \For{each scope $\set{S}$ associated with $\Prod$}
          \If{$\set{S}\not\in\mathcal{H}$}
            \State Let $\set{x}$ the dataset associated with $\Prod$
            \State $\Node\gets\textproc{Prometheus}(\set{x}_{:,\set{S}},\set{S})$
            \State Add $\Node$ as a child of $\Prod$
            \State $\mathcal{H}_\set{S}\gets\Node$
          \Else
            \State Add $\mathcal{H}_\set{S}$ as a child of $\Prod$
          \EndIf
        \EndFor
      \EndFor
      \State \textbf{return} $\Sum$
    \EndNIElse
  \end{algorithmic}
\end{algorithm}

Sum nodes are learned by clustering data into similar instances, just like in previous cases. Since
the previously mentioned procedure involving products creates a mixture of decompositions (and thus
a sum node), we can simply collapse the consecutive sum layers into a single sum node.
\Cref{alg:prometheus} shows the algorithm in its entirety. \textproc{CorrelationMST} computes the
(fully connected) correlation graph, returning its MST. It is worth mentioning that
\textproc{Prometheus} makes sure each recursive call shares subcircuits whenever scopes are the
same (this is when the hash table $\mathcal{H}$ in \Cref{alg:prometheus} comes into play). This
avoids an exponential growth from the $k\cdot(|\set{X}|-1)$ potential recursive calls.

\subsubsection{Complexity}

Up to now, the computation of decompositions is done by a $\bigo(m^2)$ construction of a fully
connected correlation graph. This gives \textproc{Prometheus} no asymptotic advantage over neither
\textproc{LearnSPN} nor \textproc{ID-SPN}. To change this, \citeauthor{jaini18a} propose a more
scalable alternative: in place of constructing the entire correlation graph, sample $m\log m$
variables and construct a correlation graph where only $\log m$ edges are added for each of these
sampled variables instead, bringing down complexity to $\bigo(m\left(\log m\right)^2)$.

The analysis of sum nodes is exactly the same as \textproc{LearnSPN} if we assume the same
clustering method. If \textproc{Prometheus} is implemented with the same multivariate distributions
as \textproc{ID-SPN} at the input nodes, the analysis for those also holds.

\subsubsection{Pros and Cons}

\paragraph{Pros.} The notable achievements of \textproc{Prometheus} are evidently the absence of
parameters for computing scope partitionings, reducing the dimension of hyperparameters to tune; a
scalable alternative to partitionings that runs in sub-quadratic time; and (more debatably) the
fact that the algorithm produces non-tree shaped computational graphs. Further, since product nodes
are learned through correlation metrics, \textproc{Prometheus} is easily adaptable to continuous
data. To some extent, \textproc{Prometheus} also inherits the modularity of $\textproc{LearnSPN}$,
as the choice of how to cluster and what input nodes to use is open to the the user.

\paragraph{Cons.} Although the construction of the correlation graph in \textproc{Prometheus} is
not done greedily (at least in the quadratic version), selecting the decompositions (i.e.\
partitioning the graph into maximal components) is; of course, this is not exactly a drawback but a
compromise, as graph partitioning is a known NP-hard problem \citep{feldmann15}. Because
\textproc{Prometheus} accounts for all decompositions yielded from components after the removal of
each edge from the MST, the circuit can grow considerably, even if we reuse subcircuits at each
recursive call. An alternative would be to globally reuse subcircuits (i.e.\ share $\mathcal{H}$
among different recursive calls) throughout learning, although this curbs expressivity somewhat, as
these subcircuits are learned from possibly (completely) different data. Another option would be to
bound the number of decompositions, or in other words remove only a bounded number of edges from
the MST.

\begin{remark}[breakable]{On variations of divide-and-conquer learning}{divconq}
  Because of \textproc{LearnSPN}'s simplicity and modularity, there is a lot of room for
  improvement. This is reflected in the many works in literature on refining \textproc{LearnSPN} to
  specific data, choosing the right parameters, producing non-tree shaped circuits, and choice of
  input nodes. In this remark segment, we briefly discuss other advances in divide-and-conquer PC
  learning.

  As we have previously mentioned, one of the drawbacks of \textproc{LearnSPN} is the sheer volume
  of hyperparameters involved. \citet{vergari15} suggests simplifying clustering to only binary row
  splits, while \citet{liu19} proposes clustering methods that automatically decide the number of
  clusters from data. Together with \textproc{Prometheus}, the space of hyperparameters to tune is
  greatly reduced.

  We again go back to the issue of reducing the cost of learning variable partitions. Apart
  from \textproc{Prometheus}, \citet{dimauro17a} also investigate more efficient decompositions,
  proposing two approximate sub-quadratic methods to producing variable splits: one by randomly
  sampling pairs of variables and running G-test, and the other by a linear time entropy criterion.

  \citet{vergari15} proposes the use of Chow-Liu Trees as input nodes instead of univariate
  distributions, while \citet{molina17} recommend Poisson distributions for modeling negative
  dependence. \citet{bueff18} combines \textproc{LearnSPN} with weighted model integration by
  learning polynomials as input nodes for continuous variables and counts for discrete data.
  \citet{molina18} adapts \textproc{LearnSPN} to hybrid domains by employing the randomized
  dependence coefficient for both clustering and variable partitioning, with pairwise polynomial
  approximations for input nodes.

  Other contributions include adapting \textproc{LearnSPN} to relational data \citep{nath15}, an
  empirical study comparing different techniques for clustering and partitioning in
  \textproc{LearnSPN} \citep{butz18a}, and \textproc{LearnSPN} post-processing strategies for
  deriving non-tree graphs \citep{tahrima16}.
\end{remark}

\section{Incremental Learning}
\label{sec:incremental}

Learning algorithms from the \divclass{} class heavily rely on recursively constructing a
probabilistic circuit in a top-down fashion. This facilitates learning, as we need only to greedily
optimize at a local level. We now draw our attention to incremental algorithms that iteratively
grow an initial circuit. These usually require a search over possible candidate nodes to be
extended, and as such involve evaluating the entire circuit to determine best scores. In this
section, we look at two examples of \incrclass{} class learning algorithms: \textproc{LearnPSDD}
and \textproc{Strudel}.

\subsection{\textproc{LearnPSDD}}

As the name suggests, \textproc{LearnPSDD} \citep{liang17} learns a smooth, structure decomposable
and deterministic probabilistic circuit (see \Cref{subsection:touncertainty}), meaning its
computational graph must respect a vtree. We therefore must address the issue of learning the vtree
before we turn to the PC learning algorithm \emph{per se}.

Recall that for a vtree $\vtree$, every inner node $v\in\vtree$ with $\set{X}=\Sc(v^\gets)$ and
$\set{Y}=\Sc(v^\to)$ determines that $\set{X}$ and $\set{Y}$ are independent, i.e.\
$p_\mathcal{C}(\set{X},\set{Y})=p_\mathcal{C}(\set{X}) p_\mathcal{C}(\set{Y})$ for a PC
$\mathcal{C}$. This means that a PC's vtree is pivotal in embedding the independencies of the
circuit's distribution. With this in mind, \citet{liang17} propose two approaches to inducing
vtrees from data, both of which use mutual information
\begin{equation*}
  \mutualinf(\set{X},\set{Y})=\sum_{\set{X}=\set{x}}\sum_{\set{Y}=\set{y}}p(\set{x},\set{y})\log\frac{p(\set{x},\set{y})}{p(\set{x})p(\set{y})}
\end{equation*}
for deciding independence. To avoid computing an exponential number of MI terms, an approximation
based on the average pairwise MI is computed instead
\begin{equation*}
  \pairmi(\set{X},\set{Y})=\frac{1}{|\set{X}||\set{Y}|}\cdot\sum_{X\in\set{X}}\sum_{Y\in\set{Y}}\mutualinf(X,Y).
\end{equation*}
The first approach learns vtrees in a top-down fashion, starting with a full scope and recursively
partitioning down to the unit set. The second learns bottom-up, starting with singletons and
joining sets of variables up to full scope.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    \def\ngon{8}
    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,0) {};
    \foreach \i in {1,...,3} {
      \foreach \j in {2,...,3} {
        \draw[gray,thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \foreach \i in {4,...,\ngon} {
      \foreach \j in {5,...,\ngon} {
        \draw[gray,thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \foreach \i in {1,...,3} {
      \foreach \j in {4,...,\ngon} {
        \draw[thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxpink!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxpink!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.5,1)$) {1};
    \node[label=below:{$\{A,B,C\}$},draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-1,-1)$) {2};
    \node[label=below:{$\{D,E,F,G,H\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (1,-1)$) {3};
    \draw (r) -- (vl); \draw (r) -- (vr);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,0) {};
    \draw[gray,thick] (p.corner 4) -- (p.corner 5);
    \draw[gray,thick] (p.corner 6) -- (p.corner 7);
    \draw[gray,thick] (p.corner 6) -- (p.corner 8);
    \draw[gray,thick] (p.corner 7) -- (p.corner 8);
    \foreach \i in {4,...,5} {
      \foreach \j in {6,...,\ngon} {
        \draw[thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxblue!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.5,1.5)$) {1};
    \node[label=below:{$\{A,B,C\}$},draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-1,-1)$) {2};
    \node[draw,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (1,-1.5)$) {3};
    \node[label=below:{$\{D,E\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vrl) at ($(vr) + (-1,-1.5)$) {4};
    \node[label=below:{$\{F,G,H\}$},draw,fill=boxblue!50,inner sep=2pt,minimum size=13pt] (vrr) at ($(vr) + (1,-1.5)$) {5};
    \draw (r) -- (vl); \draw (r) -- (vr); \draw (vr) -- (vrl); \draw (vr) -- (vrr);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,-5.5) {};
    \draw[gray,thick] (p.corner 2) -- (p.corner 1);
    \draw[thick] (p.corner 3) -- (p.corner 1);
    \draw[thick] (p.corner 3) -- (p.corner 2);

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxblue!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.75,1.5)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-0.5,-1)$) {2};
    \node[draw,fill=boxgreen!80,inner sep=2pt,minimum size=13pt] (vll) at ($(vl) + (-1.25,-1)$) {$C$};
    \node[label=below:{$\{A,B\}$},draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vlr) at ($(vl) + (0.0,-1)$) {6};
    \node[draw,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (0.5,-1)$) {3};
    \node[label=below:{$\{D,E\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vrl) at ($(vr) + (-0.0,-1)$) {4};
    \node[label=below:{$\{F,G,H\}$},draw,fill=boxblue!50,inner sep=2pt,minimum size=13pt] (vrr) at ($(vr) + (1.25,-1)$) {5};
    \draw (r) -- (vl); \draw (r) -- (vr); \draw (vr) -- (vrl); \draw (vr) -- (vrr);
    \draw (vl) -- (vll); \draw (vl) -- (vlr);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,-5.5) {};
    \draw[thick] (p.corner 1) -- (p.corner 2);

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxblue!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.75,1.5)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-0.5,-1)$) {2};
    \node[draw,fill=boxgreen!80,inner sep=2pt,minimum size=13pt] (vll) at ($(vl) + (-1.25,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (vlr) at ($(vl) + (0.0,-1)$) {6};
    \node[draw,fill=boxpurple!60,inner sep=2pt,minimum size=13pt] (vlrl) at ($(vlr) + (-1.25,-1)$) {$B$};
    \node[draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vlrr) at ($(vlr) + (-0,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (0.5,-1)$) {3};
    \node[label=below:{$\{D,E\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vrl) at ($(vr) + (-0.0,-1)$) {4};
    \node[label=below:{$\{F,G,H\}$},draw,fill=boxblue!50,inner sep=2pt,minimum size=13pt] (vrr) at ($(vr) + (1.25,-1)$) {5};
    \draw (r) -- (vl); \draw (r) -- (vr); \draw (vr) -- (vrl); \draw (vr) -- (vrr);
    \draw (vl) -- (vll); \draw (vl) -- (vlr); \draw (vlr) -- (vlrl); \draw (vlr) -- (vlrr);
  \end{tikzpicture}
  }
  \caption{Snapshots of four iterations from running the vtree top-down learning strategy with
    pairwise mutual information. Each iteration shows a variable partitioning, the cut-set that
    minimizes the average pairwise mutual information as black edges, and the subsequent (partial)
    vtree. The algorithm finishes when all partitions are singletons.}
  \label{fig:topdownvtree}
\end{figure}

\paragraph{Top-down vtree learning.} Let $\mathcal{G}$ a fully connected weighted graph where
variables are nodes. For each edge $\edge{X}{Y}$, attribute its weight as $\mutualinf(X,Y)$.
Learning the vtree top-down amounts to partitioning $\mathcal{G}$ such that the cut-set that
divides the two partitions $\set{X}$ and $\set{Y}$ is minimal with respect to $\pairmi$.
\citet{liang17} further argue that balanced vtrees produce smaller PCs, and so they reduce learning
to a balanced min-cut bipartition problem. Although this is known to be NP-complete
\citep{garey90}, optimized solvers are able to produce high quality bipartitions efficiently
\citep{karypsis98}. In a nutshell, the vtree construction goes as follows: find a balanced min-cut
bipartition $(\set{X}, \set{Y})$ in $\mathcal{G}$ minimizing the $\pairmi$ of the edges; add a
vtree inner node representing this bipartition and connect it to the two vtrees produced by the
recursive calls over $\set{X}$ and $\set{Y}$; if $\set{X}=\{X\}$ (resp. $|\set{Y}|=\{Y\}$), produce
a leaf node $X$ (resp. $Y$). \Cref{fig:topdownvtree} shows four iterations of this procedure.

\paragraph{Bottom-up vtree learning.} Again, take $\mathcal{G}$ as the fully connected weighted
graph from computing the pairwise mutual information of variables. Now consider that every node of
$\mathcal{G}$ is a vtree whose only node is the variable itself. To learn a vtree bottom-up is to
find pairings of vtrees such that the mutual information between them is high, meaning that the
partitionings at higher levels are minimized (and so determine the ``true'' independence
relationships between subsets of variables). To produce balanced vtrees, the algorithm attempts to
join vtrees of same height whose $\pairmi$ is maximal; this is equivalent to min-cost perfect
matching, which can be solved, in our case, in $\bigo(m^4)$, where $m$ is the number of variables
\citep{edmonds65,kolmogorov09}.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    \def\ngon{8}
    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,0) {};
    \pgfmathsetmacro{\ngonm}{\ngon-1}
    \draw[thick] (p.corner 1) -- (p.corner 2);
    \foreach \i in {3,...,\ngon} {\draw[gray,thick] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {2,...,\ngonm} {
      \pgfmathsetmacro{\k}{int(\i+1)}
      \foreach \j in {\k,...,\ngon} {
        \draw[gray,thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \draw[thick] (p.corner 3) -- (p.corner 4);
    \draw[thick] (p.corner 5) -- (p.corner 6);
    \draw[thick] (p.corner 7) -- (p.corner 8);
    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxred!70] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxgray] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxgoldenrod!70] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.5)$) {};
    \node[draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[fill=boxpurple!60,draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[fill=boxgreen!80,draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[fill=boxred!70,draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2)$) {$E$};
    \node[fill=boxgray,draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2)$) {$F$};
    \node[fill=boxgoldenrod!70,draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2)$) {$G$};
    \node[fill=boxblue!50,draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,0) {};

    \foreach \i in {3,...,4} {\draw[thick] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {3,...,4} {\draw[thick] (p.corner 2) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 2) -- (p.corner \i);}

    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 3) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 4) -- (p.corner \i);}

    \foreach \i in {7,...,\ngon} {\draw[thick,gray] (p.corner 5) -- (p.corner \i);}
    \foreach \i in {7,...,\ngon} {\draw[thick,gray] (p.corner 6) -- (p.corner \i);}

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxgreen!80] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.5)$) {};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[fill=boxgreen!80,draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[fill=boxblue!50,draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[draw,inner sep=2pt,minimum size=13pt] (rr1) at ($(r) + (0,0.5)$) {5};
    \node[draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2)$) {$E$};
    \node[draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2)$) {$F$};
    \node[draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2)$) {$G$};
    \node[draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);
    \draw (rr1) -- (r1); \draw (rr1) -- (r2);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,-5.5) {};

    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 2) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 3) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 4) -- (p.corner \i);}

    \foreach \i in {7,...,\ngon} {\draw[thick] (p.corner 5) -- (p.corner \i);}
    \foreach \i in {7,...,\ngon} {\draw[thick] (p.corner 6) -- (p.corner \i);}

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxorange!80] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.5)$) {};
    \node[draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[fill=boxblue!50,draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (rr1) at ($(r) + (0,0.5)$) {5};
    \node[draw,inner sep=2pt,minimum size=13pt] (rr2) at ($(r) + (0,-3.5)$) {6};
    \node[draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2)$) {$E$};
    \node[draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2)$) {$F$};
    \node[draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2)$) {$G$};
    \node[draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);
    \draw (rr1) -- (r1); \draw (rr1) -- (r2); \draw (rr2) -- (r3); \draw (rr2) -- (r4);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,-5.5) {};

    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 2) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 3) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 4) -- (p.corner \i);}

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxorange!80] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxpink!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxpink!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.75)$) {};
    \node[draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (rr1) at ($(r) + (0,0.5)$) {5};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (rr2) at ($(r) + (0,-3.75)$) {6};
    \node[draw,inner sep=2pt,minimum size=13pt] (rrr) at ($(r) + (0,-1.65)$) {7};
    \node[draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2.25)$) {$E$};
    \node[draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2.25)$) {$F$};
    \node[draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2.25)$) {$G$};
    \node[draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2.25)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);
    \draw (rr1) -- (r1); \draw (rr1) -- (r2); \draw (rr2) -- (r3); \draw (rr2) -- (r4);
    \draw (rrr) -- (rr1); \draw (rrr) -- (rr2);
  \end{tikzpicture}
  }
  \caption{Snapshots from running the vtree bottom-up learning strategy with pairwise mutual
    information. Snapshots show pairings of two vtrees, with edges between partitions joined into
    a single edge whose weight is the average pairwise mutual information of all
    collapsed edges. In black are edges that correspond to the matchings that maximize the average
    pairwise mutual information. The algorithm finishes when all vtrees have been joined together
    into a single tree.}
  \label{fig:bottomupvtree}
\end{figure}

\textproc{LearnPSDD} is an incremental learning algorithm. This means that it takes an existing PC
and incrementally grows the circuit by some criterion, preserving the structural constraints from
the PC in the process. Once a vtree $\vtree$ has been learned from data, we use it to construct an
initial circuit that respects $\vtree$. The choice of circuit initialization is dependent on our
task. For example, within the context of PSDDs, we are mostly interested in starting out with a PC
induced from an LC encoding a certain knowledge base (see \Cref{section:pckb}); this is usually
done in a case-by-case basis, where LCs are compiled for a particular task and then promoted to PCs
(see \Cref{rem:initpc}). However, if one does not require specifying the distribution's support,
any PC will do.

\emph{How} and \emph{where} the circuit is grown -- once we have acquired a vtree and an initial
circuit -- are the main topics of interest now. We first address the matter of \emph{how}, i.e.\
how can we increase a PC's expressivity such that we preserve a desired set of structural
constraints; and later of \emph{where}, i.e.\ which portions of the circuit are eligible for
growth and how do we know they are good candidates.

\begin{figure}[t]
  \begin{center}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newNamedOrNode[scale=1,draw=red,very thick,inputs=nn]{r}{0,0}{$\alpha$};
      \newAndNode[draw=red,very thick,inputs=nn]{p1}{0,-1};
      \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{$(p1.input 1) + (0,-1)$}{$\beta$};
      \newNamedOrNode[inputs=nn]{s2}{$(p1.input 2) + (1.5,-1)$}{$\gamma$};
      \newAndNode[inputs=nn]{p2}{$(s1.input 1) + (-1.0,-1)$};
      \newAndNode[inputs=nn]{p3}{$(s1.input 2) + (1.0,-1)$};
      \newOrNode[inputs=nn]{s3}{$(p2.input 2) + (0.5,-1)$};
      \newOrNode[inputs=nn]{s4}{$(p3.input 2) + (0.5,-1)$};
      \node (a) at ($(p2.input 1) + (0.0,-1)$) {$A$};
      \node (na) at ($(p3.input 1) + (0.0,-1)$) {$\neg A$};
      \draw[draw=red,very thick] (r.west) -- (p1.east);
      \draw[draw=red,very thick] (p1.input 1) -- (s1.east);
      \draw[draw=red,very thick] (p1.input 2) -- ++(0,-0.25) -| (s2.east);
      \draw (s1.input 1) -- ++(0,-0.25) -| (p2.east);
      \draw (s1.input 2) -- ++(0,-0.25) -| (p3.east);
      \draw (p2.input 1) -- (a);
      \draw (p2.input 2) -- ++(0,-0.25) -| (s3.east);
      \draw (p3.input 1) -- (na);
      \draw (p3.input 2) -- ++(0,-0.25) -| (s4.east);

      \node[style={single arrow,draw=red,thick}] (arrow) at (1.75,-0.5)
        {\small\textrm{\textsc{Split}} on $A$};

      \begin{scope}[every node/.style={minimum size=15pt}]
        \newNamedOrNode[draw=red,very thick,inputs=nn]{r}{$(arrow.east) + (1.25,0.5)$}{$\alpha$};
        \newAndNode[draw=red,very thick,inputs=nn]{p1}{$(r) + (0.75,-1)$};
        \newAndNode[draw=red,very thick,inputs=nn]{p12}{$(r) + (-0.75,-1)$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{$(p1.input 1) + (0,-1)$}{\tiny$\beta\wedge\overline{A}$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s12}{$(p12.input 1) + (0,-1)$}{\tiny$\beta\wedge A$};
        \newNamedOrNode[inputs=nn]{s2}{$(p1.input 2) + (1.25,-1)$}{$\gamma$};
        \newAndNode[inputs=nn]{p2}{$(s12.west) + (0,-1)$};
        \newAndNode[inputs=nn]{p3}{$(s1.west) + (0,-1)$};
        \newOrNode[inputs=nn]{s3}{$(p2.input 2) + (0.5,-1)$};
        \newOrNode[inputs=nn]{s4}{$(p3.input 2) + (0.5,-1)$};
      \end{scope}

      \node (a) at ($(p2.input 1) + (0.0,-1)$) {$A$};
      \node (na) at ($(p3.input 1) + (0.0,-1)$) {$\neg A$};
      \draw[draw=red,very thick] (r.input 1) -- ++(0,-0.15) -| (p12.east);
      \draw[draw=red,very thick] (r.input 2) -- ++(0,-0.15) -| (p1.east);
      \draw[draw=red,very thick] (p1.input 1) -- (s1.east);
      \draw[draw=red,very thick] (p1.input 2) -- ++(0,-0.25) -| (s2.east);
      \draw[draw=red,very thick] (p12.input 2) -- ++(0,-0.35) -| (s2.east);
      \draw[draw=red,very thick] (p12.input 1) -- (s12.east);
      \draw (s12.west) -- ++(0,-0.25) -| (p2.east);
      \draw (s1.west) -- ++(0,-0.25) -| (p3.east);
      \draw (p2.input 1) -- (a);
      \draw (p2.input 2) -- ++(0,-0.25) -| (s3.east);
      \draw (p3.input 1) -- (na);
      \draw (p3.input 2) -- ++(0,-0.25) -| (s4.east);

      \draw[dashed,boxdgray,very thick] ($(r) + (2.9,0.5)$) -- ($(r) + (2.9,-5.1)$);

      \begin{scope}[xshift=9cm]
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{0,-1.5}{$\alpha$};
        \newAndNode[inputs=nn]{r1}{-1,0};
        \newAndNode[inputs=nn]{r2}{1,0};
        \newAndNode[inputs=nn]{p1}{$(r1)+(0,-3)$};
        \newAndNode[inputs=nn]{p2}{$(r2)+(0,-3)$};
        \newOrNode[inputs=nn]{l11}{$(p1.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l12}{$(p1.input 2)+(1,-1.5)$};
        \newOrNode[inputs=nn]{l21}{$(p2.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l22}{$(p2.input 2)+(1,-1.5)$};
        \draw (r1.west) -- ++(0,-0.25) -| (s1.east);
        \draw[draw=red,very thick] (r2.west) -- ++(0,-0.25) -| (s1.east);
        \draw (s1.input 1) -- ++(0,-0.25) -| (p1.east);
        \draw (s1.input 2) -- ++(0,-0.25) -| (p2.east);
        \draw (p1.input 1) -- ++(0,-0.25) -| (l11.east);
        \draw (p1.input 2) -- ++(0,-0.25) -| (l12.east);
        \draw (p2.input 1) -- ++(0,-0.25) -| (l21.east);
        \draw (p2.input 2) -- ++(0,-0.25) -| (l22.east);

        \node[style={single arrow,draw=red,thick}] (arrow) at (2.5,-0.5)
          {\small\textrm{\textsc{Clone}}};

        \newAndNode[inputs=nn]{r1}{4.25,0};
        \newAndNode[inputs=nn]{r2}{6.25,0};
        \newAndNode[inputs=nn]{p1}{$(r1)+(-0.5,-3)$};
        \newAndNode[inputs=nn]{p2}{$(r2)+(-0.5,-3)$};
        \newOrNode[inputs=nn]{l11}{$(p1.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l12}{$(p1.input 2)+(1,-1.5)$};
        \newOrNode[inputs=nn]{l21}{$(p2.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l22}{$(p2.input 2)+(1,-1.5)$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{$(r1.west) + (0,-1)$}{$\alpha$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s2}{$(r2.west) + (0,-1)$}{$\alpha$};
        \newAndNode[draw=red,very thick,inputs=nn]{p3}{$(p1)+(1,0)$};
        \newAndNode[draw=red,very thick,inputs=nn]{p4}{$(p2)+(1,0)$};
        \draw (r1.west) -- ++(0,-0.25) -| (s1.east);
        \draw[draw=red,very thick] (r2.west) -- ++(0,-0.25) -| (s2.east);
        \draw (s1.input 1) -- ++(0,-0.5) -| (p1.east);
        \draw (s1.input 2) -- ++(0,-0.5) -| (p2.east);
        \draw[draw=red,very thick] (s2.input 1) -- ++(0,-0.25) -| (p3.east);
        \draw[draw=red,very thick] (s2.input 2) -- ++(0,-0.25) -| (p4.east);
        \draw (p1.input 1) -- ++(0,-0.25) -| (l11.east);
        \draw (p1.input 2) -- ++(0,-0.5) -| (l12.east);
        \draw (p2.input 1) -- ++(0,-0.25) -| (l21.east);
        \draw (p2.input 2) -- ++(0,-0.5) -| (l22.east);
        \draw[draw=red,very thick] (p3.input 1) -- ++(0,-0.25) -| (l11.east);
        \draw[draw=red,very thick] (p3.input 2) -- ++(0,-0.5) -| (l12.east);
        \draw[draw=red,very thick] (p4.input 1) -- ++(0,-0.25) -| (l21.east);
        \draw[draw=red,very thick] (p4.input 2) -- ++(0,-0.5) -| (l22.east);
      \end{scope}
    \end{tikzpicture}
    }
  \end{center}
  \caption{\textproc{Split} (left) and \textproc{Clone} (right) operations for growing a circuit
    when $m=1$. Nodes and edges highlighted in red show the modified structure. In both cases
  smoothness, (structure) decomposability and determinism are evidently preserved.}
  \label{fig:splitclone}
\end{figure}

\citeauthor{liang17} propose two local transformations for growing a circuit $\mathcal{C}$:
\textproc{Split} and \textproc{Clone}. The first acts by multiplying a sum node's product child
$\Prod$ into $\Prod_1,\ldots,\Prod_k$ products such that $\pi_1,\ldots,\pi_k$ (primes of
$\Prod_1,\ldots,\Prod_k$ respectively) are mutually exclusive. This is done by attributing all
possible values of a variable in $\Sc(\Prod)$, say $A$, to each prime, meaning that $\pi_i$ will
contain the assignment $A=i$ for every $i\in\left[k\right]$. This attribution is done by partially
copying $\mathcal{C}_{\Prod}$ into $k$ circuits $\mathcal{C}_{\Prod}^{(1)},\ldots,
\mathcal{C}_{\Prod}^{(k)}$ up to some depth $m$ and then conditioning $\mathcal{C}_{\Prod}^{(i)}$
on $\liv A=i\riv$. This is straightforward for the discrete case: at the appropriate vtree node
(i.e.\ one that contains $A$ as a leaf), replace the input node whose scope is $A$ into an
indicator node, setting it to the appropriate assignment of $A$. Although \citet{liang17} only
considers the binary case, the transformation can be extended to the continuous if we consider $k$
piecewise distributions whose support is over only a set interval. Naturally, input nodes must then
have their support truncated to the appropriate $i$-th interval, which is no easy feat in the
general case. The left side of \Cref{fig:splitclone} shows \textproc{Split} for the binary case.

The other proposed transformation, \textproc{Clone}, does something similar for sum nodes. Pick a
sum node $\Sum$ whose children are $\Child_1,\ldots,\Child_k$ and parents $\Prod_1$ and $\Prod_2$;
double $\Sum$ and $\Child_1,\ldots,\Child_k$, producing clones $\Sum'$ and $\Child_1',\ldots,
\Child_k'$. Disconnect the edge coming from $\Prod_2$ to $\Sum$ and instead connect it to $\Sum'$.
Connect all $\Child_1',\ldots,\Child_k'$ to the same children as their original counterparts. This
operation is visualized on the right side of \Cref{fig:splitclone}. One can further extend
\textproc{Clone} to apply this operation cloning nodes up to some depth $m$ and then joining the
last remaining deepest nodes similar to what was described for $\Child_1',\ldots,\Child_k'$.

It is easy to see that, in both cases, smoothness, structure decomposability and determinism are
preserved. In fact, if the original circuit encodes a particular support (i.e.\ a knowledge base),
the PC resulting from applying any of the two transformations must also encode the same support,
since we have only made the underlying logic circuit more redundant. Probabilistically though, this
``redudancy'' only increases the parameterization space and as such increases the expressiveness of
the PC. However, not all applications of \textproc{Split} or \textproc{Clone} are equal in terms of
performance. While it is true that the application of \textproc{Split} to any product node or
\textproc{Clone} to any sum node strictly increases expressivity, it is more meaningful to choose
candidates whose growth carries a bigger impact on the overall fit relative to the training data.
\textproc{LearnPSDD} searches for reasonable candidates by computing
\begin{equation*}
  \score(\set{D}, \mathcal{C}, \mathcal{C}')=\frac{\log\mathcal{C}'(\set{D})-\log
  \mathcal{C}(\set{D})}{|\mathcal{C}'|-|\mathcal{C}|},
\end{equation*}
where $\mathcal{C}$ and $\mathcal{C}'$ are, respectively, the PCs before and after the application
of any of the two operations. In other words, the algorithm randomly evaluates applying
\textproc{Split} and/or \textproc{Clone} and ultimately chooses the one candidate that maximizes
the log-likelihood of training data penalized by the size of the resulting PC, iteratively growing
the circuit until there is no more improvement or reaches an iteration step or time limit, as
\Cref{alg:learnpsdd} shows.

\begin{algorithm}[t]
  \caption{\textproc{LearnPSDD}}\label{alg:learnpsdd}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, vtree $\vtree$, initial PC $\mathcal{C}$, max depth $m$, scope $\set{X}$
    \Ensure A smooth, structure decomposable and deterministic PC learned from $\set{D}$
    \While{there is score improvement or has not reached the iteration/time limit}
      \State $s_{\Sum}\gets -\infty$
      \State Let $(\Sum^\ast,\Prod^\ast)$ the best \textproc{Split} candidate seen so far, initially empty
      \For{each candidate $(\Sum,\Prod)$ of all possible \textproc{Split} candidates}
        \State $\mathcal{C}'\gets\textproc{Split}(\mathcal{C},\Sum,\Prod,\vtree,m)$
        \State $s'\gets\score(\set{D},\mathcal{C},\mathcal{C}')$
        \IIf{$s'>s_{\Sum}$}{$s_{\Sum}\gets s'$ and $\Sum^\ast,\Prod^\ast\gets\Sum,\Prod$}
      \EndFor
      \State $s_{\Child}\gets -\infty$
      \State Let $\Child^\ast$ the best \textproc{Clone} candidate seen so far, initially empty
      \For{each candidate $\Child$ of all possible \textproc{Clone} candidates}
        \State $\mathcal{C}'\gets\textproc{Clone}(\mathcal{C},\Child,\vtree,m)$
        \State $s'\gets\score(\set{D},\mathcal{C},\mathcal{C}')$
        \IIf{$s'>s_{\Child}$}{$s_{\Child}\gets s'$ and $\Child^\ast\gets\Child$}
      \EndFor
      \IIf{$s_{\Sum}>s_{\Child}$}{$\mathcal{C}\gets\textproc{Split}(\mathcal{C},\Sum^\ast,\Prod^\ast,\vtree,m)$}
      \IElse{$\mathcal{C}\gets\textproc{Clone}(\mathcal{C},\Child^\ast,\vtree,m)$}
    \EndWhile
    \State \textbf{return} $\mathcal{C}$
  \end{algorithmic}
\end{algorithm}

\subsubsection{Complexity}

Although learning the vtree top-down reduces to an NP-complete min-cut graph partitioning problem,
there are approximate algorithms that provide high quality partitionings in $\bigo(|\set{X}|^2)$
\citep{karypsis98}. Learning bottom-up is reduced to min-cost perfect matching, which can be done
in $\bigo(|\set{X}|^4)$ via the Edmonds Blossom algorithm \citep{edmonds65,kolmogorov09}.

\textproc{Split} runs, for a given variable $X$, in $\bigo(v\cdot|\mathcal{C}|)$ if unbounded
by $m$, where $v$ is $|\Val(X)|$, the number of possible assignments to $X$ if $X$ is discrete; or
the number of intervals to fragment $\Val(X)$ if $X$ is continuous. \textproc{Clone}'s runtime is
$\bigo(|\mathcal{C}|)$ when $m$ is unbounded, as it needs to produce an almost exact copy of
the circuit. We say that a local transformation, such as \textproc{Split} or \textproc{Clone}, is
\emph{minimal} when the copy depth is $m=0$. When \textproc{Split} and \textproc{Clone} are minimal
and $X$ is binary, then the transformation is done in constant time. In fact, any non-minimal
transformation can be composed out of minimal transformations \citep{liang17}.

Perhaps the most costly routine of \textproc{LearnPSDD} is its score function. Although
log-likelihood is linear time computable on the number of edges of the circuit, $\mathcal{C}$ can
grow substantially as transformations pile up. Each score evaluation requires four passes on the
circuit: log-likelihoods and circuit sizes for both $\mathcal{C}$ and its updated circuit
$\mathcal{C}'$. However, since transformations are local, log-likelihood and circuit sizes only
change for the nodes affected in the transformation and their ancestors, allowing
\textproc{LearnPSDD} to cache values. The overall complexity of \textproc{LearnPSDD} at each
iteration is therefore $\bigo(|\mathcal{C}|^2)$ if we assume $m=0$, with the first $|\mathcal{C}|$
coming from the search of all candidates in $\mathcal{C}$, and the second from the computation of
$\score$. Each iteration further increases $|\mathcal{C}|$, slowing down the algorithm's runtime.

\subsubsection{Pros and Cons}

\paragraph{Pros.} The fact that \textproc{LearnPSDD} preserves smoothness, structural
decomposability, determinism \emph{and} any logical semantic coming from its underlying LC is
remarkable. On top of that, in theory and under minor modifications to \textproc{Split} and
\textproc{Clone}, any PC is eligible as an initial circuit, even ones which do not respect any
vtree. Besides, computing variable splits beforehand through a separate process of learning the
vtree relieves the learning algorithm from having to compute costly statistical tests at each
product node. Where \textproc{LearnPSDD} really shines (and perhaps more fittingly PSDDs in
general) is when the support is explicitly defined through the initial circuit's LC; because the PC
attributes non-zero probability only to events where the LC does not return false, the circuit
wastes no mass on impossible events.

\paragraph{Cons.} In practice, \textproc{LearnPSDD} is very slow even with caching; even worse, it
may take several hours for only a minor (if any) improvement. \citep{liang17} suggests improving
performance by producing ensembles of \textproc{LearnPSDD}s, although this negates determinism in
the final model, denying the access to tractably computing queries like divergences, $\mutualinf{}$
and entropies. Another issue is with the choice of the initial circuit. As previously mentioned,
any circuit will do, however the performance (and efficiency) of \textproc{LearnPSDD} is highly
dependent on it. Within the context of PSDDs and encoding their support, \textproc{LearnPSDD}
requires that a separate algorithm compiles an LC for a specific task without looking at data.
Although there are many ways of doing so, they are often not task agnostic (see \Cref{rem:initpc}).
More importantly, because the process of learning the circuit (from data) is decoupled from the
task of encoding logical constraints imposed by a knowledge base, all variables that do not appear
in the logic formula are compiled into a trivial form (e.g.\ fully factorized circuit). Lastly,
although decoupling the process of learning the vtree from learning the PC helps with scalability,
the ability of identifying the proper vtree for the most expressive PC given data is certainly
desirable, and one which might be hindered by this separated process.

\subsection{\textproc{Strudel}}

\citep{dang20} build upon the work of \textproc{LearnPSDD} and propose \textproc{Strudel}, which
mainly improves \textproc{LearnPSDD} on two fronts: (1) by providing a simple algorithm for
generating an initial circuit and vtree from data, and (2) proposing a heuristic for efficiently
searching for good transformation candidates.

We first address how to construct the initial circuit from data. \citeauthor{dang20} suggests doing
so by compiling both a vtree and linear sized PC (in the number of variables) from a Chow-Liu Tree
(CLT, \cite{chow68}). Let $\mathcal{T}$ a CLT over variables $\set{X}=\{X_1,\ldots,X_m\}$. A vtree
$\vtree$ is extracted from $\mathcal{T}$ by traversing $\mathcal{T}$ top-down. For each node
$X_i\in\mathcal{T}$, if $X_i$ is a leaf node in $\mathcal{T}$, then create a vtree leaf node of
$X_i$; otherwise create an inner vtree node $v$, attach a vtree leaf node of $X_i$ as $v^\gets$ and
assign $v^\to$ as a vtree built over all the vtrees coming from the children of $X_i$. The
construction of $v^\gets$ depends on how balanced one wishes the vtree to be: if we want a more
right-leaning vtree, it suffices to construct a right-linear vtree connecting all vtrees from each
child $X_j\in\Ch(X_i)$. Likewise, a balanced vtree is built by balancing the vtree connecting the
recursive vtree calls from each $X_j$. Note that this does not necessarily mean that $v^\to$ is
completely right-linear or balanced, only that it is somewhat close to it, as the rest of the
structure depends on the recursive calls of each CLT node.

\begin{figure}[t]
  \begin{center}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \node[fill=boxgreen!80,draw,circle,inner sep=2pt,minimum size=13pt] (d) at (0,0) {$D$};
      \node[fill=boxgray!70,draw,circle,inner sep=2pt,minimum size=13pt] (c) at ($(d) + (0,-1.5)$) {$C$};
      \node[fill=boxred!70,draw,circle,inner sep=2pt,minimum size=13pt] (b) at ($(c) + (0.5,-1.5)$) {$B$};
      \node[fill=boxgoldenrod!70,draw,circle,inner sep=2pt,minimum size=13pt] (a) at ($(c) + (-0.5,-1.5)$) {$A$};
      \draw[edge] (d) -- (c); \draw[edge] (c) -- (b); \draw[edge] (c) -- (a);

      \node at ($(d) + (0, 1.0)$) {\scalebox{0.8}{\begin{tabular}{c}
            \hline
            $p(D=0)$\\
            \hline
            0.6\\
            \hline
        \end{tabular}}};
      \node at ($(c) + (-2, 0.0)$) {\scalebox{0.8}{\begin{tabular}{c|c}
            \hline
            $D$ & $p(C=0|D)$\\
            \hline
            0 & 0.2\\
            1 & 0.7\\
            \hline
        \end{tabular}}};
      \node at ($(b) + (1.0,-1.25)$) {\scalebox{0.8}{\begin{tabular}{c|c}
            \hline
            $C$ & $p(B=0|C)$\\
            \hline
            0 & 0.5\\
            1 & 0.1\\
            \hline
        \end{tabular}}};
      \node at ($(a) + (-1.0,-1.25)$) {\scalebox{0.8}{\begin{tabular}{c|c}
            \hline
            $C$ & $p(A=0|C)$\\
            \hline
            0 & 0.3\\
            1 & 0.6\\
            \hline
        \end{tabular}}};

      \newVtreeNode[fill=boxorange!80]{v}{$(d) + (3.5,0)$}{1};
      \newVtreeNode[fill=boxblue!50]{vr}{$(v) + (0.5,-1.25)$}{2};
      \newVtreeNode[fill=boxgreen!80]{vd}{$(v) + (-0.5,-1.25)$}{$D$};
      \newVtreeNode[fill=boxpink!50]{vrl}{$(vr) + (0.5,-1.25)$}{3};
      \newVtreeNode[fill=boxgray!70]{vc}{$(vr) + (-0.5,-1.25)$}{$C$};
      \newVtreeNode[fill=boxgoldenrod!70]{va}{$(vrl) + (-0.5,-1.25)$}{$A$};
      \newVtreeNode[fill=boxred!70]{vb}{$(vrl) + (0.5,-1.25)$}{$B$};
      \draw (v) -- (vr); \draw (v) -- (vd); \draw (vr) -- (vrl); \draw (vr) -- (vc);
      \draw (vrl) -- (va); \draw (vrl) -- (vb);

      \newSumNode{r}{$(d) + (9,2)$};
      \newProdNode[fill=boxorange!80]{p1}{$(r) + (-0.75,-1)$};
      \newProdNode[fill=boxorange!80]{p2}{$(r) + (0.75,-1)$};
      \node[fill=boxgreen!80,minimum size=17pt,label=center:{$D$}] (pd) at ($(p1) + (-1.25,-1)$) {};
      \node[fill=boxgreen!80,minimum size=17pt,label=center:{$\neg D$}] (pnd) at ($(p2) + (1.25,-1)$) {};
      \newSumNode{s1}{$(p1) + (0,-1)$};
      \newSumNode{s2}{$(p2) + (0,-1)$};
      \newProdNode[fill=boxblue!50]{q1}{$(s1) + (0,-1)$};
      \newProdNode[fill=boxblue!50]{q2}{$(s2) + (0,-1)$};
      \node[fill=boxgray!70,minimum size=17pt,label=center:{$C$}] (pc) at ($(q1) + (-1.25,-1)$) {};
      \node[fill=boxgray!70,minimum size=17pt,label=center:{$\neg C$}] (pnc) at ($(q2) + (1.25,-1)$) {};
      \newSumNode{z1}{$(q1) + (0,-1)$};
      \newSumNode{z2}{$(q2) + (0,-1)$};
      \newProdNode[fill=boxpink!50]{t1}{$(z1) + (0,-1)$};
      \newProdNode[fill=boxpink!50]{t2}{$(z2) + (0,-1)$};
      \newSumNode{w2}{$(t1) + (0,-1)$};
      \newSumNode{w3}{$(t2) + (0,-1)$};
      \newSumNode{w1}{$(w2) + (-1.5,0)$};
      \newSumNode{w4}{$(w3) + (1.5,0)$};
      \node[fill=boxgoldenrod!70,minimum size=17pt,label=center:{$A$}] (pa) at ($(w1) + (0,-1)$) {};
      \node[fill=boxgoldenrod!70,minimum size=17pt,label=center:{$\neg A$}] (pna) at ($(w2) + (0,-1)$) {};
      \node[fill=boxred!70,minimum size=17pt,label=center:{$B$}] (pb) at ($(w3) + (0,-1)$) {};
      \node[fill=boxred!70,minimum size=17pt,label=center:{$\neg B$}] (pnb) at ($(w4) + (0,-1)$) {};
      \draw[edge] (r) -- node[midway,left] {.4} (p1);
      \draw[edge,very thick,red] (r) -- node[midway,right] {.6} (p2);
      \draw[edge] (p1) -- (pd);
      \draw[edge] (p1) -- (s1);
      \draw[edge,very thick,red] (p2) -- (pnd);
      \draw[edge,very thick,red] (p2) -- (s2);
      \draw[edge] (s1) -- node[midway,left] {.3} (q1);
      \draw[edge] (s1) -- node[very near start,above,xshift=0.125cm,yshift=-0.1cm] {.7} (q2);
      \draw[edge,very thick,red] (s2) -- node[very near start,above,xshift=-0.25cm,yshift=-0.1cm] {.8} (q1);
      \draw[edge] (s2) -- node[midway,right] {.2} (q2);
      \draw[edge,very thick,red] (q1) -- (pc);
      \draw[edge,very thick,red] (q1) -- (z1);
      \draw[edge] (q2) -- (z2);
      \draw[edge] (q2) -- (pnc);
      \draw[edge,very thick,red] (z1) -- (t1);
      \draw[edge] (z2) -- (t2);
      \draw[edge,very thick,red] (t1) -- (w1);
      \draw[edge,very thick,red] (t1) -- (w3);
      \draw[edge] (t2) -- (w2);
      \draw[edge] (t2) -- (w4);
      \draw[edge,very thick,red] (w1) -- node[midway,left] {.4} (pa);
      \draw[edge] (w1) -- node[very near start,above,xshift=0.125cm,yshift=-0.1cm] {.6} (pna);
      \draw[edge] (w2) -- node[very near start,above,xshift=-0.125cm,yshift=-0.1cm] {.7} (pa);
      \draw[edge] (w2) -- node[midway,right] {.3} (pna);
      \draw[edge] (w3) -- node[midway,left] {.9} (pb);
      \draw[edge,very thick,red] (w3) -- node[very near start,above,xshift=0.125cm,yshift=-0.1cm] {.1} (pnb);
      \draw[edge] (w4) -- node[very near start,above,xshift=-0.125cm,yshift=-0.1cm] {.5} (pb);
      \draw[edge] (w4) -- node[midway,right] {.5} (pnb);
    \end{tikzpicture}
    }
  \end{center}
  \caption{A vtree (middle) and probabilistic circuit (right) compiled from a Chow-Liu Tree (left).
    Each conditional probability $p(Y|X)$ is encoded as a (deterministic) sum node where each of
    the two children sets $Y$ to 0 or 1. Colors in the CLT indicate the variables in the PC, while
    vtree inner node colors match with product nodes that respect them. Edges in red indicate the
    induced subcircuit activated on assignment $\{A=1,B=0,C=1,D=0\}$.}
  \label{fig:strudel}
\end{figure}

\textproc{Strudel} compiles an initial circuit by looking at the vtree bottom-up and caching
subcircuits. Let $v$ a vtree
node and $Y\in\mathcal{T}$ a CLT node with conditional probability $p(Y|X)$, where $X$ is the
parent of $Y$. If $v$ is a leaf node in $\vtree$ and $v$'s variable is also a leaf node in
$\mathcal{T}$, two sum nodes $\Sum_0$ and $\Sum_1$ over literal nodes $\neg Y$ and $Y$ are created,
each with weights $w_{\Sum_0,\neg Y}=p(Y=0|X=0)$, $w_{\Sum_0,Y}=p(Y=1|X=0)$ and $w_{\Sum_1,\neg
Y}=p(Y=0|X=1)$, $w_{\Sum_1,Y}=p(Y=0|X=1)$. The two sum nodes connecting $B$ and $\neg B$ in the PC
shown on the right of \Cref{fig:strudel} show this exact case. The left sum node encodes $p(B|C=1)$
and the right one $p(B|C=0)$. These circuits are then cached by associating them with $v$. When $Y$
is not a leaf node in $\mathcal{T}$ but $v$ is, we simply return literal nodes. If $v$ is an inner
node, we must define a scope partition, splitting $\set{X}=\Sc(v^\gets)$ and
$\set{Y}=\Sc(v^\to)$ into product nodes $\Prod_1,\ldots,\Prod_k$, one for each value cached value
in $v$. Each prime is set to the cached circuits from $v^\gets$ and each sub the cached circuits
from $v^\to$. Finally, if two variables $X\in\set{X}$ and $Y\in\set{Y}$ are such that their parents
are the same variable, say $Z$, then $X$ and $Y$ are independent when $Z$ is given (because of a
divergent connection in $\mathcal{T}$) and thus cannot be merged together into a single sum because
of the context-specific independence set by $Z$ \citep{boutilier96}. This is visualized in the
\inode[fill=boxpink!50]{\newProdNode} nodes; in this situation, $A$ and $B$ are siblings coming
from $C$, and so $A\indep B|C$ (redundant sum nodes are added for standardization).
When the prior situation is not true, then not only $X$ is the only variable in $\set{X}$, but $X$
must also be the parent of $Y$ and so we must model $p(\set{Y}|X)$. This is the case for
\inode[fill=boxblue!50]{\newProdNode}, where $C$ is the parent of $B$ and so we have to be join the
two by sum nodes attributing the conditional probabilities $p(A,B|C=0)$ for the right-most
\inode[fill=boxblue!50]{\newProdNode} and $p(A,B|C=1)$ for the left-most sibling. This procedure is
shown more formally in \Cref{alg:strudelinit}.

\begin{algorithm}[t]
  \caption{\textproc{InitialStrudel}}\label{alg:strudelinit}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$
    \Ensure A smooth, structure decomposable and deterministic initial PC and vtree
    \State $\mathcal{T}\gets\textproc{LearnCLT}(\set{D},\set{X})$
    \State $\vtree\gets\textproc{CompileVtree}(\mathcal{T})$
    \State Let $\mathcal{M}$ a hash table for caching circuits, initially empty
    \For{each vtree node $v\in\vtree$ in reverse topological order}
      \If{$v$ is a leaf node}
        \State Let $X\in\mathcal{T}$ the variable represented by $v$, and $Y$ its parent
        \If{$X$ is a leaf node in $\mathcal{T}$}
          \State $\Sum_j\gets\sum_{i\in\Val(X)}p(X=i|Y=j)\cdot\liv X=i\riv$ for each $j\in\Val(Y)$
          \State $\mathcal{M}(v)\gets\mathcal{M}(v)\cup\{\Sum_j|\forall j\in\Val(Y)\}$
        \Else
          \State $\mathcal{M}(v)\gets\mathcal{M}(v)\cup\{\liv X=i\riv|i\in\Val(X)\}$
        \EndIf
      \Else
        \State Attribute $\set{X}\gets\Sc(v^\gets)$ and $\set{Y}\gets\Sc(v^\to)$
        \State Let $X\in\set{X}$ and $Y\in\set{Y}$
        \State Attribute $\Nodes^\gets\gets\mathcal{M}(v^\gets)$ and $\Nodes^\to\gets\mathcal{M}(v^\to)$
        \State $k\gets|\Val(X)|$
        \State Construct product nodes $\Prods=\{\Nodes_i^\gets\times\Nodes_i^\to|\forall i\in\left[k\right]\}$
        \If{$\Pa(X)=\Pa(Y)$}
          \State Create sum nodes $\Sum_i$ each with only a single child $\Prod_i\in\Prods$, for each $i\in\left[k\right]$
          \State $\mathcal{M}(v)\gets\mathcal{M}(v)\cup\{\Sum_1,\ldots,\Sum_k\}$
        \Else
          \State $\Sum_j\gets\sum_{i\in\Val(X)}p(\set{Y}|X=j)\cdot\liv X=i\riv$, for each $j\in\Val(Y)$
          \State $\mathcal{M}(v)\gets\mathcal{M}(v)\cup\{\Sum_j|\forall j\in\Val(Y)\}$
        \EndIf
      \EndIf
    \EndFor
    \State \textbf{return} $\mathcal{M}(v_r)$, where $v_r$ is $\vtree$'s root node
  \end{algorithmic}
\end{algorithm}

Once we have an initial PC constructed from \textproc{InitialStrudel}, we are ready to discuss
\textproc{Strudel}'s second contribution. To do so, we must first understand the notion of
\emph{circuit flows} introduced in \citet{dang20}. In short, the circuit flow of a deterministic
probabilistic circuit $\mathcal{C}$ with respect to a variable assignment $\set{x}$ is the induced
tree (see \Cref{def:inducedsub}) whose edges are all non-zero when $\mathcal{C}$ is evaluated under
$\set{x}$. Such an induced tree is unique in deterministic PCs because every sum node admits only
one non-zero valued child for $\set{x}$ (or any assignment for that matter). Note how circuit flows
are more specific in the sense they are intrinsically linked to an assignment, while induced
subcircuits specify a deterministic subcircuit within its supercircuit. The circuit flow of
deterministic PCs helps us understand how to efficiently compute inference in circuits of that
nature. As we briefly mentioned before, for any assignment $\set{x}$ in a smooth, decomposable and
deterministic PC $\mathcal{C}$, there exists a unique circuit flow $\mathcal{F}$ that encodes the
log-likelihood computation
\begin{equation*}
  \mathcal{C}(\set{x})=\mathcal{F}_\mathcal{C}(\set{x})=\prod_{(\Sum,\Child)\in
  \FEdges(\mathcal{F}_\mathcal{C})}w_{\Sum,\Child}\prod_{\Leaf\in\FInputs(\mathcal{F}_\mathcal{C})}
  p_{\Leaf}(\set{x}),
\end{equation*}
where $\FInputs(\cdot)$ returns the set of input nodes of a circuit. When inputs are all binary,
then one might encode $\mathcal{F}_\mathcal{C}$ as a mapping $f_\mathcal{C}:\mathcal{X}\to
\{0,1\}^{|\mathcal{W}_\mathcal{C}|}$, here $\mathcal{W}_\mathcal{C}$ denoting the set of all
parameters (i.e.\ sum node weights) of $\mathcal{C}$, which ``activates'' edge
$w\in\mathcal{W}_\mathcal{C}$ under assignment $\set{x}$. With this, the above operation under
log-space is reduced to a vector multiplication
\begin{equation*}
  \log\mathcal{C}(\set{x})=f_\mathcal{C}(\set{x})^\intercal\cdot\log\left(\mathcal{W}_\mathcal{C}\right).
\end{equation*}
Importantly, by aggregating circuit flows through counting the number of activations of each
parameter $w_{\Sum,\Child}$ in the entire training dataset $\set{D}$, we get a sense of the number
of samples $w_{\Sum,\Child}$ impacts over $\set{D}$, and thus a sense of how meaningful is that
edge on the fitness of data. As we shall see briefly, this aggregated circuit flow shall then be
used as a score for a greedy search over the space of candidates for local transformations.
%and then normalizing over the number of all activations of edges coming out of
%$\Sum$, we get the maximum likelihood estimator (MLE) of edge $\edge{\Sum}{\Child}$ in the whole
%training dataset $\set{D}$.

To overcome the scalability limitations of \textproc{LearnPSDD}, \textproc{Strudel} proposes using
only \textproc{Split} to reduce the search space, looking at performing the search greedily instead
of exhaustively and exploiting the efficiency of aggregate circuit flows as a fast heuristic in
place of computing the whole likelihood. Searching is done by finding the edge to \textproc{Split}
whose aggregate circuit flow is maximal
\begin{equation*}
  \score_{\textsf{eFlow}}(w_{\Sum,\Child}|\mathcal{C},\set{D})=\sum_{\set{x}\in\set{D}}f_{\mathcal{C}}(\set{x})\left[w_{\Sum,\Child}\right],
\end{equation*}
while the choice of which variable to condition \textproc{Split} on is done by selecting the
variable $X$ that shares the most dependencies (and thus the higher pairwise mutual information)
with other variables within the scope of that edge, estimated from the aggregate flows
\begin{equation*}
  \score_{\textsf{vMI}}(X,w_{\Sum,\Child}|\mathcal{C},\set{D})=\sum_{\substack{Y\in\Sc(\Sum)\\Y\neq
    X}}\mutualinf(X,Y).
\end{equation*}
The entire algorithm for \textproc{Strudel} is showcased in \Cref{alg:strudel}.

\begin{algorithm}[t]
  \caption{\textproc{Strudel}}\label{alg:strudel}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, max depth $m$, scope $\set{X}$
    \Ensure A smooth, structure decomposable and deterministic PC learned from $\set{D}$
    \State $\mathcal{C},\vtree\gets\textproc{InitialStrudel}(\set{D},\set{X})$
    \While{there is score improvement of has not reached the iteration/time limit}
      \State Compute the aggregate flow over all edges
      \State $w_{\Sum,\Child}^\ast\gets\argmax_{w\in\mathcal{W}_\mathcal{C}}\score_{\textsf{eFlow}}(w|\mathcal{C},\set{D})$
      \State $X^\ast\gets\argmax_{X\in\Sc(\Sum)}\score_{\textsf{vMI}}(X,w_{\Sum,\Child}^\ast|\mathcal{C},\set{D})$
      \State $\mathcal{C}\gets\textproc{Split}(\mathcal{C},\Sum,\Child,\vtree,m)$
    \EndWhile
    \State \textbf{return} $\mathcal{C}$
  \end{algorithmic}
\end{algorithm}

\subsubsection{Complexity}

Learning the Chow-Liu Tree is done in $\bigo(|\set{X}|^2\cdot|\set{D}|)$ through Chow-Liu's
algorithm \citep{chow68}, while the vtree is compiled in time linear to the size of the CLT, i.e.\
$\bigo(|\set{X}|)$ since the Bayesian network is a tree. Consequentially, \textproc{InitialStrudel}
runs in $\bigo(|\set{X}|\cdot|\Val(X)|)$, or linear on $|\set{X}|$ if we assume binary variables as
originally intended. The bulk of the computation falls under \textproc{Strudel}, which runs in
$\bigo\left(|\set{X}|^2\cdot|\set{D}|+i(|\mathcal{C}|\cdot|\set{D}|+|\set{X}|^2)\right)$ assuming a
bounded max depth $m$ and binary variables. Term $|\set{X}|^2\cdot|\set{D}|$ corresponds to learning
the CLT, $|\mathcal{C}|\cdot|\set{D}|$ to the computation of the aggregate circuit flows,
$|\set{X}|^2$ to the computation of $\score_{\textsf{vMI}}$ which involves the pairwise mutual
informations of $\set{X}$, and $i$ the number of iterations of $\textproc{Strudel}$.

\subsubsection{Pros and Cons}

\paragraph{Pros.} Arguably, the most valuable contribution in \textproc{Strudel} is its
scalability. Compared to \textproc{LearnPSDD}, \textproc{Strudel} can take orders of magnitude less
time per iterationCompared to \textproc{LearnPSDD}, \textproc{Strudel} can take orders of magnitude
less time per iteration, meaning that it can produce more \textproc{Split}s 

\paragraph{Cons.}

\begin{remark}[breakable]{On the choice of initial circuits}{initpc}
  \citep{choi13,oztok15,choi17,shen17}
\end{remark}

\section{Random Learning}
\label{sec:random}
