\chapter{Learning Probabilistic Circuits}
\label{ch:learning}

As we have seen in \Cref{ch:pc}, inference in probabilistic circuits is, for the most part,
straightforward.  This is not so much the case when \emph{learning} PCs. Despite the uncomplicated
syntax, learning sufficiently expressive PCs in a principled way is comparatively harder to, say
the usual neural network. For a start, we are usually required to comply with smoothness and
decomposability to ensure marginalization at the least. This restriction excludes the possibility
of adopting any of the most popular neural network patterns or architectures used in deep learning
today. To make matters worse, constructing a PC graph more often than not involves costly
statistical tests that make learning their structure a challenge for high dimensional data.

In this chapter, we review the most popular PC structure learning algorithms, their pros and cons,
and more importantly, what can we learn from them to efficiently build scalable probabilistic
circuits. We broadly divide existing structure learners into three main categories:
divide-and-conquer (\divclass{}, \Cref{sec:divconq}), incremental methods (\incrclass{},
\Cref{sec:incremental}) and random approaches (\randclass{}, \Cref{sec:random}).

\section{Divide-and-Conquer Learning}
\label{sec:divconq}

Arguably the most popular approach to learning the structure of probabilistic circuits are
algorithms that follow a \emph{divide-and-conquer} scheme. This class of PC learning algorithms,
which here we denote by \divclass{}, are characterized by recursive calls over (usually mutually
exclusive) subsets of data in true divide-and-conquer fashion. This kind of procedure is more
clearly visualized by \textproc{LearnSPN}, the first, most well-known, and perhaps most archetypal
of its class.

Before we start however, we must first address how we denote data. Data is commonly represented as
a matrix where rows are assignments (of all variables), and columns are the values that each variable
takes at each assignment. Let $\set{D}\in\mathbb{R}^{m \times n}$ a matrix with $m$ rows and $n$
columns. We use $\set{D}_{i,j}$ to access an element of $\set{D}$ at the $i$-th row, $j$-th column
of matrix $\set{D}$. We denote by $\set{D}_{\set{i},\set{j}}$, where $\set{i}\subseteq
\left[1..n\right]$ and $\set{j}\subseteq\left[1..m\right]$ are sets of indices, a submatrix from
the extraction of the $\set{i}$ rows and $\set{j}$ columns of $\set{D}$. We use a colon as a
shorthand for selecting all rows or columns, e.g.\ $\set{D}_{:,:}=\set{D}$, $\set{D}_{:,j}$ is the
$j$-th column and $\set{D}_{i,:}$ is the $i$-th row.

\subsection{\textproc{LearnSPN}}

\begin{figure}[t]
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{subfigure}{0.45\textwidth}
      \resizebox{\textwidth}{!}{
      \begin{tabular}{ccccc}
        \hline
        $A$ & $B$ & $C$ & $D$ & $E$\\
        \hline
        \rowcolor{boxgreen!70}
        0 & 1 & 0 & 0 & 1\\
        \rowcolor{boxgreen!70}
        1 & 0 & 1 & 1 & 1\\
        \rowcolor{boxblue!50}
        1 & 1 & 0 & 1 & 1\\
        \rowcolor{boxblue!50}
        0 & 0 & 1 & 0 & 0\\
        \rowcolor{boxgreen!70}
        1 & 1 & 0 & 1 & 0\\
        \rowcolor{boxblue!50}
        0 & 1 & 1 & 0 & 1\\
        \rowcolor{boxorange!60}
        1 & 0 & 1 & 1 & 1\\
        \rowcolor{boxorange!60}
        1 & 1 & 0 & 0 & 0\\
        \rowcolor{boxblue!50}
        0 & 1 & 1 & 0 & 1\\
        \hline
      \end{tabular}
      }
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \resizebox{\textwidth}{!}{
      \begin{tikzpicture}
        \newSumNode[fill=boxpink!50]{r}{0,0};
        \newProdNode[fill=boxgreen!70]{p1}{$(r) + (-1.5,-1.5)$};
        \newProdNode[fill=boxorange!60]{p2}{$(r) + (0,-1.5)$};
        \newProdNode[fill=boxblue!50]{p3}{$(r) + (1.5,-1.5)$};
        \draw[edge] (r) -- node[midway,above left] {$\frac{3}{9}$} (p1);
        \draw[edge] (r) -- node[midway,left] {$\frac{2}{9}$} (p2);
        \draw[edge] (r) -- node[midway,above right] {$\frac{4}{9}$} (p3);
      \end{tikzpicture}
      }
    \end{subfigure}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{subfigure}{0.45\textwidth}
      \newcolumntype{x}{>{\columncolor{boxgreen!70}}c}
      \newcolumntype{y}{>{\columncolor{boxorange!60}}c}
      \newcolumntype{z}{>{\columncolor{boxblue!50}}c}
      \resizebox{\textwidth}{!}{
      \begin{tabular}{xyyzx}
        \hline
        \multicolumn{1}{c}{$A$} & \multicolumn{1}{c}{$B$} & \multicolumn{1}{c}{$C$} &
        \multicolumn{1}{c}{$D$} & \multicolumn{1}{c}{$E$}\\
        \hline
        0 & 1 & 0 & 0 & 1\\
        1 & 0 & 1 & 1 & 1\\
        1 & 1 & 0 & 1 & 1\\
        0 & 0 & 1 & 0 & 0\\
        1 & 1 & 0 & 1 & 0\\
        0 & 1 & 1 & 0 & 1\\
        1 & 0 & 1 & 1 & 1\\
        1 & 1 & 0 & 0 & 0\\
        0 & 1 & 1 & 0 & 1\\
        \hline
      \end{tabular}
      }
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \resizebox{\textwidth}{!}{
      \begin{tikzpicture}
        \newProdNode[fill=boxpink!50]{r}{0,0};
        \newSumNode[label=below:{$\{A,E\}$},fill=boxgreen!70]{s1}{$(r) + (-1.5,-1.5)$};
        \newSumNode[label=below:{$\{B,C\}$},fill=boxorange!60]{s2}{$(r) + (0,-1.5)$};
        \newSumNode[label=below:{$\{D\}$},fill=boxblue!50]{s3}{$(r) + (1.5,-1.5)$};
        \draw[edge] (r) -- (s1); \draw[edge] (r) -- (s2); \draw[edge] (r) -- (s3);
      \end{tikzpicture}
      }
    \end{subfigure}
    \caption{}
  \end{subfigure}
  \caption{\textproc{LearnSPN} assigns either rows (a) or columns (b) for sum and product nodes
    respectively. For sums, their edge weights are set proportionally to the assignments. For
    product children, scopes are defined by which columns are assigned to them.}
  \label{fig:learnspn}
\end{figure}

Recall the semantics of sum and product nodes in a smooth and decomposable probabilistic circuit.
A sum is a mixture of distributions $p(\set{X})=\sum_{i=1}^m w_i\cdot p_i(\set{X})$ whose children
scopes are all the same. A product is a factorization $p(\set{X}_1,\ldots,\set{X}_n)=\prod_{i=1}^n
p(\set{X}_i)$, implying that $\set{X}_i\indep\set{X}_j$ for $i,j\in [n]$ and $i\neq j$.
\textproc{LearnSPN} \citep{gens13} exploits these semantics in an intuitive and uncomplicated
manner: sum children are defined by sub-PCs learned from similar (by some arbitrary metric)
assignments, and product children are sub-PCs learned from data conditioned on the variables
defined by their scope. In practice, this means that, for a dataset $\set{D}\in\mathbb{R}^{m\times
n}$, sums assign rows to their children, while product children are assigned columns. This
procedures continues recursively until data are reduced to a $k\times 1$ matrix, in which case a
univariate distribution acting as input node is learned from it. This recursive procedure is shown
more formally in \Cref{alg:learnspn}.

\begin{algorithm}[t]
  \caption{\textproc{LearnSPN}}\label{alg:learnspn}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \IIf{$|\set{X}|=1$}{\textbf{return} an input node learned from $\set{D}$}
    \NIElse
      \State Find scope partitions $\set{X}_1,\ldots,\set{X}_t\subseteq\set{X}$ st
        $\set{X}_i\indep\set{X}_j$ for $i\neq j$
      \IIf{$k>1$}{\textbf{return} $\prod_{j=1}^t \textproc{LearnSPN}(\set{D}_{:,\set{X}_j},
        \set{X}_j)$}
      \NIElse
        \State Find subsets of data $\set{x}_1,\ldots,\set{x}_k\subseteq\set{D}$ st all assignments
          within $\set{x}_i$ are all similar
        \State \textbf{return} $\sum_{i=1}^k \frac{|\set{x}_i|}{|\set{D}|}\cdot
          \textproc{LearnSPN}(\set{x}_i,\set{X})$
      \EndNIElse
    \EndNIElse
  \end{algorithmic}
\end{algorithm}

Notably, \citep{gens13} purposely does not strictly specify which techniques should be used for
assigning rows and columns, although they do provide empirical results on a particular form of
\textproc{LearnSPN} where row assignments are computed through EM clustering and products by
pairwise G-testing. Instead, they call the algorithm a \emph{schema} that incorporates several
actual learning algorithms whose concrete form depends on the choice of how to split data.

\subsubsection{Complexity}

To be able to analyze the complexity of \textproc{LearnSPN}, we assume a common implementation
where sums are learned from $k$-means clustering, and products through pairwise G-testing. We know
learning sums is efficient: $k$-means takes $\bigo(n\cdot k\cdot m\cdot c)$ time, where $k$ is the
number of clusters and $c$ the number of iterations to be run. Products, on the other hand, are
much more costly. The na√Øve approach would be to compute whether $X_i\indep X_j$ for every possible
combination. This is clearly quadratic on the number of variables
$\bigo\left(\binom{m}{2}=\frac{m!}{2(m-2)!}\right)$ assuming an $\bigo(1)$ oracle for independence
testing. In reality, G-test takes $\bigo(n\cdot m)$ time, as we must compute a ratio of observed
versus expected values for each cell in the contingency table. This brings the total runtime for
products to a whopping $\bigo\left(n\cdot m^3\right)$, prohibitive to any reasonably large dataset.
In terms of space, independence tests most commonly used require either a correlation (for
continuous data) or contingency (for discrete data) matrix that takes up $\bigo(m^2)$ space,
another barrier for scaling up to high dimensional data.

Alternatively, instead of computing the G-test for every possible combination of variables,
\citep{gens13} constructs an independence graph $\mathcal{G}$ whose nodes are variables and edges
indicate whether two variables are statistically dependent. Within this context, the variable
partitions we attribute to product children are exactly the connected components of $\mathcal{G}$,
meaning it suffices testing only some combinations. Even so, this heuristic is still quadratic
worst case. \Cref{fig:indepgraph} shows $\mathcal{G}$, the spanning forest resulted from the
connected component heuristic, and the equivalent product node from this decomposition.

\begin{figure}[t]
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=8,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxgreen] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxred!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
      \node[circle,fill=boxgray] (p6) at (p.corner 6) {$F$};
      \node[circle,fill=boxgoldenrod!70] (p7) at (p.corner 7) {$G$};
      \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};
      \draw[thick] (p1) -- (p2) -- (p3); \draw[thick] (p1) -- (p3);
      \draw[thick] (p3) -- (p4); \draw[thick] (p1) -- (p4);
      \draw[thick] (p5) -- (p6) -- (p7); \draw[thick] (p5) -- (p7);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=8,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxgreen] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxred!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
      \node[circle,fill=boxgray] (p6) at (p.corner 6) {$F$};
      \node[circle,fill=boxgoldenrod!70] (p7) at (p.corner 7) {$G$};
      \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};
      \draw[thick] (p1) -- (p2); \draw[thick] (p1) -- (p3);
      \draw[thick] (p1) -- (p4); \draw[thick] (p1) -- (p4);
      \draw[thick] (p5) -- (p6); \draw[thick] (p5) -- (p7);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \begin{tikzpicture}
      \newProdNode[fill=boxgreen]{r}{0,0};
      \newSumNode[label=below:{$\{A,B,C,D\}$},fill=boxorange!80]{s1}{$(r) + (-1.25,-2.5)$};
      \newSumNode[label=below:{$\{E,F,G\}$},fill=boxpink!50]{s2}{$(r) + (0,-1.75)$};
      \newSumNode[label=below:{$\{H\}$},fill=boxblue!50]{s3}{$(r) + (1.25,-1.0)$};
      \draw[edge] (r) -- (s1); \draw[edge] (r) -- (s2); \draw[edge] (r) -- (s3);
    \end{tikzpicture}
    \caption{}
  \end{subfigure}
  \caption{The pairwise (in)dependence graph where each node is a variable. In (a) we show the full
    graph, computing independence tests for each pair of variables in $\bigo(m^2)$. However, it
    suffices to compute for only the connected components (b), saving up pairwise computation time
    for reachable nodes.  The resulting product node and scope partitioning is shown in (c).}
  \label{fig:indepgraph}
\end{figure}

\subsubsection{Pros and cons}

\paragraph{Pros.} Perhaps the main factor for \textproc{LearnSPN}'s popularity is how easily
implementable, intuitive and modular it is. Even more remarkably, it is an empirically competitive
PC learning algorithm despite its age, serving as a baseline for most subsequent works in PC
literature. Lastly, the fact that each recursive call from \textproc{LearnSPN} is completely
independent from each the other makes it an attractive candidate for CPU parallelization.

\paragraph{Cons.} Debatably, one of the key weakness of \textproc{LearnSPN} is its tree-shaped
computational graph, meaning that they are strictly less succint compared to non-tree DAG PCs
\citep{martens14}. In terms of runtime efficiency, the algorithm struggles on high dimensional
data due to the complexity involved in computing costly statistical tests. Despite
\Cref{alg:learnspn} giving the impression that no hyperparameter tuning is needed for
\textproc{LearnSPN}, in practice the modules for learning sums and products often take many
parameters, most of which (if not all) are exactly the same for every recursive call. This can have
a negative impact on the algorithm's performance, since the same parameters are repeatedly used
even under completely different data.

\subsection{\textproc{ID-SPN}}

A subtle yet effective way of improving the performance of \textproc{LearnSPN} is to consider
tractable probabilistic models over many variables as input nodes instead of univariate
distributions. \textproc{ID-SPN} \citep{rooshenas14} does so by assuming that input nodes are
Markov networks. Further, instead of blindly applying the recursion over subsequent sub-data, it
attempts to compute some metric of quality from each node. The worst scored node is then replaced
with a \textproc{LearnSPN}-like tree. This is repeated until no significant increase in likelihood
is observed. \Cref{alg:idspn} shows the \textproc{ID-SPN} pipeline, where \textproc{ExtendID} is
used in line \ref{alg:idspn:line:extend} to grow the circuit in a divide-and-conquer fashion. The
name \textproc{ID-SPN} comes from \emph{direct} variable interactions, meaning the relationships
modeled through the Markov networks as input nodes; and \emph{indirect} interactions brought from
the latent variable interpretation of sum nodes.

\begin{algorithm}[t]
  \caption{\textproc{ExtendID}}\label{alg:extendid}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$, and memoization
      function $\mathcal{M}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \State Find scope partitions $\set{X}_1,\ldots,\set{X}_t\subseteq\set{X}$ st
    \If{$k>1$}
      \For{each $j\in\left[t\right]$}
        \State $\Node_j\gets\textproc{LearnMarkov}(\set{D}_{:,\set{X}_j},\set{X}_j)$
        \State Associate $\mathcal{M}(\Node_j)$ with $\set{D}_{:,\set{X}_j}$ and $\set{X}_j$
      \EndFor
      \State \textbf{return} $\prod_{j=1}^t \Node_j$
    \Else
      \State Find subsets of data $\set{x}_1,\ldots,\set{x}_k\subseteq\set{D}$ st all assignments
        within $\set{x}_i$ are all similar
      \For{each $i\in\left[k\right]$}
        \State $\Node_i\gets\textproc{LearnMarkov}(\set{x}_i,\set{X})$
        \State Associate $\mathcal{M}(\Node_i)$ with $\set{x}_i$ and $\set{X}$
      \EndFor
      \State \textbf{return} $\sum_{i=1}^k \frac{|\set{x}_i|}{|\set{D}|}\cdot\Node_i$
    \EndIf
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
  \caption{\textproc{ID-SPN}}\label{alg:idspn}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \State Create a single-node PC: $\mathcal{C}\gets\textproc{LearnMarkov}(\set{D},\set{X})$
    \State Let $\mathcal{M}$ a memoization function associating a node with a dataset and scope
    \State Call $\mathcal{C}'$ a copy of $\mathcal{C}$
    \While{improving $\mathcal{C}$ yields better likelihood}
      \State Pick worse node $\Node$ from $\mathcal{C}'$
      \State Extract sub-data $\set{D}'$ and sub-scope $\set{X}'$ from $\mathcal{M}(\Node)$
      \State Replace $\Node$ with $\textproc{ExtendID}(\set{D}',\set{X}',\mathcal{M})$\label{alg:idspn:line:extend}
      \IIf{$\mathcal{C}'$ has better likelihood than $\mathcal{C}$}{$\mathcal{C}\gets\mathcal{C}$}
    \EndWhile
    \State \textbf{return} $\mathcal{C}$
  \end{algorithmic}
\end{algorithm}

With respect to its implementation, \textproc{ID-SPN} is as modular as \textproc{LearnSPN} in the
sense that the data partitioning is left as a subroutine. Indeed, even the choice of input
distributions is customizable: although \citeauthor{rooshenas14} recommend Markov networks, any
tractable distribution will do. Despite this seemingly small change compared to the original
\textproc{LearnSPN} algorithm, \textproc{ID-SPN} seems to perform better compared to its
counterpart most of the time \citep{rooshenas14,jaini18a}, although at a cost to learning speed.
Further, because of the enormous parameter space brought by having to learn Markov networks as
inputs \emph{and} perform the optimizations from sums and products, grid search hyperparameter
tuning is infeasible. \citep{rooshenas14} recommend random search \citep{bergstra12a} as an
alternative.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    % Markov 1
    \node[fill=boxteal] (a) at (0,0) {$A$};
    \node[fill=boxorange!80] (b) at ($(a) + (1,-1)$) {$B$};
    \node[fill=boxpink!50] (c) at ($(a) + (1,0)$) {$C$};
    \node[fill=boxgoldenrod!70] (d) at ($(a) + (0,-1)$) {$D$};
    \node[fill=boxred!70] (e) at ($(d) + (0,-1)$) {$E$};
    \node[fill=boxpurple!60] (f) at ($(b) + (0,-1)$) {$F$};
    \draw (a) -- (b); \draw (b) -- (c);
    \draw (a) -- (c); \draw (a) -- (d);
    \draw (e) -- (f); \draw (f) -- (d);
    \draw[red,very thick,dashed] (-0.5,0.5) rectangle ($(f) + (0.5,-0.5)$);
    \node at ($(e) + (0.5,-1)$) {Initial Markov network};

    \draw[edge,line width=0.2cm,red] (2,-1) -- (3.5,-1);

    \newProdNode[fill=boxgreen]{r}{6.25,0};

    % Markov 2
    \node[fill=boxpink!50] (a) at ($(r) + (0.75,-1)$) {$A$};
    \node[fill=boxteal] (b) at ($(a) + (1,0)$) {$B$};
    \node[fill=boxorange!80] (c) at ($(b) + (0,-1)$) {$C$};
    \draw (a) -- (b); \draw (b) -- (c);
    \draw[edge] (r) -- ($(a) + (0.5,0.5)$);
    \draw[red,very thick,dashed] ($(a) + (-0.5,0.5)$) rectangle ($(c) + (0.5,-0.5)$);

    % Markov 3
    \node[fill=boxgoldenrod!70] (f) at ($(r) + (-0.75,-1)$) {$F$};
    \node[fill=boxred!70] (d) at ($(f) + (-1,0)$) {$D$};
    \node[fill=boxpurple!60] (e) at ($(d) + (1,-1)$) {$E$};
    \draw (d) -- (f); \draw (e) -- (d);
    \draw[edge] (r) -- ($(d) + (0.5,0.5)$);
    \draw[boxdgray,thick,dashed] ($(d) + (-0.5,0.5)$) rectangle ($(e) + (0.5,-0.5)$);

    \node (i1) at ($(r) + (0,-3)$) {Iteration 1};

    \draw[edge,line width=0.2cm,red] ($(b) + (1,0)$) -- ($(b) + (2.5,0)$);

    \newProdNode[fill=boxgreen]{r}{14.5,0.5};
    \newSumNode[fill=boxbrown!60]{s}{$(r) + (2.0,-0.5)$};
    \draw[edge] (r) -- (s);

    \begin{scope}[local bounding box=m1]
      \node[fill=boxorange!80] (b) at ($(s) + (-1.5,-1)$) {$B$};
      \node[fill=boxteal] (a) at ($(b) + (-1,0)$) {$A$};
      \node[fill=boxpink!50] (c) at ($(b) + (0,-1)$) {$C$};
      \draw (a) -- (b); \draw (a) -- (c);
      \draw[boxdgray,thick,dashed] ($(a) + (-0.5,0.5)$) rectangle ($(c) + (0.5,-0.5)$);
    \end{scope}
    \draw[edge] (s) -- (m1.north);

    \begin{scope}[local bounding box=m2]
      \node[fill=boxteal] (a) at ($(s) + (0.0,-1.5)$) {$A$};
      \node[fill=boxorange!80] (b) at ($(a) + (1,0)$) {$B$};
      \node[fill=boxpink!50] (c) at ($(b) + (0,-1)$) {$C$};
      \draw (a) -- (c); \draw (b) -- (c); \draw (a) -- (b);
      \draw[boxdgray,thick,dashed] ($(a) + (-0.5,0.5)$) rectangle ($(c) + (0.5,-0.5)$);
    \end{scope}
    \draw[edge] (s) -- (m2.north);

    % Markov 3
    \node[fill=boxgoldenrod!70] (f) at ($(r) + (-2,-1)$) {$F$};
    \node[fill=boxred!70] (d) at ($(f) + (-1,0)$) {$D$};
    \node[fill=boxpurple!60] (e) at ($(d) + (1,-1)$) {$E$};
    \draw (d) -- (f); \draw (e) -- (d);
    \draw[edge] (r) -- ($(d) + (0.5,0.5)$);
    \draw[boxdgray,thick,dashed] ($(d) + (-0.5,0.5)$) rectangle ($(e) + (0.5,-0.5)$);

    \node at ($(r) + (0,-3.5)$) {Iteration 2};
  \end{tikzpicture}
  }
  \caption{Two iterations of \textproc{ID-SPN}, where the contents inside the dashed line are
  Markov networks. The red color indicates that a node has been chosen as the best candidate for an
  extension with \textproc{ExtendID}. Although here we only extend input nodes, inner nodes can in
  fact be extended as well.}
\end{figure}

\subsubsection{Complexity}

As \textproc{ID-SPN} is a special case of \textproc{LearnSPN}, the analysis for the sums and
products subroutines holds. The only difference is on the runtime complexity for learning input
nodes and the convergence rate for \textproc{ID-SPN}. Assuming input nodes are learned from the
method suggested by \citet{rooshenas14}, which involves learning a probabilistic circuit from a
Markov network \citep{lowd13a}, then each ``input'' node takes time $\bigo(i\cdot c(k\cdot n+m))$,
where $i$ is the number of iterations to run, $c$ is the size of the generated PC, and constant $k$
is a bound on the number of candidate improvements to the circuit, which can grow exponentially for
multi-valued variables. Importantly, opposite from \textproc{LearnSPN} where we only learn input
nodes once per call \emph{if} data is univariate, \textproc{ID-SPN} requires learning multiple
multivariate inputs for \emph{every} \textproc{ExtendID} call.

\subsubsection{Pros and Cons}

\paragraph{Pros.} If we assume any multivariate distribution in place of Markov networks, PCs
learned from \textproc{ID-SPN} are strictly more expressive than ones learned from
\textproc{LearnSPN}, as input nodes could potentially be replaced with \textproc{LearnSPN}
distributions. Additionally, the modularity inherited from \textproc{LearnSPN} allows
\textproc{ID-SPN} to adapt to data according to expert knowledge, bringing some flexibility to the
algorithm.

\paragraph{Cons.} Unfortunately, most of the disadvantages from \textproc{LearnSPN} also apply to
\textproc{ID-SPN}. Just like \textproc{LearnSPN}, independence tests are more often than not a
bottleneck for most executions with resonably large number of variables. However, \textproc{ID-SPN}
relies on a likelihood improvement for the computational graph to be extended, which ends up
curbing the easy parallelization aspect of \textproc{LearnSPN}. Besides, the complexity involved in
learning Markov networks (or any other complex multivariate distribution as input node) carries a
heavy weight during learning. This, coupled with the fact that hyperparameter tuning in the huge
parameter space of \textproc{ID-SPN} must be done by a random search method, can take a heavy price
in terms of learning time.

\subsection{\textproc{Prometheus}}

So far, we have only considered structure learning algorithms that produce tree-shaped circuits.
Even though \textproc{ID-SPN} \emph{might} produce non-tree graphs at the input nodes depending on
the choice of families of multivariate distributions, it does not do so as a rule. We now turn our
attention to a PC learner that \emph{does} generate non-tree computational graphs in a
divide-and-conquer manner.

Recall that in both \textproc{LearnSPN} and \textproc{ID-SPN} the scope partitioning is done
greedily; we define a graph encoding the pairwise (in)dependencies of variables and greedily search
for connected components by comparing independence test results with some correlation threshold,
adding an edge if the correlation is sufficiently high. The choice of this threshold is often
arbitrary and subject to hyperparameter tuning during learning, which is especially worrying when
dealing with high dimensional data. In this section we review \textproc{Prometheus}
\citep{jaini18a}, a divide-and-conquer \textproc{LearnSPN}-like PC learning algorithm with two main
features that stand out compared to the last two methods we have seen so far: (1) it requires no
hyperparameter tuning for variable partitionings, and (2) accepts a more scalable alternative to
computing all pairwise correlations.

\begin{figure}[t]
  \begin{subfigure}[t]{0.25\textwidth}
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=5,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxred!70] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxpink!50] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxgoldenrod!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxblue!50] (p5) at (p.corner 5) {$E$};
      \draw[thick] (p1) -- node[midway,above left] {$0.8$} (p2);
      \draw[thick] (p1) -- node[midway,left,yshift=0.1cm] {$0.2$} (p3);
      \draw[thick] (p1) -- node[midway,right,yshift=0.1cm] {$0.4$} (p4);
      \draw[thick] (p1) -- node[midway,above right] {$0.3$} (p5);
      \draw[thick] (p2) -- node[midway,below left] {$0.6$} (p3);
      \draw[thick] (p2) -- node[midway,below left,xshift=0.2cm] {$0.4$} (p4);
      \draw[thick] (p2) -- node[midway,above] {$0.1$} (p5);
      \draw[thick] (p3) -- node[midway,below] {$0.9$} (p4);
      \draw[thick] (p3) -- node[midway,below right,xshift=-0.2cm] {$0.5$} (p5);
      \draw[thick] (p4) -- node[midway,right] {$0.7$} (p5);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.25\textwidth}
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=5,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxred!70] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxpink!50] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxgoldenrod!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxblue!50] (p5) at (p.corner 5) {$E$};
      \draw[very thick,blue] (p1) -- node[midway,above left,xshift=-0.1cm,yshift=0.1cm] {$\mathbf{0.8}$} node[fill=white,inner sep=1pt] {$e_2$} (p2);
      \draw[very thick,red] (p2) -- node[midway,left,xshift=-0.1cm,yshift=-0.1cm] {$\mathbf{0.6}$} node[fill=white,inner sep=1pt] {$e_4$} (p3);
      \draw[very thick,boxdgray] (p3) -- node[midway,below,yshift=-0.1cm] {$\mathbf{0.9}$} node[fill=white,inner sep=1pt] {$e_1$} (p4);
      \draw[very thick,orange!80] (p4) -- node[midway,right,xshift=0.1cm,yshift=-0.1cm] {$\mathbf{0.7}$} node[fill=white,inner sep=1pt] {$e_3$} (p5);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newSumNode[fill=boxgreen!75]{r}{0,0};
      \newProdNode[fill=boxdgray!80]{p1}{$(r) + (-3,-1)$};
      \newProdNode[fill=blue!50]{p2}{$(r) + (-1,-1)$};
      \newProdNode[fill=orange!80]{p3}{$(r) + (1,-1)$};
      \newProdNode[fill=red!60]{p4}{$(r) + (3,-1)$};
      \newSumNode[label=below:{$\{A,B,C\}$},fill=boxgreen!75]{s1}{$(r) + (-3.5,-3.0)$};
      \newSumNode[label=below:{$\{D,E\}$},fill=boxgreen!75]{s2}{$(r) + (-2.5,-2.5)$};
      \newSumNode[label=below:{$\{B,C\}$},fill=boxgreen!75]{s3}{$(r) + (-1.5,-2.0)$};
      \newSumNode[label=below:{$\{A\}$},fill=boxgreen!75]{s4}{$(r) + (-0.5,-2.0)$};
      \newSumNode[label=below:{$\{D\}$},fill=boxgreen!75]{s5}{$(r) + (0.5,-2.0)$};
      \newSumNode[label=below:{$\{E\}$},fill=boxgreen!75]{s6}{$(r) + (1.5,-2.0)$};
      \newSumNode[label=below:{$\{B\}$},fill=boxgreen!75]{s7}{$(r) + (2.5,-2.0)$};
      \newSumNode[label=below:{$\{C\}$},fill=boxgreen!75]{s8}{$(r) + (3.5,-2.0)$};
      \draw[edge] (r) -- (p1.north);
      \draw[edge] (r) -- (p2.north);
      \draw[edge] (r) -- (p3.north);
      \draw[edge] (r) -- (p4.north);
      \draw[edge,blue!80] (p1) -- (s1.north);
      \draw[edge,blue!80] (p1) -- (s2.north);
      \draw[edge,red!80] (p2) -- (s2.north);
      \draw[edge,red!80] (p2) -- (s3.north);
      \draw[edge,red!80] (p2) -- (s4.north);
      \draw[edge,boxdgray!80] (p3) -- (s3.north);
      \draw[edge,boxdgray!80] (p3) -- (s4.north);
      \draw[edge,boxdgray!80] (p3) -- (s5.north);
      \draw[edge,boxdgray!80] (p3) -- (s6.north);
      \draw[edge,orange!80] (p4) -- (s4.north);
      \draw[edge,orange!80] (p4) -- (s5.north);
      \draw[edge,orange!80] (p4) -- (s6.north);
      \draw[edge,orange!80] (p4) -- (s7.north);
      \draw[edge,orange!80] (p4) -- (s8.north);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \caption{The fully connected correlation graph (a) with weights as the pairwise correlation
    measurements for each pair of variables; the maximum spanning tree for determining
    decompositions (b); and the mixture of decompositions (c). Colors in (b) match their
    partitionings in (c).}
  \label{fig:prometheus}
\end{figure}

Let $\mathcal{G}$ the independence graph for scope $\set{X}=\{X_1,X_2,\ldots,X_m\}$. Remember that
$\mathcal{G}$'s vertices are $\set{X}$ and each (undirected) edge $\overline{X_i X_j}$ coming from
$X_i$ to $X_j$ means that $X_i\notindep X_j$. Previously, we constructed $\mathcal{G}$ by comparing
the output of an independence test (such as the G-test) against a threshold (e.g.\ a sufficiently
low $p$-value). Instead, suppose $\mathcal{G}$ is fully connected and that we attribute weights
corresponding to a correlation metric of $X_i$ against $X_j$ for each edge (e.g.\ Pearson's
correlation coefficient). The \emph{maximum spanning tree} (MST) of $\mathcal{G}$, here denoted by
$\mathcal{T}$, defines a graph where the removal of any edge in $\mathcal{T}$ partitions the
component into two subcomponents. Let $e_i$ the $i$-th lowest (weight) valued edge;
\textproc{Prometheus} obtains a set of decompositions by iteratively removing edges from $e_1$ to
$e_{|\set{X}|-1}$. In other words, the algorithm constructs a product node for each decomposition,
assigning the scope of each child as the scope of each component at each edge removal. These
products are then joined together by a parent sum node that acts as a mixture of decompositions.
\Cref{fig:prometheus} shows an example of $\mathcal{T}$, the subsequent decompositions, and the
resulting mixture of decompositions.

\begin{algorithm}[t]
  \caption{\textproc{Prometheus}}\label{alg:prometheus}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \IIf{$|\set{X}|$ is sufficiently small}{\textbf{return} an input node learned from $\set{D}$}
    \NIElse
      \State Find subsets of data $\set{x}_1,\ldots,\set{x}_k\subseteq\set{D}$ st all assignments
        within $\set{x}_i$ are all similar
      \State Create a sum node $\Sum$ with initially no children and uniform weights
      \For{each $\set{x}_i$}
        \State $\mathcal{T}\gets\textproc{CorrelationMST}(\set{x}_i,\set{X})$
        \For{each weighted edge $e_j$ in $\mathcal{T}$ in decreasing order}
          \State Remove edge $e_i$ from $\mathcal{T}$
          \State Call $\set{S}_1,\ldots,\set{S}_t$ the scopes of each component in $\mathcal{T}$
          \State Create product node $\Prod_j$ and associate it with $\set{S}_1,\ldots,\set{S}_t$
          \State Associate $\Prod_j$ with dataset $\set{x}_i$
          \State Add $\Prod_j$ as a child of $\Sum$
        \EndFor
      \EndFor
      \State Let $\mathcal{H}$ a hash table (initially empty) associating scopes to sum nodes
      \For{each $\Prod\in\Ch(\Sum)$}
        \For{each scope $\set{S}$ associated with $\Prod$}
          \If{$\set{S}\not\in\mathcal{H}$}
            \State Let $\set{x}$ the dataset associated with $\Prod$
            \State $\Node\gets\textproc{Prometheus}(\set{x}_{:,\set{S}},\set{S})$
            \State Add $\Node$ as a child of $\Prod$
            \State $\mathcal{H}_\set{S}\gets\Node$
          \Else
            \State Add $\mathcal{H}_\set{S}$ as a child of $\Prod$
          \EndIf
        \EndFor
      \EndFor
      \State \textbf{return} $\Sum$
    \EndNIElse
  \end{algorithmic}
\end{algorithm}

Sum nodes are learned by clustering data into similar instances, just like in previous cases. Since
the previously mentioned procedure involving products creates a mixture of decompositions (and thus
a sum node), we can simply collapse the consecutive sum layers into a single sum node.
\Cref{alg:prometheus} shows the algorithm in its entirety. \textproc{CorrelationMST} computes the
(fully connected) correlation graph, returning its MST. It is worth mentioning that
\textproc{Prometheus} makes sure each recursive call shares subcircuits whenever scopes are the
same (this is when the hash table $\mathcal{H}$ in \Cref{alg:prometheus} comes into play). This
avoids an exponential growth from the $k\cdot(|\set{X}|-1)$ potential recursive calls.

\subsubsection{Complexity}

Up to now, the computation of decompositions is done by a $\bigo(m^2)$ construction of a fully
connected correlation graph. This gives \textproc{Prometheus} no asymptotic advantage over neither
\textproc{LearnSPN} nor \textproc{ID-SPN}. To change this, \citeauthor{jaini18a} propose a more
scalable alternative: in place of constructing the entire correlation graph, sample $m\log m$
variables and construct a correlation graph where only $\log m$ edges are added for each of these
sampled variables instead, bringing down complexity to $\bigo(m\left(\log m\right)^2)$.

The analysis of sum nodes is exactly the same as \textproc{LearnSPN} if we assume the same
clustering method. If \textproc{Prometheus} is implemented with the same multivariate distributions
as \textproc{ID-SPN} at the input nodes, the analysis for those also holds.

\subsubsection{Pros and Cons}

\paragraph{Pros.} The notable achievements of \textproc{Prometheus} are evidently the absence of
parameters for computing scope partitionings, reducing the dimension of hyperparameters to tune; a
scalable alternative to partitionings that runs in sub-quadratic time; and (more debatably) the
fact that the algorithm produces non-tree shaped computational graphs. Further, since product nodes
are learned through correlation metrics, \textproc{Prometheus} is easily adaptable to continuous
data. To some extent, \textproc{Prometheus} also inherits the modularity of $\textproc{LearnSPN}$,
as the choice of how to cluster and what input nodes to use is open to the the user.

\paragraph{Cons.} Although the construction of the correlation graph in \textproc{Prometheus} is
not done greedily (at least in the quadratic version), selecting the decompositions (i.e.\
partitioning the graph into maximal components) is; of course, this is not exactly a drawback but a
compromise, as graph partitioning is a known NP-hard problem \citep{feldmann15}. Because
\textproc{Prometheus} accounts for all decompositions yielded from components after the removal of
each edge from the MST, the circuit can grow considerably, even if we reuse subcircuits at each
recursive call. An alternative would be to globally reuse subcircuits (i.e.\ share $\mathcal{H}$
among different recursive calls) throughout learning, although this curbs expressivity somewhat, as
these subcircuits are learned from possibly (completely) different data. Another option would be to
bound the number of decompositions, or in other words remove only a bounded number of edges from
the MST.

\begin{remark}[breakable]{On variations of divide-and-conquer learning}{divconq}
  Because of \textproc{LearnSPN}'s simplicity and modularity, there is a lot of room for
  improvement. This is reflected in the many works in literature on refining \textproc{LearnSPN} to
  specific data, choosing the right parameters, producing non-tree shaped circuits, and choice of
  input nodes. In this remark segment, we briefly discuss other advances in divide-and-conquer PC
  learning.

  As we have previously mentioned, one of the drawbacks of \textproc{LearnSPN} is the sheer volume
  of hyperparameters involved. \citet{vergari15} suggests simplifying clustering to only binary row
  splits, while \citet{liu19} proposes clustering methods that automatically decide the number of
  clusters from data. Together with \textproc{Prometheus}, the space of hyperparameters to tune is
  greatly reduced.

  We again go back to the issue of reducing the cost of learning variable partitions. Apart
  from \textproc{Prometheus}, \citet{dimauro17a} also investigate more efficient decompositions,
  proposing two approximate sub-quadratic methods to producing variable splits: one by randomly
  sampling pairs of variables and running G-test, and the other by a linear time entropy criterion.

  \citet{vergari15} proposes the use of Chow-Liu Trees as input nodes instead of univariate
  distributions, while \citet{molina17} recommend Poisson distributions for modeling negative
  dependence. \citet{bueff18} combines \textproc{LearnSPN} with weighted model integration by
  learning polynomials as input nodes for continuous variables and counts for discrete data.
  \citet{molina18} adapts \textproc{LearnSPN} to hybrid domains by employing the randomized
  dependence coefficient for both clustering and variable partitioning, with pairwise polynomial
  approximations for input nodes.

  Other contributions include adapting \textproc{LearnSPN} to relational data \citep{nath15}, an
  empirical study comparing different techniques for clustering and partitioning in
  \textproc{LearnSPN} \citep{butz18a}, and \textproc{LearnSPN} post-processing strategies for
  deriving non-tree graphs \citep{tahrima16}.
\end{remark}

\section{Incremental Learning}
\label{sec:incremental}

Learning algorithms from the \divclass{} class heavily rely on recursively constructing a
probabilistic circuit in a top-down fashion. This facilitates learning, as we need only to greedily
optimize at a local level. We now draw our attention to incremental algorithms that iteratively
grow an initial circuit. These usually require a search over possible candidate nodes to be
extended, and as such involve evaluating the entire circuit to determine best scores. In this
section, we look at two examples of \incrclass{} class learning algorithms: \textproc{LearnPSDD}
and \textproc{Strudel}.

\subsection{\textproc{LearnPSDD}}

As the name suggests, \textproc{LearnPSDD} \citep{liang17} learns a smooth, structure decomposable
and deterministic probabilistic circuit (see \Cref{subsection:touncertainty}), meaning its
computational graph must respect a vtree. We therefore must address the issue of learning the vtree
before we turn to the PC learning algorithm \emph{per se}.

Recall that for a vtree $\vtree$, every inner node $v\in\vtree$ with $\set{X}=\Sc(v^\gets)$ and
$\set{Y}=\Sc(v^\to)$ determines that $\set{X}$ and $\set{Y}$ are independent, i.e.\
$p_\mathcal{C}(\set{X},\set{Y})=p_\mathcal{C}(\set{X}) p_\mathcal{C}(\set{Y})$ for a PC
$\mathcal{C}$. This means that a PC's vtree is pivotal in embedding the independencies of the
circuit's distribution. With this in mind, \citet{liang17} propose two approaches to inducing
vtrees from data, both of which use mutual information
\begin{equation*}
  \mutualinf(\set{X},\set{Y})=\sum_{\set{X}=\set{x}}\sum_{\set{Y}=\set{y}}p(\set{x},\set{y})\log\frac{p(\set{x},\set{y})}{p(\set{x})p(\set{y})}
\end{equation*}
for deciding independence. To avoid computing an exponential number of MI terms, an approximation
based on the average pairwise MI is computed instead
\begin{equation*}
  \pairmi(\set{X},\set{Y})=\frac{1}{|\set{X}||\set{Y}|}\cdot\sum_{X\in\set{X}}\sum_{Y\in\set{Y}}\mutualinf(X,Y).
\end{equation*}
The first approach learns vtrees in a top-down fashion, starting with a full scope and recursively
partitioning down to the unit set. The second learns bottom-up, starting with singletons and
joining sets of variables up to full scope.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    \def\ngon{8}
    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,0) {};
    \foreach \i in {1,...,3} {
      \foreach \j in {2,...,3} {
        \draw[gray,thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \foreach \i in {4,...,\ngon} {
      \foreach \j in {5,...,\ngon} {
        \draw[gray,thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \foreach \i in {1,...,3} {
      \foreach \j in {4,...,\ngon} {
        \draw[thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxpink!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxpink!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.5,1)$) {1};
    \node[label=below:{$\{A,B,C\}$},draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-1,-1)$) {2};
    \node[label=below:{$\{D,E,F,G,H\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (1,-1)$) {3};
    \draw (r) -- (vl); \draw (r) -- (vr);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,0) {};
    \draw[gray,thick] (p.corner 4) -- (p.corner 5);
    \draw[gray,thick] (p.corner 6) -- (p.corner 7);
    \draw[gray,thick] (p.corner 6) -- (p.corner 8);
    \draw[gray,thick] (p.corner 7) -- (p.corner 8);
    \foreach \i in {4,...,5} {
      \foreach \j in {6,...,\ngon} {
        \draw[thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxblue!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.5,1.5)$) {1};
    \node[label=below:{$\{A,B,C\}$},draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-1,-1)$) {2};
    \node[draw,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (1,-1.5)$) {3};
    \node[label=below:{$\{D,E\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vrl) at ($(vr) + (-1,-1.5)$) {4};
    \node[label=below:{$\{F,G,H\}$},draw,fill=boxblue!50,inner sep=2pt,minimum size=13pt] (vrr) at ($(vr) + (1,-1.5)$) {5};
    \draw (r) -- (vl); \draw (r) -- (vr); \draw (vr) -- (vrl); \draw (vr) -- (vrr);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,-5.5) {};
    \draw[gray,thick] (p.corner 2) -- (p.corner 1);
    \draw[thick] (p.corner 3) -- (p.corner 1);
    \draw[thick] (p.corner 3) -- (p.corner 2);

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxblue!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.75,1.5)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-0.5,-1)$) {2};
    \node[draw,fill=boxgreen!80,inner sep=2pt,minimum size=13pt] (vll) at ($(vl) + (-1.25,-1)$) {$C$};
    \node[label=below:{$\{A,B\}$},draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vlr) at ($(vl) + (0.0,-1)$) {6};
    \node[draw,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (0.5,-1)$) {3};
    \node[label=below:{$\{D,E\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vrl) at ($(vr) + (-0.0,-1)$) {4};
    \node[label=below:{$\{F,G,H\}$},draw,fill=boxblue!50,inner sep=2pt,minimum size=13pt] (vrr) at ($(vr) + (1.25,-1)$) {5};
    \draw (r) -- (vl); \draw (r) -- (vr); \draw (vr) -- (vrl); \draw (vr) -- (vrr);
    \draw (vl) -- (vll); \draw (vl) -- (vlr);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,-5.5) {};
    \draw[thick] (p.corner 1) -- (p.corner 2);

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxblue!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.75,1.5)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-0.5,-1)$) {2};
    \node[draw,fill=boxgreen!80,inner sep=2pt,minimum size=13pt] (vll) at ($(vl) + (-1.25,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (vlr) at ($(vl) + (0.0,-1)$) {6};
    \node[draw,fill=boxpurple!60,inner sep=2pt,minimum size=13pt] (vlrl) at ($(vlr) + (-1.25,-1)$) {$B$};
    \node[draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vlrr) at ($(vlr) + (-0,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (0.5,-1)$) {3};
    \node[label=below:{$\{D,E\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vrl) at ($(vr) + (-0.0,-1)$) {4};
    \node[label=below:{$\{F,G,H\}$},draw,fill=boxblue!50,inner sep=2pt,minimum size=13pt] (vrr) at ($(vr) + (1.25,-1)$) {5};
    \draw (r) -- (vl); \draw (r) -- (vr); \draw (vr) -- (vrl); \draw (vr) -- (vrr);
    \draw (vl) -- (vll); \draw (vl) -- (vlr); \draw (vlr) -- (vlrl); \draw (vlr) -- (vlrr);
  \end{tikzpicture}
  }
  \caption{Snapshots of four iterations from running the vtree top-down learning strategy with
    pairwise mutual information. Each iteration shows a variable partitioning, the cut-set that
    minimizes the average pairwise mutual information as black edges, and the subsequent (partial)
    vtree. The algorithm finishes when all partitions are singletons.}
  \label{fig:topdownvtree}
\end{figure}

\paragraph{Top-down vtree learning.} Let $\mathcal{G}$ a fully connected weighted graph where
variables are nodes. For each edge $\edge{X}{Y}$, attribute its weight as $\mutualinf(X,Y)$.
Learning the vtree top-down amounts to partitioning $\mathcal{G}$ such that the cut-set that
divides the two partitions $\set{X}$ and $\set{Y}$ is minimal with respect to $\pairmi$.
\citet{liang17} further argue that balanced vtrees produce smaller PCs, and so they reduce learning
to a balanced min-cut bipartition problem. Although this is known to be NP-complete
\citep{garey90}, optimized solvers are able to produce high quality bipartitions efficiently
\citep{karypsis98}. In a nutshell, the vtree construction goes as follows: find a balanced min-cut
bipartition $(\set{X}, \set{Y})$ in $\mathcal{G}$ minimizing the $\pairmi$ of the edges; add a
vtree inner node representing this bipartition and connect it to the two vtrees produced by the
recursive calls over $\set{X}$ and $\set{Y}$; if $\set{X}=\{X\}$ (resp. $|\set{Y}|=\{Y\}$), produce
a leaf node $X$ (resp. $Y$). \Cref{fig:topdownvtree} shows four iterations of this procedure.

\paragraph{Bottom-up vtree learning.} Again, take $\mathcal{G}$ as the fully connected weighted
graph from computing the pairwise mutual information of variables. Now consider that every node of
$\mathcal{G}$ is a vtree whose only node is the variable itself. To learn a vtree bottom-up is to
find pairings of vtrees such that the mutual information between them is high, meaning that the
partitionings at higher levels are minimized (and so determine the ``true'' independence
relationships between subsets of variables). To produce balanced vtrees, the algorithm attempts to
join vtrees of same height whose $\pairmi$ is maximal; this is equivalent to min-cost perfect
matching, which can be solved, in our case, in $\bigo(m^4)$, where $m$ is the number of variables
\citep{edmonds65,kolmogorov09}.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    \def\ngon{8}
    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,0) {};
    \pgfmathsetmacro{\ngonm}{\ngon-1}
    \draw[thick] (p.corner 1) -- (p.corner 2);
    \foreach \i in {3,...,\ngon} {\draw[gray,thick] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {2,...,\ngonm} {
      \pgfmathsetmacro{\k}{int(\i+1)}
      \foreach \j in {\k,...,\ngon} {
        \draw[gray,thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \draw[thick] (p.corner 3) -- (p.corner 4);
    \draw[thick] (p.corner 5) -- (p.corner 6);
    \draw[thick] (p.corner 7) -- (p.corner 8);
    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxred!70] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxgray] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxgoldenrod!70] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.5)$) {};
    \node[draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[fill=boxpurple!60,draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[fill=boxgreen!80,draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[fill=boxred!70,draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2)$) {$E$};
    \node[fill=boxgray,draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2)$) {$F$};
    \node[fill=boxgoldenrod!70,draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2)$) {$G$};
    \node[fill=boxblue!50,draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,0) {};

    \foreach \i in {3,...,4} {\draw[thick] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {3,...,4} {\draw[thick] (p.corner 2) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 2) -- (p.corner \i);}

    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 3) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 4) -- (p.corner \i);}

    \foreach \i in {7,...,\ngon} {\draw[thick,gray] (p.corner 5) -- (p.corner \i);}
    \foreach \i in {7,...,\ngon} {\draw[thick,gray] (p.corner 6) -- (p.corner \i);}

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxgreen!80] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.5)$) {};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[fill=boxgreen!80,draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[fill=boxblue!50,draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[draw,inner sep=2pt,minimum size=13pt] (rr1) at ($(r) + (0,0.5)$) {5};
    \node[draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2)$) {$E$};
    \node[draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2)$) {$F$};
    \node[draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2)$) {$G$};
    \node[draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);
    \draw (rr1) -- (r1); \draw (rr1) -- (r2);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,-5.5) {};

    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 2) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 3) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 4) -- (p.corner \i);}

    \foreach \i in {7,...,\ngon} {\draw[thick] (p.corner 5) -- (p.corner \i);}
    \foreach \i in {7,...,\ngon} {\draw[thick] (p.corner 6) -- (p.corner \i);}

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxorange!80] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.5)$) {};
    \node[draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[fill=boxblue!50,draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (rr1) at ($(r) + (0,0.5)$) {5};
    \node[draw,inner sep=2pt,minimum size=13pt] (rr2) at ($(r) + (0,-3.5)$) {6};
    \node[draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2)$) {$E$};
    \node[draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2)$) {$F$};
    \node[draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2)$) {$G$};
    \node[draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);
    \draw (rr1) -- (r1); \draw (rr1) -- (r2); \draw (rr2) -- (r3); \draw (rr2) -- (r4);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,-5.5) {};

    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 2) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 3) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 4) -- (p.corner \i);}

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxorange!80] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxpink!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxpink!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.75)$) {};
    \node[draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (rr1) at ($(r) + (0,0.5)$) {5};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (rr2) at ($(r) + (0,-3.75)$) {6};
    \node[draw,inner sep=2pt,minimum size=13pt] (rrr) at ($(r) + (0,-1.65)$) {7};
    \node[draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2.25)$) {$E$};
    \node[draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2.25)$) {$F$};
    \node[draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2.25)$) {$G$};
    \node[draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2.25)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);
    \draw (rr1) -- (r1); \draw (rr1) -- (r2); \draw (rr2) -- (r3); \draw (rr2) -- (r4);
    \draw (rrr) -- (rr1); \draw (rrr) -- (rr2);
  \end{tikzpicture}
  }
  \caption{Snapshots from running the vtree bottom-up learning strategy with pairwise mutual
    information. Snapshots show pairings of two vtrees, with edges between partitions joined into
    a single edge whose weight is the average pairwise mutual information of all
    collapsed edges. In black are edges that correspond to the matchings that maximize the average
    pairwise mutual information. The algorithm finishes when all vtrees have been joined together
    into a single tree.}
  \label{fig:bottomupvtree}
\end{figure}

\textproc{LearnPSDD} is an incremental learning algorithm. This means that it takes an existing PC
and incrementally grows the circuit by some criterion, preserving the structural constraints from
the PC in the process. Once a vtree $\vtree$ has been learned from data, we use it to construct an
initial circuit that respects $\vtree$. The choice of circuit initialization is dependent on our
task. For example, within the context of PSDDs, we are mostly interested in starting out with a PC
induced from an LC encoding a certain knowledge base (see \Cref{section:pckb}); this is usually
done in a case-by-case basis, where LCs are compiled for a particular task and then promoted to PCs
(see \Cref{rem:initpc}). However, if one does not require specifying the distribution's support,
any PC will do.

\emph{How} and \emph{where} the circuit is grown -- once we have acquired a vtree and an initial
circuit -- are the main topics of interest now. We first address the matter of \emph{how}, i.e.\
how can we increase a PC's expressivity such that we preserve a desired set of structural
constraints; and later of \emph{where}, i.e.\ which portions of the circuit are eligible for
growth and how do we know they are good candidates.

\begin{figure}[t]
  \begin{center}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newNamedOrNode[scale=1,draw=red,very thick,inputs=nn]{r}{0,0}{$\alpha$};
      \newAndNode[draw=red,very thick,inputs=nn]{p1}{0,-1};
      \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{$(p1.input 1) + (0,-1)$}{$\beta$};
      \newNamedOrNode[inputs=nn]{s2}{$(p1.input 2) + (1.5,-1)$}{$\gamma$};
      \newAndNode[inputs=nn]{p2}{$(s1.input 1) + (-1.0,-1)$};
      \newAndNode[inputs=nn]{p3}{$(s1.input 2) + (1.0,-1)$};
      \newOrNode[inputs=nn]{s3}{$(p2.input 2) + (0.5,-1)$};
      \newOrNode[inputs=nn]{s4}{$(p3.input 2) + (0.5,-1)$};
      \node (a) at ($(p2.input 1) + (0.0,-1)$) {$A$};
      \node (na) at ($(p3.input 1) + (0.0,-1)$) {$\neg A$};
      \draw[draw=red,very thick] (r.west) -- (p1.east);
      \draw[draw=red,very thick] (p1.input 1) -- (s1.east);
      \draw[draw=red,very thick] (p1.input 2) -- ++(0,-0.25) -| (s2.east);
      \draw (s1.input 1) -- ++(0,-0.25) -| (p2.east);
      \draw (s1.input 2) -- ++(0,-0.25) -| (p3.east);
      \draw (p2.input 1) -- (a);
      \draw (p2.input 2) -- ++(0,-0.25) -| (s3.east);
      \draw (p3.input 1) -- (na);
      \draw (p3.input 2) -- ++(0,-0.25) -| (s4.east);

      \node[style={single arrow,draw=red,thick}] (arrow) at (1.75,-0.5)
        {\small\textrm{\textsc{Split}} on $A$};

      \begin{scope}[every node/.style={minimum size=15pt}]
        \newNamedOrNode[draw=red,very thick,inputs=nn]{r}{$(arrow.east) + (1.25,0.5)$}{$\alpha$};
        \newAndNode[draw=red,very thick,inputs=nn]{p1}{$(r) + (0.75,-1)$};
        \newAndNode[draw=red,very thick,inputs=nn]{p12}{$(r) + (-0.75,-1)$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{$(p1.input 1) + (0,-1)$}{\tiny$\beta\wedge\overline{A}$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s12}{$(p12.input 1) + (0,-1)$}{\tiny$\beta\wedge A$};
        \newNamedOrNode[inputs=nn]{s2}{$(p1.input 2) + (1.25,-1)$}{$\gamma$};
        \newAndNode[inputs=nn]{p2}{$(s12.west) + (0,-1)$};
        \newAndNode[inputs=nn]{p3}{$(s1.west) + (0,-1)$};
        \newOrNode[inputs=nn]{s3}{$(p2.input 2) + (0.5,-1)$};
        \newOrNode[inputs=nn]{s4}{$(p3.input 2) + (0.5,-1)$};
      \end{scope}

      \node (a) at ($(p2.input 1) + (0.0,-1)$) {$A$};
      \node (na) at ($(p3.input 1) + (0.0,-1)$) {$\neg A$};
      \draw[draw=red,very thick] (r.input 1) -- ++(0,-0.15) -| (p12.east);
      \draw[draw=red,very thick] (r.input 2) -- ++(0,-0.15) -| (p1.east);
      \draw[draw=red,very thick] (p1.input 1) -- (s1.east);
      \draw[draw=red,very thick] (p1.input 2) -- ++(0,-0.25) -| (s2.east);
      \draw[draw=red,very thick] (p12.input 2) -- ++(0,-0.35) -| (s2.east);
      \draw[draw=red,very thick] (p12.input 1) -- (s12.east);
      \draw (s12.west) -- ++(0,-0.25) -| (p2.east);
      \draw (s1.west) -- ++(0,-0.25) -| (p3.east);
      \draw (p2.input 1) -- (a);
      \draw (p2.input 2) -- ++(0,-0.25) -| (s3.east);
      \draw (p3.input 1) -- (na);
      \draw (p3.input 2) -- ++(0,-0.25) -| (s4.east);

      \draw[dashed,boxdgray,very thick] ($(r) + (2.9,0.5)$) -- ($(r) + (2.9,-5.1)$);

      \begin{scope}[xshift=9cm]
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{0,-1.5}{$\alpha$};
        \newAndNode[inputs=nn]{r1}{-1,0};
        \newAndNode[inputs=nn]{r2}{1,0};
        \newAndNode[inputs=nn]{p1}{$(r1)+(0,-3)$};
        \newAndNode[inputs=nn]{p2}{$(r2)+(0,-3)$};
        \newOrNode[inputs=nn]{l11}{$(p1.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l12}{$(p1.input 2)+(1,-1.5)$};
        \newOrNode[inputs=nn]{l21}{$(p2.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l22}{$(p2.input 2)+(1,-1.5)$};
        \draw (r1.west) -- ++(0,-0.25) -| (s1.east);
        \draw[draw=red,very thick] (r2.west) -- ++(0,-0.25) -| (s1.east);
        \draw (s1.input 1) -- ++(0,-0.25) -| (p1.east);
        \draw (s1.input 2) -- ++(0,-0.25) -| (p2.east);
        \draw (p1.input 1) -- ++(0,-0.25) -| (l11.east);
        \draw (p1.input 2) -- ++(0,-0.25) -| (l12.east);
        \draw (p2.input 1) -- ++(0,-0.25) -| (l21.east);
        \draw (p2.input 2) -- ++(0,-0.25) -| (l22.east);

        \node[style={single arrow,draw=red,thick}] (arrow) at (2.5,-0.5)
          {\small\textrm{\textsc{Clone}}};

        \newAndNode[inputs=nn]{r1}{4.25,0};
        \newAndNode[inputs=nn]{r2}{6.25,0};
        \newAndNode[inputs=nn]{p1}{$(r1)+(-0.5,-3)$};
        \newAndNode[inputs=nn]{p2}{$(r2)+(-0.5,-3)$};
        \newOrNode[inputs=nn]{l11}{$(p1.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l12}{$(p1.input 2)+(1,-1.5)$};
        \newOrNode[inputs=nn]{l21}{$(p2.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l22}{$(p2.input 2)+(1,-1.5)$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{$(r1.west) + (0,-1)$}{$\alpha$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s2}{$(r2.west) + (0,-1)$}{$\alpha$};
        \newAndNode[draw=red,very thick,inputs=nn]{p3}{$(p1)+(1,0)$};
        \newAndNode[draw=red,very thick,inputs=nn]{p4}{$(p2)+(1,0)$};
        \draw (r1.west) -- ++(0,-0.25) -| (s1.east);
        \draw[draw=red,very thick] (r2.west) -- ++(0,-0.25) -| (s2.east);
        \draw (s1.input 1) -- ++(0,-0.5) -| (p1.east);
        \draw (s1.input 2) -- ++(0,-0.5) -| (p2.east);
        \draw[draw=red,very thick] (s2.input 1) -- ++(0,-0.25) -| (p3.east);
        \draw[draw=red,very thick] (s2.input 2) -- ++(0,-0.25) -| (p4.east);
        \draw (p1.input 1) -- ++(0,-0.25) -| (l11.east);
        \draw (p1.input 2) -- ++(0,-0.5) -| (l12.east);
        \draw (p2.input 1) -- ++(0,-0.25) -| (l21.east);
        \draw (p2.input 2) -- ++(0,-0.5) -| (l22.east);
        \draw[draw=red,very thick] (p3.input 1) -- ++(0,-0.25) -| (l11.east);
        \draw[draw=red,very thick] (p3.input 2) -- ++(0,-0.5) -| (l12.east);
        \draw[draw=red,very thick] (p4.input 1) -- ++(0,-0.25) -| (l21.east);
        \draw[draw=red,very thick] (p4.input 2) -- ++(0,-0.5) -| (l22.east);
      \end{scope}
    \end{tikzpicture}
    }
  \end{center}
  \caption{\textproc{Split} (left) and \textproc{Clone} (right) operations for growing a circuit.
    Nodes and edges highlighted in red show the modified structure. In both cases smoothness,
    (structure) decomposability and determinism are evidently preserved.}
  \label{fig:splitclone}
\end{figure}

\citeauthor{liang17} propose two local transformations for growing a circuit $\mathcal{C}$:
\textproc{Split} and \textproc{Clone}. The first acts by multiplying a product node $\Prod$ into
$\Prod_1,\ldots,\Prod_k$ products such that $\pi_1,\ldots,\pi_k$ (primes of $\Prod_1,\ldots,
\Prod_k$ respectively) are mutually exclusive. This is done by attributing all possible values of a
variable in $\Sc(\Prod)$, say $A$, to each prime, meaning that $\pi_i$ will contain the assignment
$A=i$ for every $i\in\left[k\right]$. This attribution is done by partially copying
$\mathcal{C}_{\Prod}$ into $k$ circuits $\mathcal{C}_{\Prod}^{(1)},\ldots,\mathcal{C}_{\Prod}^{(k)}$
up to some depth and then conditioning $\mathcal{C}_{\Prod}^{(i)}$ with $\liv A=i\riv$. This is
straightforward for the discrete case: at the appropriate vtree node (i.e.\ one that contains $A$
as a leaf), replace the input node whose scope is $A$ into an indicator node, setting it to the
appropriate assignment of $A$. Although \citet{liang17} only considers the binary case, the
transformation can be extended to the continuous if we consider $k$ piecewise distributions whose
support is over only a set interval. Naturally, input nodes must then have their support truncated
to the appropriate $i$-th interval, which is no easy feat in the general case. The left side of
\Cref{fig:splitclone} shows \textproc{Split} for the binary case.

The other proposed transformation, \textproc{Clone}, does something similar for sum nodes. Pick a
sum node $\Sum$ whose children are $\Child_1,\ldots,\Child_k$ and parents $\Prod_1$ and $\Prod_2$;
double $\Sum$ and $\Child_1,\ldots,\Child_k$, producing clones $\Sum'$ and $\Child_1',\ldots,
\Child_k'$. Disconnect the edge coming from $\Prod_2$ to $\Sum$ and instead connect it to $\Sum'$.
Connect all $\Child_1',\ldots,\Child_k'$ to the same children as their original. This operation is
visualized on the right side of \Cref{fig:splitclone}.

\subsection{\textproc{Strudel}}

\begin{remark}[breakable]{On the choice of an initial circuit}{initpc}
  \citep{choi13,oztok15,choi17,shen17}
\end{remark}

\section{Random Learning}
\label{sec:random}
