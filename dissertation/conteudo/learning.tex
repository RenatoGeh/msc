\chapter{Learning Probabilistic Circuits}
\label{ch:learning}

As we have seen in \Cref{ch:pc}, inference in probabilistic circuits is, for the most part,
straightforward.  This is not so much the case when \emph{learning} PCs. Despite the uncomplicated
syntax, learning sufficiently expressive PCs in a principled way is comparatively harder than, say
the usual neural network. For a start, we are usually required to comply with smoothness and
decomposability to ensure marginalization at the least. This restriction excludes the possibility
of adopting any of the most popular neural network patterns or architectures used in deep learning
today. To make matters worse, constructing a PC graph more often than not involves costly
statistical tests that make learning their structure a challenge for high dimensional data.

In this chapter, we review the most popular PC structure learning algorithms, their pros and cons,
and more importantly, what can we learn from them to efficiently build scalable probabilistic
circuits. We broadly divide existing structure learners into three main categories:
divide-and-conquer (\divclass{}, \Cref{sec:divconq}), incremental methods (\incrclass{},
\Cref{sec:incremental}) and random approaches (\randclass{}, \Cref{sec:random}).

\section{Divide-and-Conquer Learning}
\label{sec:divconq}

Arguably the most popular approach to learning the structure of probabilistic circuits are
algorithms that follow a \emph{divide-and-conquer} scheme\footnote{The algorithms we shall see in
this class are sometimes classified as \emph{constraint-based} \citep{spirtes95} learners, as they
learn the model by identifying independences within data. Although this is true for the examples
here mentioned, the two taxonomies are not equivalent. In fact, we describe a random
divide-and-conquer structure learning approach in \Cref{sec:data} that does not (directly) rely on
statistical tests.}. This class of PC learning algorithms, which here we denote by \divclass{}, are
characterized by recursive calls over (usually mutually exclusive) subsets of data in true
divide-and-conquer fashion. This kind of procedure is more clearly visualized by
\textproc{LearnSPN}, the first, most well-known, and perhaps most archetypal of its class.

Before we explain \textproc{LearnSPN} however, we must first address how we denote data. Data is
commonly represented as a matrix where rows are assignments (of all variables), and columns are the
values that each variable takes at each assignment. Let $\set{D}\in\mathbb{R}^{n \times m}$ be a
matrix with $n$ rows and $m$ columns. We use $\set{D}_{i,j}$ to access an element of $\set{D}$ at
the $i$-th row, $j$-th column of matrix $\set{D}$. We denote by $\set{D}_{\set{i},\set{j}}$, where
$\set{i}\subseteq \left[1..n\right]$ and $\set{j}\subseteq\left[1..m\right]$ are sets of indices, a
submatrix from the extraction of the $\set{i}$ rows and $\set{j}$ columns of $\set{D}$. We use a
colon as a shorthand for selecting all rows or columns, e.g.\ $\set{D}_{:,:}=\set{D}$,
$\set{D}_{:,j}$ is the $j$-th column and $\set{D}_{i,:}$ is the $i$-th row.

\subsection{\textproc{LearnSPN}}
\label{sec:learnspn}

Recall the semantics of sum and product nodes in a smooth and decomposable probabilistic circuit.
A sum node encodes a mixture of distributions $p(\set{X})=\sum_{i=1}^m w_i\cdot p_i(\set{X})$ whose
children scopes are all the same. A product node encodes a factorization
$p(\set{X}_1,\ldots,\set{X}_m)=\prod_{i=1}^m p(\set{X}_i)$, implying that
$\set{X}_i\indep\set{X}_j$ for $i,j\in [m]$ and $i\neq j$. \textproc{LearnSPN} \citep{gens13}
exploits these semantics in an intuitive and uncomplicated manner: sum children are defined by
sub-PCs learned from similar (by some arbitrary metric) assignments, and product children are
sub-PCs learned from data conditioned on the variables defined by their scope. In practice, this
means that, for a dataset $\set{D}\in\mathbb{R}^{n\times m}$, sums assign rows to their children,
while product children are assigned columns. This procedures continues recursively until data are
reduced to a $k\times 1$ matrix, in which case a univariate distribution acting as input node is
learned from it. This recursive procedure is shown more formally in \Cref{alg:learnspn}.

\begin{algorithm}[t]
  \caption{\textproc{LearnSPN}}\label{alg:learnspn}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \IIf{$|\set{X}|=1$}{\textbf{return} an input node learned from $\set{D}$}
    \NIElse
      \State Find scope partitions $\set{X}_1,\ldots,\set{X}_t\subseteq\set{X}$ st
        $\set{X}_i\indep\set{X}_j$ for $i\neq j$
      \IIf{$k>1$}{\textbf{return} $\prod_{j=1}^t \textproc{LearnSPN}(\set{D}_{:,\set{X}_j},
        \set{X}_j)$}
      \NIElse
        \State Find subsets of data $\set{x}_1,\ldots,\set{x}_k\subseteq\set{D}$ st all assignments
          within $\set{x}_i$ are all similar
        \State \textbf{return} $\sum_{i=1}^k \frac{|\set{x}_i|}{|\set{D}|}\cdot
          \textproc{LearnSPN}(\set{x}_i,\set{X})$
      \EndNIElse
    \EndNIElse
  \end{algorithmic}
\end{algorithm}

Notably, \citep{gens13} purposely does not strictly specify which techniques should be used for
assigning rows and columns, although they do provide empirical results on a particular form of
\textproc{LearnSPN} where row assignments are computed through EM clustering and products by
pairwise G-testing. Instead, they call the algorithm a \emph{schema} that incorporates several
actual learning algorithms whose concrete form depends on the choice of how to split data.

\subsubsection{Complexity}

To be able to analyze the complexity of \textproc{LearnSPN}, we assume a common implementation
where sums are learned from $k$-means clustering, and products through pairwise G-testing. We know
learning sums is efficient: $k$-means takes $\bigo(n\cdot k\cdot m\cdot c)$ time, where $k$ is the
number of clusters and $c$ the number of iterations to be run. Products, on the other hand, are
much more costly. The naÃ¯ve approach would be to find every possible combination of variable
partitions of any size and compute statistical independence tests over these subsets of variables,
which would take superexponential time on the number of variables. Instead, \textproc{LearnSPN}
proposes the faster approach of testing for pairwise independence $X_i\indep X_j$ for every
possible combination. This is clearly quadratic on the number of variables
$\bigo\left(\binom{m}{2}=\frac{m!}{2(m-2)!}\right)$ assuming an $\bigo(1)$ oracle for independence
testing. In reality, G-test takes $\bigo(n\cdot m)$ time, as we must compute a ratio of observed
versus expected values for each cell in the contingency table. This brings the total runtime for
products to a whopping $\bigo\left(n\cdot m^3\right)$, prohibitive to any reasonably large dataset.
In terms of space, independence tests most commonly used require either a correlation (for
continuous data) or contingency (for discrete data) matrix that takes up $\bigo(m^2)$ space,
another barrier for scaling up to high dimensional data.

\begin{figure}[t]
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{subfigure}{0.45\textwidth}
      \resizebox{\textwidth}{!}{
      \begin{tabular}{ccccc}
        \hline
        $A$ & $B$ & $C$ & $D$ & $E$\\
        \hline
        \rowcolor{boxgreen!70}
        0 & 1 & 0 & 0 & 1\\
        \rowcolor{boxgreen!70}
        1 & 0 & 1 & 1 & 1\\
        \rowcolor{boxblue!50}
        1 & 1 & 0 & 1 & 1\\
        \rowcolor{boxblue!50}
        0 & 0 & 1 & 0 & 0\\
        \rowcolor{boxgreen!70}
        1 & 1 & 0 & 1 & 0\\
        \rowcolor{boxblue!50}
        0 & 1 & 1 & 0 & 1\\
        \rowcolor{boxorange!60}
        1 & 0 & 1 & 1 & 1\\
        \rowcolor{boxorange!60}
        1 & 1 & 0 & 0 & 0\\
        \rowcolor{boxblue!50}
        0 & 1 & 1 & 0 & 1\\
        \hline
      \end{tabular}
      }
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \resizebox{\textwidth}{!}{
      \begin{tikzpicture}
        \newSumNode[fill=boxpink!50]{r}{0,0};
        \newProdNode[fill=boxgreen!70]{p1}{$(r) + (-1.5,-1.5)$};
        \newProdNode[fill=boxorange!60]{p2}{$(r) + (0,-1.5)$};
        \newProdNode[fill=boxblue!50]{p3}{$(r) + (1.5,-1.5)$};
        \draw[edge] (r) -- node[midway,above left] {$\frac{3}{9}$} (p1);
        \draw[edge] (r) -- node[midway,left] {$\frac{2}{9}$} (p2);
        \draw[edge] (r) -- node[midway,above right] {$\frac{4}{9}$} (p3);
      \end{tikzpicture}
      }
    \end{subfigure}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{subfigure}{0.45\textwidth}
      \newcolumntype{x}{>{\columncolor{boxgreen!70}}c}
      \newcolumntype{y}{>{\columncolor{boxorange!60}}c}
      \newcolumntype{z}{>{\columncolor{boxblue!50}}c}
      \resizebox{\textwidth}{!}{
      \begin{tabular}{xyyzx}
        \hline
        \multicolumn{1}{c}{$A$} & \multicolumn{1}{c}{$B$} & \multicolumn{1}{c}{$C$} &
        \multicolumn{1}{c}{$D$} & \multicolumn{1}{c}{$E$}\\
        \hline
        0 & 1 & 0 & 0 & 1\\
        1 & 0 & 1 & 1 & 1\\
        1 & 1 & 0 & 1 & 1\\
        0 & 0 & 1 & 0 & 0\\
        1 & 1 & 0 & 1 & 0\\
        0 & 1 & 1 & 0 & 1\\
        1 & 0 & 1 & 1 & 1\\
        1 & 1 & 0 & 0 & 0\\
        0 & 1 & 1 & 0 & 1\\
        \hline
      \end{tabular}
      }
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \resizebox{\textwidth}{!}{
      \begin{tikzpicture}
        \newProdNode[fill=boxpink!50]{r}{0,0};
        \newSumNode[label=below:{$\{A,E\}$},fill=boxgreen!70]{s1}{$(r) + (-1.5,-1.5)$};
        \newSumNode[label=below:{$\{B,C\}$},fill=boxorange!60]{s2}{$(r) + (0,-1.5)$};
        \newSumNode[label=below:{$\{D\}$},fill=boxblue!50]{s3}{$(r) + (1.5,-1.5)$};
        \draw[edge] (r) -- (s1); \draw[edge] (r) -- (s2); \draw[edge] (r) -- (s3);
      \end{tikzpicture}
      }
    \end{subfigure}
    \caption{}
  \end{subfigure}
  \caption{\textproc{LearnSPN} assigns either rows \uncaption{(a)} or columns \uncaption{(b)} for
    sum and product nodes respectively. For sums, their edge weights are set proportionally to the
    assignments. For product children, scopes are defined by which columns are assigned to them.}
  \label{fig:learnspn}
\end{figure}

Alternatively, instead of computing the G-test for every possible combination of variables,
\citep{gens13} constructs an independence graph $\mathcal{G}$ whose nodes are variables and edges
indicate whether two variables are statistically dependent. Within this context, the variable
partitions we attribute to product children are exactly the connected components of $\mathcal{G}$,
meaning it suffices testing only some combinations. This is made clear by the following example:
suppose we have an incomplete independence graph $\mathcal{G}$ where, at a certain point in the
process of finding the (independent) variable partitions, we know there to be two components
$\set{X}$ and $\set{Y}$; by hypothesis there is no edge connecting any variable in $\set{X}$ to any
other variable in $\set{Y}$. The task of determining whether $\set{X}$ and $\set{Y}$ are, truly, a single
component boils down to finding a pair of variables $X\in\set{X}$ and $Y\in\set{Y}$ such that the
independence tests on the two returns that $X\not\indep Y$. If, at the next iteration, we luckily choose
such a pair, no other pair of $\set{X}$ and $\set{Y}$ needs to be tested any longer, as we have
already shown $\set{X}$ and $\set{Y}$ to belong to the same component. Note that, had we not used
this heuristic, every pair would still need to be tested, even if they are known to be in the same
component. Even so, this heuristic is still cubic on the number of variables in the worst case.
\Cref{fig:indepgraph} shows $\mathcal{G}$, the spanning forest resulted from the connected
component heuristic, and the equivalent product node from this decomposition.

\subsubsection{Pros and cons}

\paragraph{Pros.} Perhaps the main factor for \textproc{LearnSPN}'s popularity is how easily
implementable, intuitive and modular it is. Even more remarkably, it is an empirically competitive
PC learning algorithm despite its age, serving as a baseline for most subsequent works in PC
literature. Lastly, the fact that each recursive call from \textproc{LearnSPN} is completely
independent from each the other makes it an attractive candidate for CPU parallelization.

\paragraph{Cons.} Debatably, one of the key weakness of \textproc{LearnSPN} is its tree-shaped
computational graph, meaning that they are strictly less succint compared to non-tree DAG PCs
\citep{martens14}. In terms of runtime efficiency, the algorithm struggles on high dimensional
data due to the complexity involved in computing costly statistical tests. Despite
\Cref{alg:learnspn} giving the impression that no hyperparameter tuning is needed for
\textproc{LearnSPN}, in practice the modules for learning sums and products often take many
parameters, most of which (if not all) are exactly the same for every recursive call. This can have
a negative impact on the algorithm's performance, since the same parameters are repeatedly used
even under completely different data.

\begin{figure}[t]
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=8,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxgreen] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxred!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
      \node[circle,fill=boxgray] (p6) at (p.corner 6) {$F$};
      \node[circle,fill=boxgoldenrod!70] (p7) at (p.corner 7) {$G$};
      \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};
      \draw[thick] (p1) -- (p2) -- (p3); \draw[thick] (p1) -- (p3);
      \draw[thick] (p3) -- (p4); \draw[thick] (p1) -- (p4);
      \draw[thick] (p5) -- (p6) -- (p7); \draw[thick] (p5) -- (p7);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=8,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxgreen] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxred!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
      \node[circle,fill=boxgray] (p6) at (p.corner 6) {$F$};
      \node[circle,fill=boxgoldenrod!70] (p7) at (p.corner 7) {$G$};
      \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};
      \draw[thick] (p1) -- (p2); \draw[thick] (p1) -- (p3);
      \draw[thick] (p1) -- (p4); \draw[thick] (p1) -- (p4);
      \draw[thick] (p5) -- (p6); \draw[thick] (p5) -- (p7);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \begin{tikzpicture}
      \newProdNode[fill=boxgreen]{r}{0,0};
      \newSumNode[label=below:{$\{A,B,C,D\}$},fill=boxorange!80]{s1}{$(r) + (-1.25,-2.5)$};
      \newSumNode[label=below:{$\{E,F,G\}$},fill=boxpink!50]{s2}{$(r) + (0,-1.75)$};
      \newSumNode[label=below:{$\{H\}$},fill=boxblue!50]{s3}{$(r) + (1.25,-1.0)$};
      \draw[edge] (r) -- (s1); \draw[edge] (r) -- (s2); \draw[edge] (r) -- (s3);
    \end{tikzpicture}
    \caption{}
  \end{subfigure}
  \caption{The pairwise (in)dependence graph where each node is a variable. In \uncaption{(a)} we
    show the full graph, computing independence tests for each pair of variables in $\bigo(m^2)$.
    However, it suffices to compute for only the connected components \uncaption{(b)}, saving up
    pairwise computation time for reachable nodes.  The resulting product node and scope
    partitioning is shown in \uncaption{(c)}.}
  \label{fig:indepgraph}
\end{figure}

\subsection{\textproc{ID-SPN}}
\label{sec:idspn}

A subtle yet effective way of improving the performance of \textproc{LearnSPN} is to consider
tractable probabilistic models over many variables as input nodes instead of univariate
distributions. \textproc{ID-SPN} \citep{rooshenas14} does so by assuming that input nodes are
tractable Markov networks. Further, instead of blindly applying the recursion over subsequent
sub-data, it attempts to compute some metric of quality from each node. The worst scored node is
then replaced with a \textproc{LearnSPN}-like tree. This is repeated until no significant increase
in likelihood is observed. \Cref{alg:idspn} shows the \textproc{ID-SPN} pipeline, where
\textproc{ExtendID} is used in line \ref{alg:idspn:line:extend} to grow the circuit in a
divide-and-conquer fashion. The name \textproc{ID-SPN} comes from \emph{direct} variable
interactions, meaning the relationships modeled through the Markov networks as input nodes; and
\emph{indirect} interactions brought from the latent variable interpretation of sum nodes.
\Cref{fig:idspn} shows two hypothetical iterations of ID-SPN, with each call expanding the
probabilistic circuit into either a sum or product over Markov networks.

With respect to its implementation, \textproc{ID-SPN} is as modular as \textproc{LearnSPN} in the
sense that the data partitioning is left as a subroutine. Indeed, even the choice of input
distributions is customizable: although \citeauthor{rooshenas14} recommend Markov networks, any
tractable distribution will do. Despite this seemingly small change compared to the original
\textproc{LearnSPN} algorithm, \textproc{ID-SPN} seems to perform better compared to its
counterpart most of the time \citep{rooshenas14,jaini18a}, although at a cost to learning speed.
Further, because of the enormous parameter space brought by having to learn Markov networks as
inputs \emph{and} perform the optimizations from sums and products, grid search hyperparameter
tuning is infeasible. \citep{rooshenas14} recommend random search \citep{bergstra12a} as an
alternative.

\begin{algorithm}[t]
  \caption{\textproc{ExtendID}}\label{alg:extendid}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$, and memoization
      function $\mathcal{M}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \State Find scope partitions $\set{X}_1,\ldots,\set{X}_t\subseteq\set{X}$ st
    \If{$k>1$}
      \For{each $j\in\left[t\right]$}
        \State $\Node_j\gets\textproc{LearnMarkov}(\set{D}_{:,\set{X}_j},\set{X}_j)$
        \State Associate $\mathcal{M}(\Node_j)$ with $\set{D}_{:,\set{X}_j}$ and $\set{X}_j$
      \EndFor
      \State \textbf{return} $\prod_{j=1}^t \Node_j$
    \Else
      \State Find subsets of data $\set{x}_1,\ldots,\set{x}_k\subseteq\set{D}$ st all assignments
        within $\set{x}_i$ are all similar
      \For{each $i\in\left[k\right]$}
        \State $\Node_i\gets\textproc{LearnMarkov}(\set{x}_i,\set{X})$
        \State Associate $\mathcal{M}(\Node_i)$ with $\set{x}_i$ and $\set{X}$
      \EndFor
      \State \textbf{return} $\sum_{i=1}^k \frac{|\set{x}_i|}{|\set{D}|}\cdot\Node_i$
    \EndIf
  \end{algorithmic}
\end{algorithm}

\subsubsection{Complexity}

As \textproc{ID-SPN} is a special case of \textproc{LearnSPN}, the analysis for the sums and
products subroutines holds. The only difference is on the runtime complexity for learning input
nodes and the convergence rate for \textproc{ID-SPN}. Assuming input nodes are learned from the
method suggested by \citet{rooshenas14}, which involves learning a probabilistic circuit from a
Markov network \citep{lowd13a}, then each ``input'' node takes time $\bigo(i\cdot c(r\cdot n+m))$,
where $i$ is the number of iterations to run, $c$ is the size of the generated PC, and constant $r$
is a bound on the number of candidate improvements to the circuit, which can grow exponentially for
multi-valued variables. Importantly, opposite from \textproc{LearnSPN} where we only learn input
nodes once per call \emph{if} data is univariate, \textproc{ID-SPN} requires learning multiple
multivariate inputs for \emph{every} \textproc{ExtendID} call.

\begin{algorithm}[t]
  \caption{\textproc{ID-SPN}}\label{alg:idspn}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \State Create a single-node PC: $\mathcal{C}\gets\textproc{LearnMarkov}(\set{D},\set{X})$
    \State Let $\mathcal{M}$ be a memoization function associating a node with a dataset and scope
    \State Call $\mathcal{C}'$ a copy of $\mathcal{C}$
    \While{improving $\mathcal{C}$ yields better likelihood}
      \State Pick worse node $\Node$ from $\mathcal{C}'$
      \State Extract sub-data $\set{D}'$ and sub-scope $\set{X}'$ from $\mathcal{M}(\Node)$
      \State Replace $\Node$ with $\textproc{ExtendID}(\set{D}',\set{X}',\mathcal{M})$\label{alg:idspn:line:extend}
      \IIf{$\mathcal{C}'$ has better likelihood than $\mathcal{C}$}{$\mathcal{C}\gets\mathcal{C}$}
    \EndWhile
    \State \textbf{return} $\mathcal{C}$
  \end{algorithmic}
\end{algorithm}

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    % Markov 1
    \node[fill=boxteal] (a) at (0,0) {$A$};
    \node[fill=boxorange!80] (b) at ($(a) + (1,-1)$) {$B$};
    \node[fill=boxpink!50] (c) at ($(a) + (1,0)$) {$C$};
    \node[fill=boxgoldenrod!70] (d) at ($(a) + (0,-1)$) {$D$};
    \node[fill=boxred!70] (e) at ($(d) + (0,-1)$) {$E$};
    \node[fill=boxpurple!60] (f) at ($(b) + (0,-1)$) {$F$};
    \draw (a) -- (b); \draw (b) -- (c);
    \draw (a) -- (c); \draw (a) -- (d);
    \draw (e) -- (f); \draw (f) -- (d);
    \draw[red,very thick,dashed] (-0.5,0.5) rectangle ($(f) + (0.5,-0.5)$);
    \node at ($(e) + (0.5,-1)$) {Initial Markov network};

    \draw[edge,line width=0.2cm,red] (2,-1) -- (3.5,-1);

    \newProdNode[fill=boxgreen]{r}{6.25,0};

    % Markov 2
    \node[fill=boxpink!50] (a) at ($(r) + (0.75,-1)$) {$A$};
    \node[fill=boxteal] (b) at ($(a) + (1,0)$) {$B$};
    \node[fill=boxorange!80] (c) at ($(b) + (0,-1)$) {$C$};
    \draw (a) -- (b); \draw (b) -- (c);
    \draw[edge] (r) -- ($(a) + (0.5,0.5)$);
    \draw[red,very thick,dashed] ($(a) + (-0.5,0.5)$) rectangle ($(c) + (0.5,-0.5)$);

    % Markov 3
    \node[fill=boxgoldenrod!70] (f) at ($(r) + (-0.75,-1)$) {$F$};
    \node[fill=boxred!70] (d) at ($(f) + (-1,0)$) {$D$};
    \node[fill=boxpurple!60] (e) at ($(d) + (1,-1)$) {$E$};
    \draw (d) -- (f); \draw (e) -- (d);
    \draw[edge] (r) -- ($(d) + (0.5,0.5)$);
    \draw[boxdgray,thick,dashed] ($(d) + (-0.5,0.5)$) rectangle ($(e) + (0.5,-0.5)$);

    \node (i1) at ($(r) + (0,-3)$) {Iteration 1};

    \draw[edge,line width=0.2cm,red] ($(b) + (1,0)$) -- ($(b) + (2.5,0)$);

    \newProdNode[fill=boxgreen]{r}{14.5,0.5};
    \newSumNode[fill=boxbrown!60]{s}{$(r) + (2.0,-0.5)$};
    \draw[edge] (r) -- (s);

    \begin{scope}[local bounding box=m1]
      \node[fill=boxorange!80] (b) at ($(s) + (-1.5,-1)$) {$B$};
      \node[fill=boxteal] (a) at ($(b) + (-1,0)$) {$A$};
      \node[fill=boxpink!50] (c) at ($(b) + (0,-1)$) {$C$};
      \draw (a) -- (b); \draw (a) -- (c);
      \draw[boxdgray,thick,dashed] ($(a) + (-0.5,0.5)$) rectangle ($(c) + (0.5,-0.5)$);
    \end{scope}
    \draw[edge] (s) -- (m1.north);

    \begin{scope}[local bounding box=m2]
      \node[fill=boxteal] (a) at ($(s) + (0.0,-1.5)$) {$A$};
      \node[fill=boxorange!80] (b) at ($(a) + (1,0)$) {$B$};
      \node[fill=boxpink!50] (c) at ($(b) + (0,-1)$) {$C$};
      \draw (a) -- (c); \draw (b) -- (c); \draw (a) -- (b);
      \draw[boxdgray,thick,dashed] ($(a) + (-0.5,0.5)$) rectangle ($(c) + (0.5,-0.5)$);
    \end{scope}
    \draw[edge] (s) -- (m2.north);

    % Markov 3
    \node[fill=boxgoldenrod!70] (f) at ($(r) + (-2,-1)$) {$F$};
    \node[fill=boxred!70] (d) at ($(f) + (-1,0)$) {$D$};
    \node[fill=boxpurple!60] (e) at ($(d) + (1,-1)$) {$E$};
    \draw (d) -- (f); \draw (e) -- (d);
    \draw[edge] (r) -- ($(d) + (0.5,0.5)$);
    \draw[boxdgray,thick,dashed] ($(d) + (-0.5,0.5)$) rectangle ($(e) + (0.5,-0.5)$);

    \node at ($(r) + (0,-3.5)$) {Iteration 2};
  \end{tikzpicture}
  }
  \caption{Two iterations of \textproc{ID-SPN}, where the contents inside the dashed line are
  Markov networks. The red color indicates that a node has been chosen as the best candidate for an
  extension with \textproc{ExtendID}. Although here we only extend input nodes, inner nodes can in
  fact be extended as well.}
  \label{fig:idspn}
\end{figure}

\subsubsection{Pros and Cons}

\paragraph{Pros.} If we assume any multivariate distribution in place of Markov networks, PCs
learned from \textproc{ID-SPN} are strictly more expressive than ones learned from
\textproc{LearnSPN}, as input nodes could potentially be replaced with \textproc{LearnSPN}
distributions. Additionally, the modularity inherited from \textproc{LearnSPN} allows
\textproc{ID-SPN} to adapt to data according to expert knowledge, bringing some flexibility to the
algorithm.

\paragraph{Cons.} Unfortunately, most of the disadvantages from \textproc{LearnSPN} also apply to
\textproc{ID-SPN}. Just like \textproc{LearnSPN}, independence tests are more often than not a
bottleneck for most executions with resonably large number of variables. However, \textproc{ID-SPN}
relies on a likelihood improvement for the computational graph to be extended, which ends up
curbing the easy parallelization aspect of \textproc{LearnSPN}. Besides, the complexity involved in
learning Markov networks (or any other complex multivariate distribution as input node) carries a
heavy weight during learning. This, coupled with the fact that hyperparameter tuning in the huge
parameter space of \textproc{ID-SPN} must be done by a random search method, can take a heavy price
in terms of learning time.

\subsection{\textproc{Prometheus}}
\label{sec:prometheus}

So far, we have only considered structure learning algorithms that produce tree-shaped circuits.
Even though \textproc{ID-SPN} \emph{might} produce non-tree graphs at the input nodes depending on
the choice of families of multivariate distributions, it does not do so as a rule. We now turn our
attention to a PC learner that \emph{does} generate non-tree computational graphs in a
divide-and-conquer manner.

Recall that in both \textproc{LearnSPN} and \textproc{ID-SPN} the scope partitioning is done
greedily; we define a graph encoding the pairwise (in)dependencies of variables and greedily search
for connected components by comparing independence test results with some correlation threshold,
adding an edge if the correlation is sufficiently high. The choice of this threshold is often
arbitrary and subject to hyperparameter tuning during learning, which is especially worrying when
dealing with high dimensional data. In this section we review \textproc{Prometheus}
\citep{jaini18a}, a divide-and-conquer \textproc{LearnSPN}-like PC learning algorithm with two main
features that stand out compared to the last two methods we have seen so far: (1) it requires no
hyperparameter tuning for variable partitionings, and (2) accepts a more scalable alternative to
computing all pairwise correlations.

\begin{figure}[t]
  \begin{subfigure}[t]{0.25\textwidth}
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=5,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxred!70] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxpink!50] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxgoldenrod!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxblue!50] (p5) at (p.corner 5) {$E$};
      \draw[thick] (p1) -- node[midway,above left] {$0.8$} (p2);
      \draw[thick] (p1) -- node[midway,left,yshift=0.1cm] {$0.2$} (p3);
      \draw[thick] (p1) -- node[midway,right,yshift=0.1cm] {$0.4$} (p4);
      \draw[thick] (p1) -- node[midway,above right] {$0.3$} (p5);
      \draw[thick] (p2) -- node[midway,below left] {$0.6$} (p3);
      \draw[thick] (p2) -- node[midway,below left,xshift=0.2cm] {$0.4$} (p4);
      \draw[thick] (p2) -- node[midway,above] {$0.1$} (p5);
      \draw[thick] (p3) -- node[midway,below] {$0.9$} (p4);
      \draw[thick] (p3) -- node[midway,below right,xshift=-0.2cm] {$0.5$} (p5);
      \draw[thick] (p4) -- node[midway,right] {$0.7$} (p5);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.25\textwidth}
    \centering
    \resizebox{0.9\textwidth}{!}{
    \begin{tikzpicture}
      \node[regular polygon,regular polygon sides=5,minimum size=4cm] (p) {};
      \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
      \node[circle,fill=boxred!70] (p2) at (p.corner 2) {$B$};
      \node[circle,fill=boxpink!50] (p3) at (p.corner 3) {$C$};
      \node[circle,fill=boxgoldenrod!70] (p4) at (p.corner 4) {$D$};
      \node[circle,fill=boxblue!50] (p5) at (p.corner 5) {$E$};
      \draw[very thick,blue] (p1) -- node[midway,above left,xshift=-0.1cm,yshift=0.1cm] {$\mathbf{0.8}$} node[fill=white,inner sep=1pt] {$e_2$} (p2);
      \draw[very thick,red] (p2) -- node[midway,left,xshift=-0.1cm,yshift=-0.1cm] {$\mathbf{0.6}$} node[fill=white,inner sep=1pt] {$e_4$} (p3);
      \draw[very thick,boxdgray] (p3) -- node[midway,below,yshift=-0.1cm] {$\mathbf{0.9}$} node[fill=white,inner sep=1pt] {$e_1$} (p4);
      \draw[very thick,orange!80] (p4) -- node[midway,right,xshift=0.1cm,yshift=-0.1cm] {$\mathbf{0.7}$} node[fill=white,inner sep=1pt] {$e_3$} (p5);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newSumNode[fill=boxgreen!75]{r}{0,0};
      \newProdNode[fill=boxdgray!80]{p1}{$(r) + (-3,-1)$};
      \newProdNode[fill=blue!50]{p2}{$(r) + (-1,-1)$};
      \newProdNode[fill=orange!80]{p3}{$(r) + (1,-1)$};
      \newProdNode[fill=red!60]{p4}{$(r) + (3,-1)$};
      \newSumNode[label=below:{$\{A,B,C\}$},fill=boxgreen!75]{s1}{$(r) + (-3.5,-3.0)$};
      \newSumNode[label=below:{$\{D,E\}$},fill=boxgreen!75]{s2}{$(r) + (-2.5,-2.5)$};
      \newSumNode[label=below:{$\{B,C\}$},fill=boxgreen!75]{s3}{$(r) + (-1.5,-2.0)$};
      \newSumNode[label=below:{$\{A\}$},fill=boxgreen!75]{s4}{$(r) + (-0.5,-2.0)$};
      \newSumNode[label=below:{$\{D\}$},fill=boxgreen!75]{s5}{$(r) + (0.5,-2.0)$};
      \newSumNode[label=below:{$\{E\}$},fill=boxgreen!75]{s6}{$(r) + (1.5,-2.0)$};
      \newSumNode[label=below:{$\{B\}$},fill=boxgreen!75]{s7}{$(r) + (2.5,-2.0)$};
      \newSumNode[label=below:{$\{C\}$},fill=boxgreen!75]{s8}{$(r) + (3.5,-2.0)$};
      \draw[edge] (r) -- (p1.north);
      \draw[edge] (r) -- (p2.north);
      \draw[edge] (r) -- (p3.north);
      \draw[edge] (r) -- (p4.north);
      \draw[edge,blue!80] (p1) -- (s1.north);
      \draw[edge,blue!80] (p1) -- (s2.north);
      \draw[edge,red!80] (p2) -- (s2.north);
      \draw[edge,red!80] (p2) -- (s3.north);
      \draw[edge,red!80] (p2) -- (s4.north);
      \draw[edge,boxdgray!80] (p3) -- (s3.north);
      \draw[edge,boxdgray!80] (p3) -- (s4.north);
      \draw[edge,boxdgray!80] (p3) -- (s5.north);
      \draw[edge,boxdgray!80] (p3) -- (s6.north);
      \draw[edge,orange!80] (p4) -- (s4.north);
      \draw[edge,orange!80] (p4) -- (s5.north);
      \draw[edge,orange!80] (p4) -- (s6.north);
      \draw[edge,orange!80] (p4) -- (s7.north);
      \draw[edge,orange!80] (p4) -- (s8.north);
    \end{tikzpicture}
    }
    \caption{}
  \end{subfigure}
  \caption{The fully connected correlation graph \uncaption{(a)} with weights as the pairwise
    correlation measurements for each pair of variables; the maximum spanning tree for determining
    decompositions \uncaption{(b)}; and the mixture of decompositions \uncaption{(c)}. Colors in
    \uncaption{(b)} match their partitionings in \uncaption{(c)}.}
  \label{fig:prometheus}
\end{figure}

Let $\mathcal{G}$ be the independence graph for scope $\set{X}=\{X_1,X_2,\ldots,X_m\}$. Remember that
$\mathcal{G}$'s vertices are $\set{X}$ and each (undirected) edge $\overline{X_i X_j}$ coming from
$X_i$ to $X_j$ means that $X_i\notindep X_j$. Previously, we constructed $\mathcal{G}$ by comparing
the output of an independence test (such as the G-test) against a threshold (e.g.\ a sufficiently
low $p$-value). Instead, suppose $\mathcal{G}$ is fully connected and that we attribute weights
corresponding to a correlation metric of $X_i$ against $X_j$ for each edge (e.g.\ Pearson's
correlation coefficient). The \emph{maximum spanning tree} (MST) of $\mathcal{G}$, here denoted by
$\mathcal{T}$, defines a graph where the removal of any edge in $\mathcal{T}$ partitions the
component into two subcomponents. Let $e_i$ be the $i$-th lowest (weight) valued edge;
\textproc{Prometheus} obtains a set of decompositions by iteratively removing edges from $e_1$ to
$e_{|\set{X}|-1}$. In other words, the algorithm constructs a product node for each decomposition,
assigning the scope of each child as the scope of each component at each edge removal. These
products are then joined together by a parent sum node that acts as a mixture of decompositions.
\Cref{fig:prometheus} shows an example of $\mathcal{T}$, the subsequent decompositions, and the
resulting mixture of decompositions.

\begin{algorithm}[t]
  \caption{\textproc{Prometheus}}\label{alg:prometheus}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$
    \Ensure A smooth and decomposable probabilistic circuit learned from $\set{D}$
    \IIf{$|\set{X}|$ is sufficiently small}{\textbf{return} an input node learned from $\set{D}$}
    \NIElse
      \State Find subsets of data $\set{x}_1,\ldots,\set{x}_k\subseteq\set{D}$ st all assignments
        within $\set{x}_i$ are all similar
      \State Create a sum node $\Sum$ with initially no children and uniform weights
      \For{each $\set{x}_i$}
        \State $\mathcal{T}\gets\textproc{CorrelationMST}(\set{x}_i,\set{X})$
        \For{each weighted edge $e_j$ in $\mathcal{T}$ in decreasing order}
          \State Remove edge $e_i$ from $\mathcal{T}$
          \State Call $\set{S}_1,\ldots,\set{S}_t$ the scopes of each component in $\mathcal{T}$
          \State Create product node $\Prod_j$ and associate it with $\set{S}_1,\ldots,\set{S}_t$
          \State Associate $\Prod_j$ with dataset $\set{x}_i$
          \State Add $\Prod_j$ as a child of $\Sum$
        \EndFor
      \EndFor
      \State Let $\mathcal{H}$ be a hash table (initially empty) associating scopes to sum nodes
      \For{each $\Prod\in\Ch(\Sum)$}
        \For{each scope $\set{S}$ associated with $\Prod$}
          \If{$\set{S}\not\in\mathcal{H}$}
            \State Let $\set{x}$ be the dataset associated with $\Prod$
            \State $\Node\gets\textproc{Prometheus}(\set{x}_{:,\set{S}},\set{S})$
            \State Add $\Node$ as a child of $\Prod$
            \State $\mathcal{H}_\set{S}\gets\Node$
          \Else
            \State Add $\mathcal{H}_\set{S}$ as a child of $\Prod$
          \EndIf
        \EndFor
      \EndFor
      \State \textbf{return} $\Sum$
    \EndNIElse
  \end{algorithmic}
\end{algorithm}

Sum nodes are learned by clustering data into similar instances, just like in previous cases. Since
the previously mentioned procedure involving products creates a mixture of decompositions (and thus
a sum node), we can simply collapse the consecutive sum layers into a single sum node.
\Cref{alg:prometheus} shows the algorithm in its entirety. \textproc{CorrelationMST} computes the
(fully connected) correlation graph, returning its MST. It is worth mentioning that
\textproc{Prometheus} makes sure each recursive call shares subcircuits whenever scopes are the
same (this is when the hash table $\mathcal{H}$ in \Cref{alg:prometheus} comes into play). This
avoids an exponential growth from the $k\cdot(|\set{X}|-1)$ potential recursive calls.

\subsubsection{Complexity}

Up to now, the computation of decompositions is done by a $\bigo(m^2)$ construction of a fully
connected correlation graph. This gives \textproc{Prometheus} no asymptotic advantage over neither
\textproc{LearnSPN} nor \textproc{ID-SPN}. To change this, \citeauthor{jaini18a} propose a more
scalable alternative: in place of constructing the entire correlation graph, sample $m\log m$
variables and construct a correlation graph where only $\log m$ edges are added for each of these
sampled variables instead, bringing down complexity to $\bigo(m\left(\log m\right)^2)$.

The analysis of sum nodes is exactly the same as \textproc{LearnSPN} if we assume the same
clustering method. If \textproc{Prometheus} is implemented with the same multivariate distributions
as \textproc{ID-SPN} at the input nodes, the analysis for those also holds.

\subsubsection{Pros and Cons}

\paragraph{Pros.} The notable achievements of \textproc{Prometheus} are evidently the absence of
parameters for computing scope partitionings, reducing the dimension of hyperparameters to tune; a
scalable alternative to partitionings that runs in sub-quadratic time; and (more debatably) the
fact that the algorithm produces non-tree shaped computational graphs. Further, since product nodes
are learned through correlation metrics, \textproc{Prometheus} is easily adaptable to continuous
data. To some extent, \textproc{Prometheus} also inherits the modularity of $\textproc{LearnSPN}$,
as the choice of how to cluster and what input nodes to use is open to the the user.

\paragraph{Cons.} Although the construction of the correlation graph in \textproc{Prometheus} is
not done greedily (at least in the quadratic version), selecting the decompositions (i.e.\
partitioning the graph into maximal components) is; of course, this is not exactly a drawback but a
compromise, as graph partitioning is a known NP-hard problem \citep{feldmann15}. Because
\textproc{Prometheus} accounts for all decompositions yielded from components after the removal of
each edge from the MST, the circuit can grow considerably, even if we reuse subcircuits at each
recursive call. An alternative would be to globally reuse subcircuits (i.e.\ share $\mathcal{H}$
among different recursive calls) throughout learning, although this curbs expressivity somewhat, as
these subcircuits are learned from possibly (completely) different data. Another option would be to
bound the number of decompositions, or in other words remove only a bounded number of edges from
the MST.

\begin{remark}[breakable]{On variations of divide-and-conquer learning}{divconq}
  Because of \textproc{LearnSPN}'s simplicity and modularity, there is a lot of room for
  improvement. This is reflected in the many works in literature on refining \textproc{LearnSPN} to
  specific data, choosing the right parameters, producing non-tree shaped circuits, and choice of
  input nodes. In this remark segment, we briefly discuss other advances in divide-and-conquer PC
  learning.

  As we have previously mentioned, one of the drawbacks of \textproc{LearnSPN} is the possibly
  large number of hyperparameters involved, usually dependent on the methods chosen for clustering
  and independence testing. \citet{vergari15} suggests simplifying clustering to only binary row
  splits, while \citet{liu19} proposes clustering methods that automatically decide the number of
  clusters from data. Together with \textproc{Prometheus}, the space of hyperparameters to tune is
  greatly reduced.

  We again go back to the issue of reducing the cost of learning variable partitions. Apart
  from \textproc{Prometheus}, \citet{dimauro17a} also investigate more efficient decompositions,
  proposing two approximate sub-quadratic methods to producing variable splits: one by randomly
  sampling pairs of variables and running G-test, and the other by a linear time entropy criterion.

  \citet{vergari15} proposes the use of Chow-Liu Trees as input nodes instead of univariate
  distributions, while \citet{molina17} recommend Poisson distributions for modeling negative
  dependence. \citet{bueff18} combines \textproc{LearnSPN} with weighted model integration by
  learning polynomials as input nodes for continuous variables and counts for discrete data.
  \citet{molina18} adapts \textproc{LearnSPN} to hybrid domains by employing the randomized
  dependence coefficient for both clustering and variable partitioning, with pairwise polynomial
  approximations for input nodes.

  Other contributions include adapting \textproc{LearnSPN} to relational data \citep{nath15}, an
  empirical study comparing different techniques for clustering and partitioning in
  \textproc{LearnSPN} \citep{butz18a}, and \textproc{LearnSPN} post-processing strategies for
  deriving non-tree graphs \citep{tahrima16}.
\end{remark}

\section{Incremental Learning}
\label{sec:incremental}

Learning algorithms from the \divclass{} class heavily rely on recursively constructing a
probabilistic circuit in a top-down fashion. This facilitates learning, as we need only to greedily
optimize at a local level. We now focus our attention to incremental\footnote{Despite the ambiguous
name, we draw no connection to \emph{online learning}.} algorithms that iteratively grow an initial
circuit. These usually require a search over possible candidate nodes to be extended, and as such
involve evaluating the entire circuit to determine best scores. For this reason, these are also
sometimes classified as \emph{search-and-score} methods \citep{teyssier05}. In this section, we
look at two examples of \incrclass{} class learning algorithms: \textproc{LearnPSDD} and
\textproc{Strudel}.

\subsection{\textproc{LearnPSDD}}
\label{sec:learnpsdd}

As the name suggests, \textproc{LearnPSDD} \citep{liang17} learns a smooth, structure decomposable
and deterministic probabilistic circuit (see \Cref{sec:touncertainty}), meaning its
computational graph must respect a vtree. We therefore must address the issue of learning the vtree
before we turn to the PC learning algorithm \emph{per se}.

Recall that for a vtree $\vtree$, every inner node $v\in\vtree$ with $\set{X}=\Sc(v^\gets)$ and
$\set{Y}=\Sc(v^\to)$ determines that $\set{X}$ and $\set{Y}$ are independent, i.e.\
$p_\mathcal{C}(\set{X},\set{Y})=p_\mathcal{C}(\set{X}) p_\mathcal{C}(\set{Y})$ for a PC
$\mathcal{C}$. This means that a PC's vtree is pivotal in embedding the independencies of the
circuit's distribution. With this in mind, \citet{liang17} propose two approaches to inducing
vtrees from data, both of which use mutual information
\begin{equation}
  \mutualinf(\set{X},\set{Y})=\sum_{\set{X}=\set{x}}\sum_{\set{Y}=\set{y}}p(\set{x},\set{y})\log\frac{p(\set{x},\set{y})}{p(\set{x})p(\set{y})}
\end{equation}
for deciding independence. To avoid computing an exponential number of MI terms, an approximation
based on the average pairwise MI is computed instead
\begin{equation}
  \pairmi(\set{X},\set{Y})=\frac{1}{|\set{X}||\set{Y}|}\cdot\sum_{X\in\set{X}}\sum_{Y\in\set{Y}}\mutualinf(X,Y).
\end{equation}
The first approach learns vtrees in a top-down fashion, starting with a full scope and recursively
partitioning down to the unit set. The second learns bottom-up, starting with singletons and
joining sets of variables up to full scope.

\paragraph{Top-down vtree learning.} Let $\mathcal{G}$ be a fully connected weighted graph where
variables are nodes. For each edge $\edge{XY}$, attribute its weight as $\mutualinf(X,Y)$.
Learning the vtree top-down amounts to partitioning $\mathcal{G}$ such that the cut-set that
divides the two partitions $\set{X}$ and $\set{Y}$ is minimal with respect to $\pairmi$.
\citet{liang17} further argue that balanced vtrees produce smaller PCs, and so they reduce learning
to a balanced min-cut bipartition problem. Although this is known to be NP-complete
\citep{garey90}, optimized solvers are able to produce high quality bipartitions efficiently
\citep{karypsis98}. In a nutshell, the vtree construction goes as follows: find a balanced min-cut
bipartition $(\set{X}, \set{Y})$ in $\mathcal{G}$ minimizing the $\pairmi$ of the edges; add a
vtree inner node representing this bipartition and connect it to the two vtrees produced by the
recursive calls over $\set{X}$ and $\set{Y}$; if $\set{X}=\{X\}$ (resp. $\set{Y}=\{Y\}$), produce
a leaf node $X$ (resp. $Y$). \Cref{fig:topdownvtree} shows four iterations of this procedure.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    \def\ngon{8}
    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,0) {};
    \foreach \i in {1,...,3} {
      \foreach \j in {2,...,3} {
        \draw[gray,thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \foreach \i in {4,...,\ngon} {
      \foreach \j in {5,...,\ngon} {
        \draw[gray,thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \foreach \i in {1,...,3} {
      \foreach \j in {4,...,\ngon} {
        \draw[thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxpink!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxpink!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.5,1)$) {1};
    \node[label=below:{$\{A,B,C\}$},draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-1,-1)$) {2};
    \node[label=below:{$\{D,E,F,G,H\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (1,-1)$) {3};
    \draw (r) -- (vl); \draw (r) -- (vr);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,0) {};
    \draw[gray,thick] (p.corner 4) -- (p.corner 5);
    \draw[gray,thick] (p.corner 6) -- (p.corner 7);
    \draw[gray,thick] (p.corner 6) -- (p.corner 8);
    \draw[gray,thick] (p.corner 7) -- (p.corner 8);
    \foreach \i in {4,...,5} {
      \foreach \j in {6,...,\ngon} {
        \draw[thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxblue!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.5,1.5)$) {1};
    \node[label=below:{$\{A,B,C\}$},draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-1,-1)$) {2};
    \node[draw,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (1,-1.5)$) {3};
    \node[label=below:{$\{D,E\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vrl) at ($(vr) + (-1,-1.5)$) {4};
    \node[label=below:{$\{F,G,H\}$},draw,fill=boxblue!50,inner sep=2pt,minimum size=13pt] (vrr) at ($(vr) + (1,-1.5)$) {5};
    \draw (r) -- (vl); \draw (r) -- (vr); \draw (vr) -- (vrl); \draw (vr) -- (vrr);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,-5.5) {};
    \draw[gray,thick] (p.corner 2) -- (p.corner 1);
    \draw[thick] (p.corner 3) -- (p.corner 1);
    \draw[thick] (p.corner 3) -- (p.corner 2);

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxblue!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.75,1.5)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-0.5,-1)$) {2};
    \node[draw,fill=boxgreen!80,inner sep=2pt,minimum size=13pt] (vll) at ($(vl) + (-1.25,-1)$) {$C$};
    \node[label=below:{$\{A,B\}$},draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vlr) at ($(vl) + (0.0,-1)$) {6};
    \node[draw,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (0.5,-1)$) {3};
    \node[label=below:{$\{D,E\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vrl) at ($(vr) + (-0.0,-1)$) {4};
    \node[label=below:{$\{F,G,H\}$},draw,fill=boxblue!50,inner sep=2pt,minimum size=13pt] (vrr) at ($(vr) + (1.25,-1)$) {5};
    \draw (r) -- (vl); \draw (r) -- (vr); \draw (vr) -- (vrl); \draw (vr) -- (vrr);
    \draw (vl) -- (vll); \draw (vl) -- (vlr);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,-5.5) {};
    \draw[thick] (p.corner 1) -- (p.corner 2);

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxpink!50] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxblue!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node[draw,inner sep=2pt,minimum size=13pt] (r) at ($(p) + (4.75,1.5)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (vl) at ($(r) + (-0.5,-1)$) {2};
    \node[draw,fill=boxgreen!80,inner sep=2pt,minimum size=13pt] (vll) at ($(vl) + (-1.25,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (vlr) at ($(vl) + (0.0,-1)$) {6};
    \node[draw,fill=boxpurple!60,inner sep=2pt,minimum size=13pt] (vlrl) at ($(vlr) + (-1.25,-1)$) {$B$};
    \node[draw,fill=boxorange!80,inner sep=2pt,minimum size=13pt] (vlrr) at ($(vlr) + (-0,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (vr) at ($(r) + (0.5,-1)$) {3};
    \node[label=below:{$\{D,E\}$},draw,fill=boxpink!50,inner sep=2pt,minimum size=13pt] (vrl) at ($(vr) + (-0.0,-1)$) {4};
    \node[label=below:{$\{F,G,H\}$},draw,fill=boxblue!50,inner sep=2pt,minimum size=13pt] (vrr) at ($(vr) + (1.25,-1)$) {5};
    \draw (r) -- (vl); \draw (r) -- (vr); \draw (vr) -- (vrl); \draw (vr) -- (vrr);
    \draw (vl) -- (vll); \draw (vl) -- (vlr); \draw (vlr) -- (vlrl); \draw (vlr) -- (vlrr);
  \end{tikzpicture}
  }
  \caption{Snapshots of four iterations from running the vtree top-down learning strategy with
    pairwise mutual information. Each iteration shows a variable partitioning, the cut-set that
    minimizes the average pairwise mutual information as black edges, and the subsequent (partial)
    vtree. The algorithm finishes when all partitions are singletons.}
  \label{fig:topdownvtree}
\end{figure}

\paragraph{Bottom-up vtree learning.} Again, take $\mathcal{G}$ as the fully connected weighted
graph from computing the pairwise mutual information of variables. Now consider that every node of
$\mathcal{G}$ is a vtree whose only node is the variable itself. To learn a vtree bottom-up is to
find pairings of vtrees such that the mutual information between them is high, meaning that the
partitionings at higher levels are minimized (and so determine the ``true'' independence
relationships between subsets of variables). To produce balanced vtrees, the algorithm attempts to
join vtrees of same height whose $\pairmi$ is maximal; this is equivalent to min-cost perfect
matching, which can be solved, in our case, in $\bigo(m^4)$, where $m$ is the number of variables
\citep{edmonds65,kolmogorov09}. \Cref{fig:bottomupvtree} exemplifies the algorithm.

\textproc{LearnPSDD} is an incremental learning algorithm. This means that it takes an existing PC
and incrementally grows the circuit by some criterion, preserving the structural constraints from
the PC in the process. Once a vtree $\vtree$ has been learned from data, we use it to construct an
initial circuit that respects $\vtree$. The choice of circuit initialization is dependent on our
task. For example, within the context of PSDDs, we are mostly interested in starting out with a PC
induced from an LC encoding a certain knowledge base (see \Cref{sec:pckb}); this is usually
done in a case-by-case basis, where LCs are compiled for a particular task and then promoted to PCs
(see \Cref{rem:initpc}). However, if one does not require specifying the distribution's support,
any PC will do.

\emph{How} and \emph{where} the circuit is grown -- once we have acquired a vtree and an initial
circuit -- are the main topics of interest now. We first address the matter of \emph{how}, i.e.\
how can we increase a PC's expressivity such that we preserve a desired set of structural
constraints; and later of \emph{where}, i.e.\ which portions of the circuit are eligible for
growth and how do we know they are good candidates.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    \def\ngon{8}
    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,0) {};
    \pgfmathsetmacro{\ngonm}{\ngon-1}
    \draw[thick] (p.corner 1) -- (p.corner 2);
    \foreach \i in {3,...,\ngon} {\draw[gray,thick] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {2,...,\ngonm} {
      \pgfmathsetmacro{\k}{int(\i+1)}
      \foreach \j in {\k,...,\ngon} {
        \draw[gray,thick] (p.corner \i) -- (p.corner \j);
      }
    }
    \draw[thick] (p.corner 3) -- (p.corner 4);
    \draw[thick] (p.corner 5) -- (p.corner 6);
    \draw[thick] (p.corner 7) -- (p.corner 8);
    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxpurple!60] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxred!70] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxgray] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxgoldenrod!70] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.5)$) {};
    \node[draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[fill=boxpurple!60,draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[fill=boxgreen!80,draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[fill=boxred!70,draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2)$) {$E$};
    \node[fill=boxgray,draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2)$) {$F$};
    \node[fill=boxgoldenrod!70,draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2)$) {$G$};
    \node[fill=boxblue!50,draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,0) {};

    \foreach \i in {3,...,4} {\draw[thick] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {3,...,4} {\draw[thick] (p.corner 2) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 2) -- (p.corner \i);}

    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 3) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 4) -- (p.corner \i);}

    \foreach \i in {7,...,\ngon} {\draw[thick,gray] (p.corner 5) -- (p.corner \i);}
    \foreach \i in {7,...,\ngon} {\draw[thick,gray] (p.corner 6) -- (p.corner \i);}

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxgreen!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxgreen!80] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.5)$) {};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[fill=boxgreen!80,draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[fill=boxblue!50,draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[draw,inner sep=2pt,minimum size=13pt] (rr1) at ($(r) + (0,0.5)$) {5};
    \node[draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2)$) {$E$};
    \node[draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2)$) {$F$};
    \node[draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2)$) {$G$};
    \node[draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);
    \draw (rr1) -- (r1); \draw (rr1) -- (r2);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (0,-5.5) {};

    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 2) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 3) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick,gray] (p.corner 4) -- (p.corner \i);}

    \foreach \i in {7,...,\ngon} {\draw[thick] (p.corner 5) -- (p.corner \i);}
    \foreach \i in {7,...,\ngon} {\draw[thick] (p.corner 6) -- (p.corner \i);}

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxorange!80] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxblue!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxblue!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.5)$) {};
    \node[draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[fill=boxblue!50,draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (rr1) at ($(r) + (0,0.5)$) {5};
    \node[draw,inner sep=2pt,minimum size=13pt] (rr2) at ($(r) + (0,-3.5)$) {6};
    \node[draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2)$) {$E$};
    \node[draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2)$) {$F$};
    \node[draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2)$) {$G$};
    \node[draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);
    \draw (rr1) -- (r1); \draw (rr1) -- (r2); \draw (rr2) -- (r3); \draw (rr2) -- (r4);

    \node[regular polygon,regular polygon sides=\ngon,minimum size=4cm] (p) at (10,-5.5) {};

    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 1) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 2) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 3) -- (p.corner \i);}
    \foreach \i in {5,...,\ngon} {\draw[thick] (p.corner 4) -- (p.corner \i);}

    \node[circle,fill=boxorange!80] (p1) at (p.corner 1) {$A$};
    \node[circle,fill=boxorange!80] (p2) at (p.corner 2) {$B$};
    \node[circle,fill=boxorange!80] (p3) at (p.corner 3) {$C$};
    \node[circle,fill=boxorange!80] (p4) at (p.corner 4) {$D$};
    \node[circle,fill=boxpink!50] (p5) at (p.corner 5) {$E$};
    \node[circle,fill=boxpink!50] (p6) at (p.corner 6) {$F$};
    \node[circle,fill=boxpink!50] (p7) at (p.corner 7) {$G$};
    \node[circle,fill=boxpink!50] (p8) at (p.corner 8) {$H$};

    \node (r) at ($(p) + (4.75,1.75)$) {};
    \node[draw,inner sep=2pt,minimum size=13pt] (r1) at ($(r) + (-1,0)$) {1};
    \node[draw,inner sep=2pt,minimum size=13pt] (r2) at ($(r) + (1,0)$) {2};
    \node[draw,inner sep=2pt,minimum size=13pt] (r3) at ($(r) + (-1,-3)$) {3};
    \node[draw,inner sep=2pt,minimum size=13pt] (r4) at ($(r) + (1,-3)$) {4};
    \node[fill=boxorange!80,draw,inner sep=2pt,minimum size=13pt] (rr1) at ($(r) + (0,0.5)$) {5};
    \node[fill=boxpink!50,draw,inner sep=2pt,minimum size=13pt] (rr2) at ($(r) + (0,-3.75)$) {6};
    \node[draw,inner sep=2pt,minimum size=13pt] (rrr) at ($(r) + (0,-1.65)$) {7};
    \node[draw,inner sep=2pt,minimum size=13pt] (a) at    ($(r) + (-1.5,-1)$) {$A$};
    \node[draw,inner sep=2pt,minimum size=13pt] (b) at    ($(r) + (-0.5,-1)$) {$B$};
    \node[draw,inner sep=2pt,minimum size=13pt] (c) at   ($(r) + (0.5,-1)$) {$C$};
    \node[draw,inner sep=2pt,minimum size=13pt] (d) at       ($(r) + (1.5,-1)$) {$D$};
    \node[draw,inner sep=2pt,minimum size=13pt] (e) at      ($(r) + (-1.5,-2.25)$) {$E$};
    \node[draw,inner sep=2pt,minimum size=13pt] (f) at         ($(r) + (-0.5,-2.25)$) {$F$};
    \node[draw,inner sep=2pt,minimum size=13pt] (g) at ($(r) + (0.5,-2.25)$) {$G$};
    \node[draw,inner sep=2pt,minimum size=13pt] (h) at      ($(r) + (1.5,-2.25)$) {$H$};
    \draw (r1) -- (a); \draw (r1) -- (b);
    \draw (r2) -- (c); \draw (r2) -- (d);
    \draw (r3) -- (e); \draw (r3) -- (f);
    \draw (r4) -- (g); \draw (r4) -- (h);
    \draw (rr1) -- (r1); \draw (rr1) -- (r2); \draw (rr2) -- (r3); \draw (rr2) -- (r4);
    \draw (rrr) -- (rr1); \draw (rrr) -- (rr2);
  \end{tikzpicture}
  }
  \caption{Snapshots from running the vtree bottom-up learning strategy with pairwise mutual
    information. Snapshots show pairings of two vtrees, with edges between partitions joined into a
    single edge whose weight is the average pairwise mutual information of all collapsed edges. In
    black are edges that correspond to the matchings that maximize the average pairwise mutual
    information. The algorithm finishes when all vtrees have been joined together into a single
    tree.}
  \label{fig:bottomupvtree}
\end{figure}

\citet{liang17} propose two local transformations for growing a circuit $\mathcal{C}$:
\textproc{Split} and \textproc{Clone}. The first acts by multiplying a sum node's product child
$\Prod$ into $\Prod_1,\ldots,\Prod_k$ products such that $\pi_1,\ldots,\pi_k$ (primes of
$\Prod_1,\ldots,\Prod_k$ respectively) are mutually exclusive. This is done by attributing all
possible values of a variable in $\Sc(\Prod)$, say $A$, to each prime, meaning that $\pi_i$ will
contain the assignment $A=i$ for every $i\in\left[k\right]$. This attribution is done by partially
copying $\mathcal{C}_{\Prod}$ into $k$ circuits $\mathcal{C}_{\Prod}^{(1)},\ldots,
\mathcal{C}_{\Prod}^{(k)}$ up to some depth $m$ and then conditioning $\mathcal{C}_{\Prod}^{(i)}$
on $\liv A=i\riv$. This is straightforward for the discrete case: at the appropriate vtree node
(i.e.\ one that contains $A$ as a leaf), replace the input node whose scope is $A$ into an
indicator node, setting it to the appropriate assignment of $A$. Although \citet{liang17} only
considers the binary case, the transformation can be extended to the continuous if we consider $k$
piecewise distributions whose support is over only a set interval. Naturally, input nodes must then
have their support truncated to the appropriate $i$-th interval, which is no easy feat in the
general case. The left side of \Cref{fig:splitclone} shows \textproc{Split} for the binary case.

The other proposed transformation, \textproc{Clone}, does something similar for sum nodes. Pick a
sum node $\Sum$ whose children are $\Child_1,\ldots,\Child_k$ and parents $\Prod_1$ and $\Prod_2$;
double $\Sum$ and $\Child_1,\ldots,\Child_k$, producing clones $\Sum'$ and $\Child_1',\ldots,
\Child_k'$. Disconnect the edge coming from $\Prod_2$ to $\Sum$ and instead connect it to $\Sum'$.
Connect all $\Child_1',\ldots,\Child_k'$ to the same children as their original counterparts. This
operation is visualized on the right side of \Cref{fig:splitclone}. One can further extend
\textproc{Clone} to apply this operation cloning nodes up to some depth $m$ and then joining the
last remaining deepest nodes similar to what was described for $\Child_1',\ldots,\Child_k'$.

\begin{figure}[t]
  \begin{center}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \newNamedOrNode[scale=1,draw=red,very thick,inputs=nn]{r}{0,0}{$\alpha$};
      \newAndNode[draw=red,very thick,inputs=nn]{p1}{0,-1};
      \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{$(p1.input 1) + (0,-1)$}{$\beta$};
      \newNamedOrNode[inputs=nn]{s2}{$(p1.input 2) + (1.5,-1)$}{$\gamma$};
      \newAndNode[inputs=nn]{p2}{$(s1.input 1) + (-1.0,-1)$};
      \newAndNode[inputs=nn]{p3}{$(s1.input 2) + (1.0,-1)$};
      \newOrNode[inputs=nn]{s3}{$(p2.input 2) + (0.5,-1)$};
      \newOrNode[inputs=nn]{s4}{$(p3.input 2) + (0.5,-1)$};
      \node (a) at ($(p2.input 1) + (0.0,-1)$) {$A$};
      \node (na) at ($(p3.input 1) + (0.0,-1)$) {$\neg A$};
      \draw[draw=red,very thick] (r.west) -- (p1.east);
      \draw[draw=red,very thick] (p1.input 1) -- (s1.east);
      \draw[draw=red,very thick] (p1.input 2) -- ++(0,-0.25) -| (s2.east);
      \draw (s1.input 1) -- ++(0,-0.25) -| (p2.east);
      \draw (s1.input 2) -- ++(0,-0.25) -| (p3.east);
      \draw (p2.input 1) -- (a);
      \draw (p2.input 2) -- ++(0,-0.25) -| (s3.east);
      \draw (p3.input 1) -- (na);
      \draw (p3.input 2) -- ++(0,-0.25) -| (s4.east);

      \node[style={single arrow,draw=red,thick}] (arrow) at (1.75,-0.5)
        {\small\textrm{\textsc{Split}} on $A$};

      \begin{scope}[every node/.style={minimum size=15pt}]
        \newNamedOrNode[draw=red,very thick,inputs=nn]{r}{$(arrow.east) + (1.25,0.5)$}{$\alpha$};
        \newAndNode[draw=red,very thick,inputs=nn]{p1}{$(r) + (0.75,-1)$};
        \newAndNode[draw=red,very thick,inputs=nn]{p12}{$(r) + (-0.75,-1)$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{$(p1.input 1) + (0,-1)$}{\tiny$\beta\wedge\overline{A}$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s12}{$(p12.input 1) + (0,-1)$}{\tiny$\beta\wedge A$};
        \newNamedOrNode[inputs=nn]{s2}{$(p1.input 2) + (1.25,-1)$}{$\gamma$};
        \newAndNode[inputs=nn]{p2}{$(s12.west) + (0,-1)$};
        \newAndNode[inputs=nn]{p3}{$(s1.west) + (0,-1)$};
        \newOrNode[inputs=nn]{s3}{$(p2.input 2) + (0.5,-1)$};
        \newOrNode[inputs=nn]{s4}{$(p3.input 2) + (0.5,-1)$};
      \end{scope}

      \node (a) at ($(p2.input 1) + (0.0,-1)$) {$A$};
      \node (na) at ($(p3.input 1) + (0.0,-1)$) {$\neg A$};
      \draw[draw=red,very thick] (r.input 1) -- ++(0,-0.15) -| (p12.east);
      \draw[draw=red,very thick] (r.input 2) -- ++(0,-0.15) -| (p1.east);
      \draw[draw=red,very thick] (p1.input 1) -- (s1.east);
      \draw[draw=red,very thick] (p1.input 2) -- ++(0,-0.25) -| (s2.east);
      \draw[draw=red,very thick] (p12.input 2) -- ++(0,-0.35) -| (s2.east);
      \draw[draw=red,very thick] (p12.input 1) -- (s12.east);
      \draw (s12.west) -- ++(0,-0.25) -| (p2.east);
      \draw (s1.west) -- ++(0,-0.25) -| (p3.east);
      \draw (p2.input 1) -- (a);
      \draw (p2.input 2) -- ++(0,-0.25) -| (s3.east);
      \draw (p3.input 1) -- (na);
      \draw (p3.input 2) -- ++(0,-0.25) -| (s4.east);

      \draw[dashed,boxdgray,very thick] ($(r) + (2.9,0.5)$) -- ($(r) + (2.9,-5.1)$);

      \begin{scope}[xshift=9cm]
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{0,-1.5}{$\alpha$};
        \newAndNode[inputs=nn]{r1}{-1,0};
        \newAndNode[inputs=nn]{r2}{1,0};
        \newAndNode[inputs=nn]{p1}{$(r1)+(0,-3)$};
        \newAndNode[inputs=nn]{p2}{$(r2)+(0,-3)$};
        \newOrNode[inputs=nn]{l11}{$(p1.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l12}{$(p1.input 2)+(1,-1.5)$};
        \newOrNode[inputs=nn]{l21}{$(p2.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l22}{$(p2.input 2)+(1,-1.5)$};
        \draw (r1.west) -- ++(0,-0.25) -| (s1.east);
        \draw[draw=red,very thick] (r2.west) -- ++(0,-0.25) -| (s1.east);
        \draw (s1.input 1) -- ++(0,-0.25) -| (p1.east);
        \draw (s1.input 2) -- ++(0,-0.25) -| (p2.east);
        \draw (p1.input 1) -- ++(0,-0.25) -| (l11.east);
        \draw (p1.input 2) -- ++(0,-0.25) -| (l12.east);
        \draw (p2.input 1) -- ++(0,-0.25) -| (l21.east);
        \draw (p2.input 2) -- ++(0,-0.25) -| (l22.east);

        \node[style={single arrow,draw=red,thick}] (arrow) at (2.5,-0.5)
          {\small\textrm{\textsc{Clone}}};

        \newAndNode[inputs=nn]{r1}{4.25,0};
        \newAndNode[inputs=nn]{r2}{6.25,0};
        \newAndNode[inputs=nn]{p1}{$(r1)+(-0.5,-3)$};
        \newAndNode[inputs=nn]{p2}{$(r2)+(-0.5,-3)$};
        \newOrNode[inputs=nn]{l11}{$(p1.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l12}{$(p1.input 2)+(1,-1.5)$};
        \newOrNode[inputs=nn]{l21}{$(p2.input 1)+(0,-1.5)$};
        \newOrNode[inputs=nn]{l22}{$(p2.input 2)+(1,-1.5)$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s1}{$(r1.west) + (0,-1)$}{$\alpha$};
        \newNamedOrNode[draw=red,very thick,inputs=nn]{s2}{$(r2.west) + (0,-1)$}{$\alpha$};
        \newAndNode[draw=red,very thick,inputs=nn]{p3}{$(p1)+(1,0)$};
        \newAndNode[draw=red,very thick,inputs=nn]{p4}{$(p2)+(1,0)$};
        \draw (r1.west) -- ++(0,-0.25) -| (s1.east);
        \draw[draw=red,very thick] (r2.west) -- ++(0,-0.25) -| (s2.east);
        \draw (s1.input 1) -- ++(0,-0.5) -| (p1.east);
        \draw (s1.input 2) -- ++(0,-0.5) -| (p2.east);
        \draw[draw=red,very thick] (s2.input 1) -- ++(0,-0.25) -| (p3.east);
        \draw[draw=red,very thick] (s2.input 2) -- ++(0,-0.25) -| (p4.east);
        \draw (p1.input 1) -- ++(0,-0.25) -| (l11.east);
        \draw (p1.input 2) -- ++(0,-0.5) -| (l12.east);
        \draw (p2.input 1) -- ++(0,-0.25) -| (l21.east);
        \draw (p2.input 2) -- ++(0,-0.5) -| (l22.east);
        \draw[draw=red,very thick] (p3.input 1) -- ++(0,-0.25) -| (l11.east);
        \draw[draw=red,very thick] (p3.input 2) -- ++(0,-0.5) -| (l12.east);
        \draw[draw=red,very thick] (p4.input 1) -- ++(0,-0.25) -| (l21.east);
        \draw[draw=red,very thick] (p4.input 2) -- ++(0,-0.5) -| (l22.east);
      \end{scope}
    \end{tikzpicture}
    }
  \end{center}
  \caption{\textproc{Split} (left) and \textproc{Clone} (right) operations for growing a circuit
    when $m=1$. Nodes and edges highlighted in red show the modified structure. In both cases
  smoothness, (structure) decomposability and determinism are evidently preserved.}
  \label{fig:splitclone}
\end{figure}

It is easy to see that, in both cases, smoothness, structure decomposability and determinism are
preserved. In fact, if the original circuit encodes a particular support (i.e.\ a knowledge base),
the PC resulting from applying any of the two transformations must also encode the same support,
since we have only made the underlying logic circuit more redundant. Probabilistically though, this
``redudancy'' only increases the parameterization space and as such increases the expressiveness of
the PC. However, not all applications of \textproc{Split} or \textproc{Clone} are equal in terms of
performance. While it is true that the application of \textproc{Split} to any product node or
\textproc{Clone} to any sum node strictly increases expressivity, it is more meaningful to choose
candidates whose growth carries a bigger impact on the overall fit relative to the training data.
\textproc{LearnPSDD} searches for reasonable candidates by computing
\begin{equation}
  \score(\set{D}, \mathcal{C}, \mathcal{C}')=\frac{\log\mathcal{C}'(\set{D})-\log
  \mathcal{C}(\set{D})}{|\mathcal{C}'|-|\mathcal{C}|},
\end{equation}
where $\mathcal{C}$ and $\mathcal{C}'$ are, respectively, the PCs before and after the application
of any of the two operations. In other words, the algorithm randomly evaluates applying
\textproc{Split} and/or \textproc{Clone} and ultimately chooses the one candidate that maximizes
the log-likelihood of training data penalized by the size of the resulting PC, iteratively growing
the circuit until there is no more improvement or reaches an iteration step or time limit, as
\Cref{alg:learnpsdd} shows.

\subsubsection{Complexity}

Although learning the vtree top-down reduces to an NP-complete min-cut graph partitioning problem,
there are approximate algorithms that provide high quality partitionings in $\bigo(|\set{X}|^2)$
\citep{karypsis98}. Learning bottom-up is reduced to min-cost perfect matching, which can be done
in $\bigo(|\set{X}|^4)$ via the Edmonds Blossom algorithm \citep{edmonds65,kolmogorov09}.

\textproc{Split} runs, for a given variable $X$, in $\bigo(v\cdot|\mathcal{C}|)$ if unbounded
by $m$, where $v$ is $|\Val(X)|$, the number of possible assignments to $X$ if $X$ is discrete; or
the number of intervals to fragment $\Val(X)$ if $X$ is continuous. \textproc{Clone}'s runtime is
$\bigo(|\mathcal{C}|)$ when $m$ is unbounded, as it needs to produce an almost exact copy of
the circuit. We say that a local transformation, such as \textproc{Split} or \textproc{Clone}, is
\emph{minimal} when the copy depth is $m=0$. When \textproc{Split} and \textproc{Clone} are minimal
and $X$ is binary, then the transformation is done in constant time. In fact, any non-minimal
transformation can be composed out of minimal transformations \citep{liang17}.

Perhaps the most costly routine of \textproc{LearnPSDD} is its score function. Although
log-likelihood is linear time computable on the number of edges of the circuit, $\mathcal{C}$ can
grow substantially as transformations pile up. Each score evaluation requires four passes on the
circuit: log-likelihoods and circuit sizes for both $\mathcal{C}$ and its updated circuit
$\mathcal{C}'$. However, since transformations are local, log-likelihood and circuit sizes only
change for the nodes affected in the transformation and their ancestors, allowing
\textproc{LearnPSDD} to cache values. The overall complexity of \textproc{LearnPSDD} at each
iteration is therefore $\bigo(|\mathcal{C}|^2)$ if we assume $m=0$, with the first $|\mathcal{C}|$
coming from the search of all candidates in $\mathcal{C}$, and the second from the computation of
$\score$. Each iteration further increases $|\mathcal{C}|$, slowing down the algorithm's runtime.

\begin{algorithm}[t]
  \caption{\textproc{LearnPSDD}}\label{alg:learnpsdd}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, vtree $\vtree$, initial PC $\mathcal{C}$, max depth $m$, scope $\set{X}$
    \Ensure A smooth, structure decomposable and deterministic PC learned from $\set{D}$
    \While{there is score improvement or has not reached the iteration/time limit}
      \State $s_{\Sum}\gets -\infty$
      \State Let $(\Sum^\ast,\Prod^\ast)$ be the best \textproc{Split} candidate seen so far, initially empty
      \For{each candidate $(\Sum,\Prod)$ of all possible \textproc{Split} candidates}
        \State $\mathcal{C}'\gets\textproc{Split}(\mathcal{C},\Sum,\Prod,\vtree,m)$
        \State $s'\gets\score(\set{D},\mathcal{C},\mathcal{C}')$
        \IIf{$s'>s_{\Sum}$}{$s_{\Sum}\gets s'$ and $\Sum^\ast,\Prod^\ast\gets\Sum,\Prod$}
      \EndFor
      \State $s_{\Child}\gets -\infty$
      \State Let $\Child^\ast$ be the best \textproc{Clone} candidate seen so far, initially empty
      \For{each candidate $\Child$ of all possible \textproc{Clone} candidates}
        \State $\mathcal{C}'\gets\textproc{Clone}(\mathcal{C},\Child,\vtree,m)$
        \State $s'\gets\score(\set{D},\mathcal{C},\mathcal{C}')$
        \IIf{$s'>s_{\Child}$}{$s_{\Child}\gets s'$ and $\Child^\ast\gets\Child$}
      \EndFor
      \IIf{$s_{\Sum}>s_{\Child}$}{$\mathcal{C}\gets\textproc{Split}(\mathcal{C},\Sum^\ast,\Prod^\ast,\vtree,m)$}
      \IElse{$\mathcal{C}\gets\textproc{Clone}(\mathcal{C},\Child^\ast,\vtree,m)$}
    \EndWhile
    \State \textbf{return} $\mathcal{C}$
  \end{algorithmic}
\end{algorithm}

\subsubsection{Pros and Cons}

\paragraph{Pros.} The fact that \textproc{LearnPSDD} preserves smoothness, structural
decomposability, determinism \emph{and} any logical semantic coming from its underlying LC is
remarkable. On top of that, in theory and under minor modifications to \textproc{Split} and
\textproc{Clone}, any PC is eligible as an initial circuit, even ones which do not respect any
vtree. Besides, computing variable splits beforehand through a separate process of learning the
vtree relieves the learning algorithm from having to compute costly statistical tests at each
product node. Where \textproc{LearnPSDD} really shines (and perhaps more fittingly PSDDs in
general) is when the support is explicitly defined through the initial circuit's LC; because the PC
attributes non-zero probability only to events where the LC does not return false, the circuit
wastes no mass on impossible events.

\paragraph{Cons.} In practice, \textproc{LearnPSDD} is very slow even with caching; even worse, it
may take several hours for only a minor (if any) improvement. \citep{liang17} suggests improving
performance by producing ensembles of \textproc{LearnPSDD}s, although this negates determinism in
the final model (as well as structure decomposability if different vtrees are used at each
component), denying the access to tractably computing queries like divergences, $\mutualinf{}$ and
entropies, not to mention the time cost to learn all components. Another issue is with the choice
of the initial circuit. As previously mentioned, any circuit will do, however the performance (and
efficiency) of \textproc{LearnPSDD} is highly dependent on it. Within the context of PSDDs and
encoding their support, \textproc{LearnPSDD} requires that a separate algorithm compiles an LC for
a specific task without looking at data. Although there are many ways of doing so, they are often
not task agnostic (see \Cref{rem:initpc}). More importantly, because the process of learning the
circuit (from data) is decoupled from the task of encoding logical constraints imposed by a
knowledge base, all variables that do not appear in the logic formula are compiled into a trivial
form (e.g.\ fully factorized circuit). Lastly, although decoupling the process of learning the
vtree from learning the PC helps with scalability, the ability of identifying the proper vtree for
the most expressive PC given data is certainly desirable, and one which might be hindered by this
separated process.

\subsection{\textproc{Strudel}}\label{sec:strudel}

\citet{dang20} build upon the work of \textproc{LearnPSDD} and propose \textproc{Strudel}, which
mainly improves \textproc{LearnPSDD} on two fronts: (1) by providing a simple algorithm for
generating an initial circuit and vtree from data, and (2) proposing a heuristic for efficiently
searching for good transformation candidates.

We first address how to construct the initial circuit from data. \citeauthor{dang20} suggests doing
so by compiling both a vtree and linear sized PC (in the number of variables) from a Chow-Liu Tree
(CLT, \cite{chow68}). Let $\mathcal{T}$ be a CLT over variables $\set{X}=\{X_1,\ldots,X_m\}$. A vtree
$\vtree$ is extracted from $\mathcal{T}$ by traversing $\mathcal{T}$ top-down. For each node
$X_i\in\mathcal{T}$, if $X_i$ is a leaf node in $\mathcal{T}$, then create a vtree leaf node of
$X_i$; otherwise create an inner vtree node $v$, attach a vtree leaf node of $X_i$ as $v^\gets$ and
assign $v^\to$ as a vtree built over all the vtrees coming from the children of $X_i$. The
construction of $v^\gets$ depends on how balanced one wishes the vtree to be: if we want a more
right-leaning vtree, it suffices to construct a right-linear vtree connecting all vtrees from each
child $X_j\in\Ch(X_i)$. Likewise, a balanced vtree is built by balancing the vtree connecting the
recursive vtree calls from each $X_j$. Note that this does not necessarily mean that $v^\to$ is
completely right-linear or balanced, only that it is somewhat close to it, as the rest of the
structure depends on the recursive calls of each CLT node.

\begin{figure}[t]
  \begin{center}
    \resizebox{\textwidth}{!}{
    \begin{tikzpicture}
      \node[fill=boxgreen!80,draw,circle,inner sep=2pt,minimum size=13pt] (d) at (0,0) {$D$};
      \node[fill=boxgray!70,draw,circle,inner sep=2pt,minimum size=13pt] (c) at ($(d) + (0,-1.5)$) {$C$};
      \node[fill=boxred!70,draw,circle,inner sep=2pt,minimum size=13pt] (b) at ($(c) + (0.5,-1.5)$) {$B$};
      \node[fill=boxgoldenrod!70,draw,circle,inner sep=2pt,minimum size=13pt] (a) at ($(c) + (-0.5,-1.5)$) {$A$};
      \draw[edge] (d) -- (c); \draw[edge] (c) -- (b); \draw[edge] (c) -- (a);

      \node at ($(d) + (0, 1.0)$) {\scalebox{0.8}{\begin{tabular}{c}
            \hline
            $p(D=0)$\\
            \hline
            0.6\\
            \hline
        \end{tabular}}};
      \node at ($(c) + (-2, 0.0)$) {\scalebox{0.8}{\begin{tabular}{c|c}
            \hline
            $D$ & $p(C=0|D)$\\
            \hline
            0 & 0.2\\
            1 & 0.7\\
            \hline
        \end{tabular}}};
      \node at ($(b) + (1.0,-1.25)$) {\scalebox{0.8}{\begin{tabular}{c|c}
            \hline
            $C$ & $p(B=0|C)$\\
            \hline
            0 & 0.5\\
            1 & 0.1\\
            \hline
        \end{tabular}}};
      \node at ($(a) + (-1.0,-1.25)$) {\scalebox{0.8}{\begin{tabular}{c|c}
            \hline
            $C$ & $p(A=0|C)$\\
            \hline
            0 & 0.3\\
            1 & 0.6\\
            \hline
        \end{tabular}}};

      \newVtreeNode[fill=boxorange!80]{v}{$(d) + (3.5,0)$}{1};
      \newVtreeNode[fill=boxblue!50]{vr}{$(v) + (0.5,-1.25)$}{2};
      \newVtreeNode[fill=boxgreen!80]{vd}{$(v) + (-0.5,-1.25)$}{$D$};
      \newVtreeNode[fill=boxpink!50]{vrl}{$(vr) + (0.5,-1.25)$}{3};
      \newVtreeNode[fill=boxgray!70]{vc}{$(vr) + (-0.5,-1.25)$}{$C$};
      \newVtreeNode[fill=boxgoldenrod!70]{va}{$(vrl) + (-0.5,-1.25)$}{$A$};
      \newVtreeNode[fill=boxred!70]{vb}{$(vrl) + (0.5,-1.25)$}{$B$};
      \draw (v) -- (vr); \draw (v) -- (vd); \draw (vr) -- (vrl); \draw (vr) -- (vc);
      \draw (vrl) -- (va); \draw (vrl) -- (vb);

      \newSumNode{r}{$(d) + (9,2)$};
      \newProdNode[fill=boxorange!80]{p1}{$(r) + (-0.75,-1)$};
      \newProdNode[fill=boxorange!80]{p2}{$(r) + (0.75,-1)$};
      \node[fill=boxgreen!80,minimum size=17pt,label=center:{$D$}] (pd) at ($(p1) + (-1.25,-1)$) {};
      \node[fill=boxgreen!80,minimum size=17pt,label=center:{$\neg D$}] (pnd) at ($(p2) + (1.25,-1)$) {};
      \newSumNode{s1}{$(p1) + (0,-1)$};
      \newSumNode{s2}{$(p2) + (0,-1)$};
      \newProdNode[fill=boxblue!50]{q1}{$(s1) + (0,-1)$};
      \newProdNode[fill=boxblue!50]{q2}{$(s2) + (0,-1)$};
      \node[fill=boxgray!70,minimum size=17pt,label=center:{$C$}] (pc) at ($(q1) + (-1.25,-1)$) {};
      \node[fill=boxgray!70,minimum size=17pt,label=center:{$\neg C$}] (pnc) at ($(q2) + (1.25,-1)$) {};
      \newSumNode{z1}{$(q1) + (0,-1)$};
      \newSumNode{z2}{$(q2) + (0,-1)$};
      \newProdNode[fill=boxpink!50]{t1}{$(z1) + (0,-1)$};
      \newProdNode[fill=boxpink!50]{t2}{$(z2) + (0,-1)$};
      \newSumNode{w2}{$(t1) + (0,-1)$};
      \newSumNode{w3}{$(t2) + (0,-1)$};
      \newSumNode{w1}{$(w2) + (-1.5,0)$};
      \newSumNode{w4}{$(w3) + (1.5,0)$};
      \node[fill=boxgoldenrod!70,minimum size=17pt,label=center:{$A$}] (pa) at ($(w1) + (0,-1)$) {};
      \node[fill=boxgoldenrod!70,minimum size=17pt,label=center:{$\neg A$}] (pna) at ($(w2) + (0,-1)$) {};
      \node[fill=boxred!70,minimum size=17pt,label=center:{$B$}] (pb) at ($(w3) + (0,-1)$) {};
      \node[fill=boxred!70,minimum size=17pt,label=center:{$\neg B$}] (pnb) at ($(w4) + (0,-1)$) {};
      \draw[edge] (r) -- node[midway,left] {.4} (p1);
      \draw[edge,very thick,red] (r) -- node[midway,right] {.6} (p2);
      \draw[edge] (p1) -- (pd);
      \draw[edge] (p1) -- (s1);
      \draw[edge,very thick,red] (p2) -- (pnd);
      \draw[edge,very thick,red] (p2) -- (s2);
      \draw[edge] (s1) -- node[midway,left] {.3} (q1);
      \draw[edge] (s1) -- node[very near start,above,xshift=0.125cm,yshift=-0.1cm] {.7} (q2);
      \draw[edge,very thick,red] (s2) -- node[very near start,above,xshift=-0.25cm,yshift=-0.1cm] {.8} (q1);
      \draw[edge] (s2) -- node[midway,right] {.2} (q2);
      \draw[edge,very thick,red] (q1) -- (pc);
      \draw[edge,very thick,red] (q1) -- (z1);
      \draw[edge] (q2) -- (z2);
      \draw[edge] (q2) -- (pnc);
      \draw[edge,very thick,red] (z1) -- (t1);
      \draw[edge] (z2) -- (t2);
      \draw[edge,very thick,red] (t1) -- (w1);
      \draw[edge,very thick,red] (t1) -- (w3);
      \draw[edge] (t2) -- (w2);
      \draw[edge] (t2) -- (w4);
      \draw[edge,very thick,red] (w1) -- node[midway,left] {.4} (pa);
      \draw[edge] (w1) -- node[very near start,above,xshift=0.125cm,yshift=-0.1cm] {.6} (pna);
      \draw[edge] (w2) -- node[very near start,above,xshift=-0.125cm,yshift=-0.1cm] {.7} (pa);
      \draw[edge] (w2) -- node[midway,right] {.3} (pna);
      \draw[edge] (w3) -- node[midway,left] {.9} (pb);
      \draw[edge,very thick,red] (w3) -- node[very near start,above,xshift=0.125cm,yshift=-0.1cm] {.1} (pnb);
      \draw[edge] (w4) -- node[very near start,above,xshift=-0.125cm,yshift=-0.1cm] {.5} (pb);
      \draw[edge] (w4) -- node[midway,right] {.5} (pnb);
    \end{tikzpicture}
    }
  \end{center}
  \caption{A vtree (middle) and probabilistic circuit (right) compiled from a Chow-Liu Tree (left).
    Each conditional probability $p(Y|X)$ is encoded as a (deterministic) sum node where each of
    the two children sets $Y$ to 0 or 1. Colors in the CLT indicate the variables in the PC, while
    vtree inner node colors match with product nodes that respect them. Edges in red indicate the
    induced subcircuit activated on assignment $\{A=1,B=0,C=1,D=0\}$.}
  \label{fig:strudel}
\end{figure}

\textproc{Strudel} compiles an initial circuit by looking at the vtree bottom-up and caching
subcircuits. Let $v$ be a vtree
node and $Y\in\mathcal{T}$ a CLT node with conditional probability $p(Y|X)$, where $X$ is the
parent of $Y$. If $v$ is a leaf node in $\vtree$ and $v$'s variable is also a leaf node in
$\mathcal{T}$, two sum nodes $\Sum_0$ and $\Sum_1$ over literal nodes $\neg Y$ and $Y$ are created,
each with weights $w_{\Sum_0,\neg Y}=p(Y=0|X=0)$, $w_{\Sum_0,Y}=p(Y=1|X=0)$ and $w_{\Sum_1,\neg
Y}=p(Y=0|X=1)$, $w_{\Sum_1,Y}=p(Y=0|X=1)$. The two sum nodes connecting $B$ and $\neg B$ in the PC
shown on the right of \Cref{fig:strudel} show this exact case. The left sum node encodes $p(B|C=1)$
and the right one $p(B|C=0)$. These circuits are then cached by associating them with $v$. When $Y$
is not a leaf node in $\mathcal{T}$ but $v$ is, we simply return literal nodes. If $v$ is an inner
node, we must define a scope partition, splitting $\set{X}=\Sc(v^\gets)$ and
$\set{Y}=\Sc(v^\to)$ into product nodes $\Prod_1,\ldots,\Prod_k$, one for each value cached value
in $v$. Each prime is set to the cached circuits from $v^\gets$ and each sub the cached circuits
from $v^\to$. Finally, if two variables $X\in\set{X}$ and $Y\in\set{Y}$ are such that their parents
are the same variable, say $Z$, then $X$ and $Y$ are independent when $Z$ is given (because of a
divergent connection in $\mathcal{T}$) and thus cannot be merged together into a single sum because
of the context-specific independence set by $Z$ \citep{boutilier96}. This is visualized in the
\inode[fill=boxpink!50]{\newProdNode} nodes; in this situation, $A$ and $B$ are siblings coming
from $C$, and so $A\indep B\,|\,C$ (redundant sum nodes are added for standardization).
When the prior situation is not true, then not only $X$ is the only variable in $\set{X}$, but $X$
must also be the parent of $Y$ and so we must model $p(\set{Y}|X)$. This is the case for
\inode[fill=boxblue!50]{\newProdNode}, where $C$ is the parent of $B$ and so we have to be join the
two by sum nodes attributing the conditional probabilities $p(A,B|C=0)$ for the right-most
\inode[fill=boxblue!50]{\newProdNode} and $p(A,B|C=1)$ for the left-most sibling. This procedure is
shown more formally in \Cref{alg:strudelinit}.

\begin{algorithm}[t]
  \caption{\textproc{InitialStrudel}}\label{alg:strudelinit}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, whose columns are indexed by variables $\set{X}$
    \Ensure A smooth, structure decomposable and deterministic initial PC and vtree
    \State $\mathcal{T}\gets\textproc{LearnCLT}(\set{D},\set{X})$
    \State $\vtree\gets\textproc{CompileVtree}(\mathcal{T})$
    \State Let $\mathcal{M}$ be a hash table for caching circuits, initially empty
    \For{each vtree node $v\in\vtree$ in reverse topological order}
      \If{$v$ is a leaf node}
        \State Let $X\in\mathcal{T}$ be the variable represented by $v$, and $Y$ its parent
        \If{$X$ is a leaf node in $\mathcal{T}$}
          \State $\Sum_j\gets\sum_{i\in\Val(X)}p(X=i|Y=j)\cdot\liv X=i\riv$ for each $j\in\Val(Y)$
          \State $\mathcal{M}(v)\gets\mathcal{M}(v)\cup\{\Sum_j\,|\,\forall j\in\Val(Y)\}$
        \Else
          \State $\mathcal{M}(v)\gets\mathcal{M}(v)\cup\{\liv X=i\riv\,|\,i\in\Val(X)\}$
        \EndIf
      \Else
        \State Attribute $\set{X}\gets\Sc(v^\gets)$ and $\set{Y}\gets\Sc(v^\to)$
        \State Let $X\in\set{X}$ and $Y\in\set{Y}$ subsets of each scope
        \State Attribute $\Nodes^\gets\gets\mathcal{M}(v^\gets)$ and $\Nodes^\to\gets\mathcal{M}(v^\to)$
        \State $k\gets|\Val(X)|$
        \State Construct product nodes $\Prods=\{\Nodes_i^\gets\cdot\Nodes_i^\to\,|\,\forall i\in\left[k\right]\}$
        \If{$\Pa(X)=\Pa(Y)$}
          \State Create sum nodes $\Sum_i$ each with only a single child $\Prod_i\in\Prods$, for each $i\in\left[k\right]$
          \State $\mathcal{M}(v)\gets\mathcal{M}(v)\cup\{\Sum_1,\ldots,\Sum_k\}$
        \Else
          \State $\Sum_j\gets\sum_{i\in\Val(X)}p(\set{Y}|X=j)\cdot\liv X=i\riv$, for each $j\in\Val(Y)$
          \State $\mathcal{M}(v)\gets\mathcal{M}(v)\cup\{\Sum_j\,|\,\forall j\in\Val(Y)\}$
        \EndIf
      \EndIf
    \EndFor
    \State \textbf{return} $\mathcal{M}(v_r)$, where $v_r$ is $\vtree$'s root node
  \end{algorithmic}
\end{algorithm}

Now that we have an initial PC constructed from \textproc{InitialStrudel}, we are ready to discuss
\textproc{Strudel}'s second contribution. To do so, we must first understand the notion of
\emph{circuit flows} introduced in \citet{dang20}. In short, the circuit flow of a deterministic
probabilistic circuit $\mathcal{C}$ with respect to a variable assignment $\set{x}$ is the induced
tree (see \Cref{def:inducedsub}) whose edges are all non-zero when $\mathcal{C}$ is evaluated under
$\set{x}$. Such an induced tree is unique in deterministic PCs because every sum node admits only
one non-zero valued child for $\set{x}$ (or any assignment for that matter). Note how circuit flows
are more specific in the sense they are intrinsically linked to an assignment, while induced
subcircuits specify a deterministic subcircuit within its supercircuit.

The circuit flow of deterministic PCs helps us understand how to efficiently compute inference in
circuits of that nature. As we briefly mentioned before, for any assignment $\set{x}$ in a smooth,
decomposable and deterministic PC $\mathcal{C}$, there exists a unique circuit flow $\mathcal{F}$
that encodes the log-likelihood computation
\begin{equation}
  \mathcal{C}(\set{x})=\mathcal{F}_\mathcal{C}(\set{x})=\prod_{(\Sum,\Child)\in
  \FEdges(\mathcal{F}_\mathcal{C})}w_{\Sum,\Child}\prod_{\Leaf\in\FInputs(\mathcal{F}_\mathcal{C})}
  p_{\Leaf}(\set{x}),
\end{equation}
where $\FInputs(\cdot)$ returns the set of input nodes of a circuit. When inputs are all binary,
then one might encode $\mathcal{F}_\mathcal{C}$ as a mapping $f_\mathcal{C}:\mathcal{X}\to
\{0,1\}^{|\mathcal{W}_\mathcal{C}|}$, here $\mathcal{W}_\mathcal{C}$ denoting the set of all
parameters (i.e.\ sum node weights) of $\mathcal{C}$, which ``activates'' edge
$w\in\mathcal{W}_\mathcal{C}$ under assignment $\set{x}$. With this, the above operation under
log-space is reduced to a vector multiplication
\begin{equation}
  \log\mathcal{C}(\set{x})=f_\mathcal{C}(\set{x})^\intercal\cdot\log\left(\mathcal{W}_\mathcal{C}\right).
\end{equation}

Importantly, by aggregating circuit flows through counting the number of activations of each
parameter $w_{\Sum,\Child}$ in the entire training dataset $\set{D}$, we get a sense of the number
of samples $w_{\Sum,\Child}$ impacts over $\set{D}$, and thus a sense of how meaningful is that
edge on the fitness of data. As we shall see briefly, this aggregated circuit flow shall then be
used as a score for a greedy search over the space of candidates for local transformations.

To overcome the scalability limitations of \textproc{LearnPSDD}, \textproc{Strudel} proposes using
only \textproc{Split} to reduce the search space, looking at performing the search greedily instead
of exhaustively and exploiting the efficiency of aggregate circuit flows as a fast heuristic in
place of computing the whole likelihood. Searching is done by finding the edge to \textproc{Split}
whose aggregate circuit flow is maximal
\begin{equation}
  \score_{\textsf{eFlow}}(w_{\Sum,\Child}|\mathcal{C},\set{D})=\sum_{\set{x}\in\set{D}}f_{\mathcal{C}}(\set{x})\left[w_{\Sum,\Child}\right],
\end{equation}
while the choice of which variable to condition \textproc{Split} on is done by selecting the
variable $X$ that shares the most dependencies (and thus the higher pairwise mutual information)
with other variables within the scope of that edge, estimated from the aggregate flows
\begin{equation}
  \score_{\textsf{vMI}}(X,w_{\Sum,\Child}|\mathcal{C},\set{D})=\sum_{\substack{Y\in\Sc(\Sum)\\Y\neq
    X}}\mutualinf(X,Y).
\end{equation}
The entire algorithm for \textproc{Strudel} is showcased in \Cref{alg:strudel}.

\begin{algorithm}[t]
  \caption{\textproc{Strudel}}\label{alg:strudel}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, max depth $m$, scope $\set{X}$
    \Ensure A smooth, structure decomposable and deterministic PC learned from $\set{D}$
    \State $\mathcal{C},\vtree\gets\textproc{InitialStrudel}(\set{D},\set{X})$
    \While{there is score improvement of has not reached the iteration/time limit}
      \State Compute the aggregate flow over all edges
      \State $w_{\Sum,\Child}^\ast\gets\argmax_{w\in\mathcal{W}_\mathcal{C}}\score_{\textsf{eFlow}}(w|\mathcal{C},\set{D})$
      \State $X^\ast\gets\argmax_{X\in\Sc(\Sum)}\score_{\textsf{vMI}}(X,w_{\Sum,\Child}^\ast|\mathcal{C},\set{D})$
      \State $\mathcal{C}\gets\textproc{Split}(\mathcal{C},\Sum,\Child,\vtree,m)$
    \EndWhile
    \State \textbf{return} $\mathcal{C}$
  \end{algorithmic}
\end{algorithm}

\subsubsection{Complexity}

Learning the Chow-Liu Tree is done in $\bigo(|\set{X}|^2\cdot|\set{D}|)$ through Chow-Liu's
algorithm \citep{chow68}, while the vtree is compiled in time linear to the size of the CLT, i.e.\
$\bigo(|\set{X}|)$ since the Bayesian network is a tree. Consequentially, \textproc{InitialStrudel}
runs in $\bigo(|\set{X}|\cdot|\Val(X)|)$, or linear on $|\set{X}|$ if we assume binary variables as
originally intended. The bulk of the computation falls under \textproc{Strudel}, which runs in
$\bigo\left(|\set{X}|^2\cdot|\set{D}|+i(|\mathcal{C}|\cdot|\set{D}|+|\set{X}|^2)\right)$ assuming a
bounded max depth $m$ and binary variables. Term $|\set{X}|^2\cdot|\set{D}|$ corresponds to learning
the CLT, $|\mathcal{C}|\cdot|\set{D}|$ to the computation of the aggregate circuit flows,
$|\set{X}|^2$ to the computation of $\score_{\textsf{vMI}}$ which involves the pairwise mutual
informations of $\set{X}$, and $i$ the number of iterations of $\textproc{Strudel}$.

\subsubsection{Pros and Cons}

\paragraph{Pros.} Arguably, the most valuable contribution of \textproc{Strudel} is its improvement
on \textproc{LearnPSDD}'s scalability. Compared to \textproc{LearnPSDD}, \textproc{Strudel} can
take orders of magnitude less time per iteration, which in practice means a higher number of
transformations accomplished in the same range of time. In addition, the nature of circuit flows
allows for easy vectorization and thus CPU or GPU parallelization. In terms of data fitness,
\citet{dang20} empirically shows that initial circuits constructed from \textproc{StrudelInitial}
greatly improve performance compared to fully factorized initial PCs from \textproc{LearnPSDD}.
Similar to \textproc{LearnPSDD}, one can learn an ensemble of \textproc{Strudel}s to further boost
performance at the cost of losing determinism. Opposite to \textproc{LearnPSDD} however,
\citeauthor{dang20} employ structure-sharing components so that the act of learning the circuit's
structure is done once, greatly reducing learning time. Parameters are then learned through closed
form EM (see \Cref{rem:paramlearn}) and bagging.

\paragraph{Cons.} Although \textproc{Strudel}'s greedy heuristic search strategy translates into
possibly more accurate PCs, it also produces more sizable circuits when compared to the exhaustive
search of \textproc{LearnPSDD}. Indeed, \citet{dang20}'s empirical evaluation shows
\textproc{Strudel} PCs up to 12 times bigger than \textproc{LearnPSDD}'s with the two somewhat tied
in terms of fitness. This is especially worrying given that \textproc{Strudel}'s complexity grows
with its circuit size. In fact, experiments show a sharp increase in seconds per iterations for the
two \incrclass{} algorithms, with both reaching multiple digits for each iteration even in smaller
sized datasets; though \textproc{LearnPSDD} much sharper and sooner comparatively \citep{dang20}.

\begin{remark}[breakable]{On the choice of initial circuits}{initpc}
  We only briefly mentioned in \Cref{sec:learnpsdd} how we might want to start out with an
  initial PC conveying a specific support and then run an \incrclass{} class algorithm to further
  boost its probabilistic expressiveness without changing the underlying knowledge base. We devote
  this remark segment to discussing several works in literature that construct a so-called
  \emph{canonical} (i.e.\ minimal with respect to their size without sacrificing its logical
  semantics) logic circuit, becoming perfect candidates to be used as an initial circuit in
  \incrclass{} learners.

  Just like in probabilistic reasoning, the field of knowledge compilation and symbolic reasoning
  is often interested in finding succinct representations capable of tractably computing queries, a
  subject which we briefly touched in \Cref{sec:fromcertainty}. For this reason, smooth,
  structure decomposable and deterministic logic circuits, who usually go by the name of Sentential
  Decision Diagrams (SDDs, \cite{darwiche11}) have proven to be a useful tool in several
  applications \citep{vlasselaer14,vlasselaer15,lomuscio15,herrmann13}. Fortunately, both
  \textproc{LearnPSDD} and \textproc{Strudel} preserve all the necessary structural constraints for
  both logical \emph{and} probabilistic queries in (P)SDDs. With this in mind, we highlight
  compilation of SDDs in this short remark.

  For most cases, SDDs can be compiled directly from CNFs and DNFs. \citep{choi13} constructs SDDs
  bottom-up by first compiling C/DNF clauses and then combining smaller SDDs by either conjoining
  or disjoining them. In contrast, \citep{oztok15} presents a faster compilation process which
  recursively breaks down C/DNFs by decomposing the formula into components according to a vtree,
  and then combines them into an SDD.

  Although CNFs and DNFs are the most common form of encoding propositional knowledge bases, they
  struggle under specific logical constraints such as cardinality constraints
  \citep{nishino16,sinz05}. Interesting alternatives include BDDs (see \Cref{eg:bdd}) which are
  also widely used in formal methods and program verification, and for which efficient compilation
  from cardinality constraints are available \citep{een06}. Because BDDs are special case SDDs
  whose vtrees are always right-linear \citep{darwiche11,bova16}, their reduced representations
  \citep{bryant86} are natural initial circuit candidates (see \Cref{sec:samplepsdd}).

  We now cover some of the existing literature on producing task specific (P)SDDs. \citet{choi16}
  analyzes the feasibility of compiling LCs (and subsequently producing a PC by parameterizing
  disjunction edges) from tic-tac-toe game traces, and route planning within a city. Both involve
  exhaustively disjoining all permutations of valid conjoined configurations and compiling the
  resulting DNF through previously cited SDD compilers. \citet{choi17} further studies route
  planning by compiling them into SDDs, but analyze the feasibility of (P)SDDs in route planning in
  larger scale maps. \citet{choi15} explore (P)SDDs in preference learning and rankings, providing
  an algorithm for compiling an SDD from total or partial rankings. Similarly, \citet{shen17}
  investigates (P)SDDs in probabilistic cardinality constraint tasks, also known as subset
  selection or $n$-choose-$k$ models.
\end{remark}

\section{Random Learning}
\label{sec:random}

We now explore random approaches to constructing probabilistic circuits, which we classify as
\randclass{} class learning algorithms. Essentially, \randclass{} class circuits are constructed
either by a completely random procedure (\Cref{sec:ratspn}), or guided by data (\Cref{sec:xpcs}).
We first look at \textproc{RAT-SPN} \citep{peharz20a}, a connectionist PC structure learning
algorithm for randomly generating tensorized smooth and decomposable probabilistic circuits. We
then address \textproc{XPC} \citep{dimauro21}, a flexible algorithm capable of learning smooth and
decomposable PCs as well as structure decomposable and/or deterministic circuits through simple
modifications to their method.

\subsection{\textproc{RAT-SPN}}
\label{sec:ratspn}

A key ingredient to \textproc{RAT-SPN} \citep{peharz20a} is the concept of \emph{region graphs}.
First introduced in PC literature in \citet{dennis12}, region graphs are tensorized templates for
PC construction. Informally, a region graph is composed out of \emph{region nodes} and
\emph{partition nodes}; the former is a set of sum or input nodes, and the latter of products.
Regions can be thought of sets of computational units explaining the same interactions among
variables \citep{dennis12}, for instance semantically similar pixel regions in an image; while
partitions define independencies between these regions. Edges coming out of region (resp.\
partition) nodes must necessarily connect to a partition (resp.\ region) node.

\begin{definition}[Region graph]
  A \emph{region graph} is a rooted connected DAG whose nodes are either \emph{regions} or
  \emph{partitions}. Children of regions are partitions, and children of partitions are regions.
  The root is always a region node.
\end{definition}

Region graphs simplify the process of constructing PCs by ensuring that they are \emph{at the
least} smooth and decomposable. Call $\mathcal{G}$ a region graph; $\mathcal{G}$ is easily
translated to a PC by applying the procedure described in \Cref{alg:region2pc}. Every region is
compiled into a set of sums or inputs, fully connecting children; every partition into a set of
products, producing a distinct permutation of children. Evidently, the resulting PC is exponential
on $s$ and $l$, as products must ensure that they encode different permutations. To deal with this
blow-up, this number is often restricted to only two children per partition.

\begin{algorithm}[t]
  \caption{\textproc{CompileRegionGraph}}\label{alg:region2pc}
  \begin{algorithmic}[1]
    \Require A region graph $\mathcal{G}$, parameters $s$ for sums and $l$ for inputs
    \Ensure A smooth and decomposable probabilistic circuit
    \State Let $\mathcal{M}$ be a mapping of region nodes to PC nodes
    \For{each node $\Node$ in $\mathcal{G}$ except the root in reverse topological order}
      \If{$\Node$ is a region}
        \If{$\Node$ is a leaf node in $\mathcal{G}$}
          \State Construct $\Leaves=\{\Leaf_1,\ldots,\Leaf_l\}$ input nodes over variables $\Sc(\Node)$
          \State Associate $\Node$ with $\Leaves$
        \Else
          \State Construct $\Sums=\{\Sum_1,\ldots,\Sum_s\}$ sum nodes
          \For{each partition node $\Prod\in\Ch(\Node)$}
            \State Every sum in $\Sums$ connects with every product in $\mathcal{M}(\Prod)$
          \EndFor
        \EndIf
      \ElsIf{$\Node$ is a partition}
        \State Let $\Ch(\Node)=\{\textsf{R}_1,\ldots,\textsf{R}_k\}$ be regions and $q=\prod_{i=1}^k
          |\mathcal{M}(\textsf{R}_i)|$
        \State Construct $\Prods=\{\Prod_1,\ldots,\Prod_q\}$ product nodes
        \For{every product $\Prod\in\Prods$}
          \State Connect $\Prod$ with a distinct combination of sums in $\mathcal{M}(\textsf{R}_1),
          \ldots,\mathcal{M}(\textsf{R}_j)$
        \EndFor
      \EndIf
    \EndFor
    \State Construct a root node $\textsf{R}$
    \State Connect $\textsf{R}$ to all products in every child of $\mathcal{G}$'s root
    \State \textbf{return} $\textsf{R}$
  \end{algorithmic}
\end{algorithm}

\textproc{RAT-SPN} works by randomly generating a region graph in a top-down divide-and-conquer
approach similar to \textproc{LearnSPN}, except that the learned structure eventually produces
non-tree shaped circuits and the procedure is done \emph{completely} random (see
\Cref{alg:ratspn}). In fact, \citet{peharz20a} argue that the parameterization of the circuit by
means of sum weights is as important as its structure, looking at probabilistic circuits as a
specific subclass of neural networks. Indeed, they show that this connectionist approach heavily
inspired by traditional deep learning produces very competitive PCs. However, to do so requires
extensive optimization of the circuit's weights, which unsurprisingly is where \textproc{RAT-SPN}
shines: because of the tensorized nature of region graphs, the resulting PC is able to exploit the
advantages of known deep learning frameworks, making the most of efficient stochastic gradient
descent optimizers and GPU parallelization.

\begin{algorithm}[t]
  \caption{\textproc{RAT-SPN}}\label{alg:ratspn}
  \begin{algorithmic}[1]
    \Require Data $\set{D}$, variables $\set{X}$, max depth $d$, $r$ \# subcircuits, $s$ \# sums,
      and $l$ \# inputs
    \Ensure A smooth and decomposable probabilistic circuit
    \Function{CreateLayer}{$\textsf{R}$, $d$, $\set{X}$}
      \State Assign $\set{X}$ as $\Sc(\textsf{R})$
      \State Sample a variable split $(\set{Y},\set{Z})$ from $\set{X}$
      \State Create a partition $\Prod$ and add it as a child of $\textsf{R}$
      \If{d>1}
        \If{$|\set{Y}|>1$}
          \State Create a region $\textsf{R}_1$
          \State \Call{CreateLayer}{$\textsf{R}_1$, $d-1$, $\set{Y}$}
        \EndIf
        \If{$|\set{Z}|>1$}
          \State Create a region $\textsf{R}_2$
          \State \Call{CreateLayer}{$\textsf{R}_2$, $d-1$, $\set{Z}$}
        \EndIf
      \EndIf
    \EndFunction
    \State Start with a root region node $\textsf{R}$
    \For{each $i\in\left[r\right]$}
      \State \Call{CreateLayer}{$\textsf{R}$, $d$, $\set{X}$}
    \EndFor
    \State $\mathcal{C}\gets\textproc{CompileRegionGraph}(\textsf{R},s,l)$
    \State \textbf{return} $\mathcal{C}$
  \end{algorithmic}
\end{algorithm}

To ensure that the compiled PC is smooth and decomposable, the region graph must also be so. We
extend the definition of scope function to region graphs. As long as leaf region nodes are assigned
the correct scope, the PC is by construction smooth (every region is fully connected to their
children) and decomposable (every partition splits variables into two nonoverlapping regions).
Function \textproc{CreateLayer} in \Cref{alg:ratspn} does exactly that, making sure each partition
decomposes into two distinct variable splits down to leaf region nodes. How deep the region graph
(and consequentially the resulting PC) goes depends on a parameter $d$, which corresponds to half
the true depth, as each \textproc{CreateLayer} call produces a partition and their children. After
the region graph is randomly buily, a PC is then constructed through \textproc{CompileRegionGraph},
passing the random region graph and number of nodes per region as parameters. The function
ultimately produces a dense probabilistic circuit from the region graph blueprint, as
\Cref{fig:ratspn} exemplifies.

\begin{figure}[t]
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    \draw[very thick,dashed,boxgreen] (-7.5,-0.75) -- (7.5,-0.75);
    \draw[very thick,dashed,boxred] (-7.5,-2.25) -- (7.5,-2.25);
    \draw[very thick,dashed,boxpurple] (-7.5,-3.75) -- (7.5,-3.75);
    \draw[very thick,dashed,boxblue] (-7.5,-5.25) -- (7.5,-5.25);
    \draw[very thick,dashed,boxorange] (-7.5,-6.75) -- (7.5,-6.75);

    \draw[draw=boxteal,very thick,fill=boxteal!30] (-1.25,-0.5) rectangle (1.25,0.5);
    \newSumNode[fill=boxgreen]{r}{0,0};
    \node at ($(r) + (0,1)$) {$\{A,B,C,D,E,F,G\}$};

    \node (p) at ($(r) + (-4,-1.5)$) {};
    \draw[very thick,fill=boxpink!30,draw=boxpink!70] ($(-1.25,-0.5) + (p)$) rectangle ($(1.25,0.5) + (p)$);
    \newProdNode[fill=boxred!70]{p1}{$(p) + (-0.75,0)$};
    \newProdNode[fill=boxred!70]{p2}{$(p) + (-0.25,0)$};
    \newProdNode[fill=boxred!70]{p3}{$(p) + (0.25,0)$};
    \newProdNode[fill=boxred!70]{p4}{$(p) + (0.75,0)$};

    \draw[edge] (r) -- (p1.north); \draw[edge] (r) -- (p2.north);
    \draw[edge] (r) -- (p3.north); \draw[edge] (r) -- (p4.north);

    \node (q) at ($(r) + (4,-1.5)$) {};
    \draw[very thick,fill=boxpink!30,draw=boxpink!70] ($(q) + (-1.25,-0.5)$) rectangle ($(q) + (1.25,0.5)$);
    \newProdNode[fill=boxred!70]{q1}{$(q) + (-0.75,0)$};
    \newProdNode[fill=boxred!70]{q2}{$(q) + (-0.25,0)$};
    \newProdNode[fill=boxred!70]{q3}{$(q) + (0.25,0)$};
    \newProdNode[fill=boxred!70]{q4}{$(q) + (0.75,0)$};

    \draw[edge] (r) -- (q1.north); \draw[edge] (r) -- (q2.north);
    \draw[edge] (r) -- (q3.north); \draw[edge] (r) -- (q4.north);

    \node (s1) at ($(p) + (-2,-1.5)$) {};
    \draw[draw=boxteal,very thick,fill=boxteal!30] ($(s1) + (-1.25,-0.5)$) rectangle ($(s1) + (1.25,0.5)$);
    \newSumNode[fill=boxpurple!60]{s11}{$(s1) + (-0.5,0)$};
    \newSumNode[fill=boxpurple!60]{s12}{$(s1) + (0.5,0)$};

    \node (s2) at ($(p) + (2,-1.5)$) {};
    \draw[draw=boxteal,very thick,fill=boxteal!30] ($(s2) + (-1.25,-0.5)$) rectangle ($(s2) + (1.25,0.5)$);
    \newSumNode[fill=boxpurple!60]{s21}{$(s2) + (-0.5,0)$};
    \newSumNode[fill=boxpurple!60]{s22}{$(s2) + (0.5,0)$};

    \draw[edge] (p1) -- (s11.north); \draw[edge] (p1) -- (s21.north);
    \draw[edge] (p2) -- (s11.north); \draw[edge] (p2) -- (s22.north);
    \draw[edge] (p3) -- (s12.north); \draw[edge] (p3) -- (s21.north);
    \draw[edge] (p4) -- (s12.north); \draw[edge] (p4) -- (s22.north);

    \node at ($(s1) + (-0.5,1)$) {$\{A,D,F\}$};
    \node at ($(s2) + (0.5,1)$) {$\{B,C,E,G\}$};

    \node (z1) at ($(q) + (-2,-1.5)$) {};
    \draw[draw=boxteal,very thick,fill=boxteal!30] ($(z1) + (-1.25,-0.5)$) rectangle ($(z1) + (1.25,0.5)$);
    \newSumNode[fill=boxpurple!60]{z11}{$(z1) + (-0.5,0)$};
    \newSumNode[fill=boxpurple!60]{z12}{$(z1) + (0.5,0)$};

    \node (z2) at ($(q) + (2,-1.5)$) {};
    \draw[draw=boxteal,very thick,fill=boxteal!30] ($(z2) + (-1.25,-0.5)$) rectangle ($(z2) + (1.25,0.5)$);
    \newSumNode[fill=boxpurple!60]{z21}{$(z2) + (-0.5,0)$};
    \newSumNode[fill=boxpurple!60]{z22}{$(z2) + (0.5,0)$};

    \node at ($(z1) + (-0.5,1)$) {$\{B,D,F,G\}$};
    \node at ($(z2) + (0.5,1)$) {$\{A,C,E\}$};

    \draw[edge] (q1) -- (z11.north); \draw[edge] (q1) -- (z21.north);
    \draw[edge] (q2) -- (z11.north); \draw[edge] (q2) -- (z22.north);
    \draw[edge] (q3) -- (z12.north); \draw[edge] (q3) -- (z21.north);
    \draw[edge] (q4) -- (z12.north); \draw[edge] (q4) -- (z22.north);

    \node (t1) at ($(s1) + (0,-1.5)$) {};
    \draw[very thick,fill=boxpink!30,draw=boxpink!70] ($(t1) + (-1.25,-0.5)$) rectangle ($(t1) + (1.25,0.5)$);
    \newProdNode[fill=boxblue!50]{t11}{$(t1) + (-0.75,0)$};
    \newProdNode[fill=boxblue!50]{t12}{$(t1) + (-0.25,0)$};
    \newProdNode[fill=boxblue!50]{t13}{$(t1) + (0.25,0)$};
    \newProdNode[fill=boxblue!50]{t14}{$(t1) + (0.75,0)$};

    \draw[edge] (s11) -- (t11.north); \draw[edge] (s11) -- (t12.north);
    \draw[edge] (s11) -- (t13.north); \draw[edge] (s11) -- (t14.north);

    \draw[edge] (s12) -- (t11.north); \draw[edge] (s12) -- (t12.north);
    \draw[edge] (s12) -- (t13.north); \draw[edge] (s12) -- (t14.north);

    \node (t2) at ($(s2) + (0,-1.5)$) {};
    \draw[very thick,fill=boxpink!30,draw=boxpink!70] ($(t2) + (-1.25,-0.5)$) rectangle ($(t2) + (1.25,0.5)$);
    \newProdNode[fill=boxblue!50]{t21}{$(t2) + (-0.75,0)$};
    \newProdNode[fill=boxblue!50]{t22}{$(t2) + (-0.25,0)$};
    \newProdNode[fill=boxblue!50]{t23}{$(t2) + (0.25,0)$};
    \newProdNode[fill=boxblue!50]{t24}{$(t2) + (0.75,0)$};

    \draw[edge] (s21) -- (t21.north); \draw[edge] (s21) -- (t22.north);
    \draw[edge] (s21) -- (t23.north); \draw[edge] (s21) -- (t24.north);

    \draw[edge] (s22) -- (t21.north); \draw[edge] (s22) -- (t22.north);
    \draw[edge] (s22) -- (t23.north); \draw[edge] (s22) -- (t24.north);

    \node (u1) at ($(z1) + (0,-1.5)$) {};
    \draw[very thick,fill=boxpink!30,draw=boxpink!70] ($(u1) + (-1.25,-0.5)$) rectangle ($(u1) + (1.25,0.5)$);
    \newProdNode[fill=boxblue!50]{u11}{$(u1) + (-0.75,0)$};
    \newProdNode[fill=boxblue!50]{u12}{$(u1) + (-0.25,0)$};
    \newProdNode[fill=boxblue!50]{u13}{$(u1) + (0.25,0)$};
    \newProdNode[fill=boxblue!50]{u14}{$(u1) + (0.75,0)$};

    \node (u2) at ($(z2) + (0,-1.5)$) {};
    \draw[very thick,fill=boxpink!30,draw=boxpink!70] ($(u2) + (-1.25,-0.5)$) rectangle ($(u2) + (1.25,0.5)$);
    \newProdNode[fill=boxblue!50]{u21}{$(u2) + (-0.75,0)$};
    \newProdNode[fill=boxblue!50]{u22}{$(u2) + (-0.25,0)$};
    \newProdNode[fill=boxblue!50]{u23}{$(u2) + (0.25,0)$};
    \newProdNode[fill=boxblue!50]{u24}{$(u2) + (0.75,0)$};

    \draw[edge] (z11) -- (u11.north); \draw[edge] (z11) -- (u12.north);
    \draw[edge] (z11) -- (u13.north); \draw[edge] (z11) -- (u14.north);

    \draw[edge] (z12) -- (u11.north); \draw[edge] (z12) -- (u12.north);
    \draw[edge] (z12) -- (u13.north); \draw[edge] (z12) -- (u14.north);

    \draw[edge] (z21) -- (u21.north); \draw[edge] (z21) -- (u22.north);
    \draw[edge] (z21) -- (u23.north); \draw[edge] (z21) -- (u24.north);

    \draw[edge] (z22) -- (u21.north); \draw[edge] (z22) -- (u22.north);
    \draw[edge] (z22) -- (u23.north); \draw[edge] (z22) -- (u24.north);

    \node (l1) at ($(u1) + (-0.75,-1.5)$) {};
    \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(l1) + (-0.65,-0.5)$) rectangle ($(l1) + (0.65,0.5)$);
    \newGaussNode[fill=boxorange!80]{l11}{$(l1) + (-0.25,0)$};
    \newGaussNode[fill=boxorange!80]{l12}{$(l1) + (0.25,0)$};

    \node (l2) at ($(u1) + (0.75,-1.5)$) {};
    \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(l2) + (-0.65,-0.5)$) rectangle ($(l2) + (0.65,0.5)$);
    \newGaussNode[fill=boxorange!80]{l21}{$(l2) + (-0.25,0)$};
    \newGaussNode[fill=boxorange!80]{l22}{$(l2) + (0.25,0)$};

    \draw[edge] (u11) -- (l11.north); \draw[edge] (u11) -- (l21.north);
    \draw[edge] (u12) -- (l11.north); \draw[edge] (u12) -- (l22.north);
    \draw[edge] (u13) -- (l12.north); \draw[edge] (u13) -- (l21.north);
    \draw[edge] (u14) -- (l12.north); \draw[edge] (u14) -- (l22.north);

    \node (l3) at ($(u2) + (-0.75,-1.5)$) {};
    \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(l3) + (-0.65,-0.5)$) rectangle ($(l3) + (0.65,0.5)$);
    \newGaussNode[fill=boxorange!80]{l31}{$(l3) + (-0.25,0)$};
    \newGaussNode[fill=boxorange!80]{l32}{$(l3) + (0.25,0)$};

    \node (l4) at ($(u2) + (0.75,-1.5)$) {};
    \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(l4) + (-0.65,-0.5)$) rectangle ($(l4) + (0.65,0.5)$);
    \newGaussNode[fill=boxorange!80]{l41}{$(l4) + (-0.25,0)$};
    \newGaussNode[fill=boxorange!80]{l42}{$(l4) + (0.25,0)$};

    \draw[edge] (u21) -- (l31.north); \draw[edge] (u21) -- (l41.north);
    \draw[edge] (u22) -- (l31.north); \draw[edge] (u22) -- (l42.north);
    \draw[edge] (u23) -- (l32.north); \draw[edge] (u23) -- (l41.north);
    \draw[edge] (u24) -- (l32.north); \draw[edge] (u24) -- (l42.north);

    \node at ($(l3) + (0,-1)$) {$\{A,C\}$};
    \node at ($(l4) + (0,-1)$) {$\{E\}$};
    \node at ($(l1) + (0,-1)$) {$\{B,D\}$};
    \node at ($(l2) + (0,-1)$) {$\{F,G\}$};

    \node (l1) at ($(t1) + (-0.75,-1.5)$) {};
    \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(l1) + (-0.65,-0.5)$) rectangle ($(l1) + (0.65,0.5)$);
    \newGaussNode[fill=boxorange!80]{l11}{$(l1) + (-0.25,0)$};
    \newGaussNode[fill=boxorange!80]{l12}{$(l1) + (0.25,0)$};

    \node (l2) at ($(t1) + (0.75,-1.5)$) {};
    \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(l2) + (-0.65,-0.5)$) rectangle ($(l2) + (0.65,0.5)$);
    \newGaussNode[fill=boxorange!80]{l21}{$(l2) + (-0.25,0)$};
    \newGaussNode[fill=boxorange!80]{l22}{$(l2) + (0.25,0)$};

    \draw[edge] (t11) -- (l11.north); \draw[edge] (t11) -- (l21.north);
    \draw[edge] (t12) -- (l11.north); \draw[edge] (t12) -- (l22.north);
    \draw[edge] (t13) -- (l12.north); \draw[edge] (t13) -- (l21.north);
    \draw[edge] (t14) -- (l12.north); \draw[edge] (t14) -- (l22.north);

    \node (l3) at ($(t2) + (-0.75,-1.5)$) {};
    \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(l3) + (-0.65,-0.5)$) rectangle ($(l3) + (0.65,0.5)$);
    \newGaussNode[fill=boxorange!80]{l31}{$(l3) + (-0.25,0)$};
    \newGaussNode[fill=boxorange!80]{l32}{$(l3) + (0.25,0)$};

    \node (l4) at ($(t2) + (0.75,-1.5)$) {};
    \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(l4) + (-0.65,-0.5)$) rectangle ($(l4) + (0.65,0.5)$);
    \newGaussNode[fill=boxorange!80]{l41}{$(l4) + (-0.25,0)$};
    \newGaussNode[fill=boxorange!80]{l42}{$(l4) + (0.25,0)$};

    \draw[edge] (t21) -- (l31.north); \draw[edge] (t21) -- (l41.north);
    \draw[edge] (t22) -- (l31.north); \draw[edge] (t22) -- (l42.north);
    \draw[edge] (t23) -- (l32.north); \draw[edge] (t23) -- (l41.north);
    \draw[edge] (t24) -- (l32.north); \draw[edge] (t24) -- (l42.north);

    \node at ($(l3) + (0,-1)$) {$\{B,E\}$};
    \node at ($(l4) + (0,-1)$) {$\{C,G\}$};
    \node at ($(l1) + (0,-1)$) {$\{A,F\}$};
    \node at ($(l2) + (0,-1)$) {$\{D\}$};
  \end{tikzpicture}
  }
  \caption{A \textproc{RAT-SPN} generated from parameters $d=3$, $r=2$, $s=2$ and $l=2$. Nodes
    within a \protect\tikz\protect\draw[draw=boxteal,very thick,fill=boxteal!30] (0,0) rectangle
    (0.5,-0.25); belong to inner region nodes, \protect\tikz\protect\draw[draw=boxpink!70,very
    thick,fill=boxpink!30] (0,0) rectangle (0.5,-0.25); to partitions and
    \protect\tikz\protect\draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] (0,0) rectangle
    (0.5,-0.25); to leaf regions; dashed lines (and node colors) indicate different PC layers.
    Scope of region nodes are shown in curly braces.}
  \label{fig:ratspn}
\end{figure}

Once the PC structure is successfully generated, \citet{peharz20a} suggest Expectation-Maximization
(EM, \cite{dempster77,peharz16,zhao16a}) for optimizing the circuit parameters (i.e.\ sum weights
and input node distributions). Although parameter learning of PCs is not the focus of this
dissertation, we briefly touch the subject in \Cref{rem:paramlearn}.

Worthy of note is a discriminative version of \textproc{RAT-SPN} where instead of a single root sum
node, $k$ roots are learned, each connecting to every \textproc{CreateLayer} subcircuit. Each
$i$-th root describes the conditional probability $p(\set{X}|Y=i)$, where $Y$ is the query variable
and $\set{X}$ evidence. Classification follows directly from Bayes Rule $p(Y|\set{X})=
\frac{p(\set{X}|Y)p(Y)}{\sum_{i=1}^k p(\set{X}|Y=i)p(Y=i)}$, where $p(Y)$ is either estimated from
the training data or assumed to be fixed. Accordingly, a discriminative objective function is
proposed involving cross-entropy and log-likelihood for hybrid generative-discriminative
optimization \citep{bouchard04} instead of running EM.

\subsubsection{Complexity}

Although the procedure described in \Cref{alg:ratspn} is $\bigo\left(r\cdot d(s+l)\right)$ if
$d<|\set{X}|$ and $\bigo\left(r\cdot\log_2 |\set{X}|(s+l)\right)$ otherwise, making the algorithm
extremely fast, it does not paint the whole picture. The main bulk of the complexity when learning
\textproc{RAT-SPN} comes from parameter learning. \citet{peharz20a} calculate the number of sum
weights to be
\begin{equation}
  |\mathcal{W}_\mathcal{C}|=\begin{cases}
    r\cdot k\cdot l^2 & \quad\text{if $d=1$,}\\
    r\cdot\left(k\cdot s^2+(2^{d-1}-2)s^3+2^{d-1}\cdot s\cdot l^2\right) & \quad\text{if $d>1$};
  \end{cases}
\end{equation}
if we assume that the number of children of partitions is at most two. This means that learning
only the non-input parameters of \textproc{RAT-SPN} takes time $\bigo\left( \left(r\cdot 2^d
\cdot\left(s^3+s\cdot l^2\right)+r\cdot k\cdot s^2\right)\cdot|\set{D}|\right)$. However, given
that most structure learning algorithms covered in this dissertation also require parameter
learning, one might argue that the true cost of structure learning in \textproc{RAT-SPN} is indeed
subquadratic.

\subsubsection{Pros and Cons}

\paragraph{Pros.} As expected from \randclass{} algorithms, the random, data-blind nature of
\textproc{RAT-SPN} makes for a very fast structure learning algorithm. More importantly, because
the structure is expected to have somewhat uniform layers with a fixed number of computational
units in each, the computations from parameter optimization can easily be brought to the GPU. This
not only helps with scalability in terms of speed, but also brings all the advantages of deep
learning frameworks to the table via well-studied stochastic gradient descent optimizers and
diagnostic tools.

\paragraph{Cons.} Clearly, \textproc{RAT-SPN} is \emph{completely} random with its structure
generation. Particularly, variable splits are done randomly, disregarding the independencies
encoded by data, meaning that certain factorizations may be assumed to be true when they would
otherwise not be. Although \textproc{RAT-SPN}s are certainly competitive against other learning
algorithms for PCs, they only produce smooth and decomposable circuits, denying the access to more
complex queries.

\begin{remark}[breakable]{On parameter learning of probabilistic circuits}{paramlearn}
  Literature in probabilistic circuits often divides learning into two (often distinct) tasks:
  structure learning and parameter learning. Although in this dissertation we almost exclusively
  cover \emph{structure} learning algorithms, it is worth also going through (even if
  superficially) some of the works on parameter optimization in PCs, as most structure learning
  algorithms assume this as a post-processing procedure.

  Expectation-Maximization (EM, \cite{dempster77}) is perhaps the most common maximum likelihood
  estimation (MLE) optimization procedure for probabilistic circuits. \citet{poon11},
  \citet{peharz15} and \citet{zhao16a} derived EM for generative learning in PCs, while
  \citet{rashwan18a} formulated a discriminative EM version for PCs through Extended Baum-Welch
  \citep{gopalakrishnan91}.

  Notably, when a circuit is smooth, decomposable and deterministic, MLE can be easily computed
  through closed-form by counting \citep{kisa14,peharz14}. Indeed, this is an attractive feature
  that extends to discriminative PCs \citep{liang19}.

  Following other more traditional deep learning models, PCs learned with stochastic gradient
  descent (SGD) have also appeared in literature, especially under convolutional and tensorial
  extensions \citep{sharir18,peharz20a,peharz20b,gens12}.

  Bayesian approaches have also received some attention by the PC community. \citet{jaini16} and
  \citet{rashwan16} developed online Bayesian moment matching algorithms to learn from streaming
  data; \citet{zhao16b} showed a variational optimization procedure that leverages inference
  tractability in PCs to efficiently compute the ELBO; \citet{trapp19} propose learning both
  structure and parameters by Gibbs sampling from a generative model on a (restricted) space of
  PCs; finally \citet{vergari19} propose PCs for automatic Bayesian density analysis.
\end{remark}

\subsection{\textproc{XPC}}
\label{sec:xpcs}

While \textproc{RAT-SPN} produces a data-blind PC architecture and then relies on parameter
optimization to learn from data, the algorithm that we shall see next does the exact opposite:
\textproc{XPC} \citep{dimauro21} randomly samples a structure from data and requires no parameter
learning. To do this, it restricts the circuit sampling space to a particular class of PCs whose
primes are logical restrictions and inputs are CLTs. By assigning a fixed number $k+1$ of (product)
children per sum node and assuming that the $k$ first primes are (random) conjunctions of literals
of a fixed length $t$, with the last $k+1$ prime their negation, the resulting PC is naturally
deterministic, as CLTs are themselves deterministic. In more practical terms, both the conjunctions
of literals as well as the $(k+1)$-th prime derived from the negation of the first $k$ conjunctions
are translated into products of (degenerate) Bernoullis. Determinism can be relaxed by applying any
form of regularization, for instance Laplace smoothing, both on CLTs and on the products of
Bernoullis.

Following the footsteps of \textproc{RAT-SPN}, they generate a tree-shaped random region graph and
produce a PC from it. Despite both employing region graphs, the graph in \citet{dimauro21} is only
used as an artifice for formalizing the structure construction: their process for reconstructing a
PC from a region graph boils down to replacing an inner region with a single sum, a leaf region
with a single input and a partition with a single product. Although one \emph{could} generate a
non-tree shaped PC by setting $s>1$ and $l>1$ (i.e.\ number of sums and inputs per inner and leaf
region respectively), the resulting circuit could be reduced to a tree, since both sums (and
inputs) coming from the same region would be syntatically the same\footnote{Whether this
``expanded'' circuit could have its performance improved if one were to run, say EM, to exploit
this increase in capacity is an interesting question that unfortunately was left unexplored in
\citet{dimauro21}.}.

A critical step to efficiently ensuring consistency with the logical restrictions is to assign only
consistent subsets of data to subcircuits. Just like in \divclass{} class algorithms, partition
(i.e.\ product) nodes in the region graph define column splits over data and regions (i.e.\ sums
and inputs) correspond to row splits. The algorithm then associates a node with a portion of data
according to its scope and logical constraint. More specifically, when the $i$-th prime defines a
conjunction of literals $\alpha_i$, only assignments $\set{x}$ whose application
$\alpha_i(\set{x})$ are true are transferred down, effectively splitting data row-wise. To ensure
that the resulting circuit is also decomposable, only $\Sc(\alpha_i)$ variables are passed to
primes, with the remaining variables going to the subs. This way, sum weights can be estimated as
the subdata size ratios just like in \textproc{LearnSPN}, as \Cref{fig:xpcs} shows. Each subdata
$\set{D}_{\alpha_i}$ corresponds to joining all assignments consistent with $\alpha_i$, as shown
through matching colors on the table on the right. Data is divided by rows according to the
satisfiability of each prime $\alpha_i$ and by column according to the scope of each prime.

At each iteration of \Cref{alg:xpcs}, sub leaf region nodes, denoted here as $\mathcal{S}$ regions,
are randomly selected for further expansion. \Cref{alg:xpcs-expand} takes a candidate region
$\Region$ and samples $k$ conjunctions $\alpha_1,\ldots,\alpha_k$ of length $t$ that appear at
least $\delta$ times in the subdata associated with $\Region$. If no such constraint has been
found, $\Region$ is discarded as a candidate for expansion. Otherwise, another layer of partitions
and regions similar to \Cref{fig:xpcs} is constructed, making sure that data splits obey both scope
and constraints $\alpha_i$. Once no more candidates are available or a limit on the number of
expansions has been reached, the region graph is translated into a probabilistic circuit by
replacing leaf region nodes of type $\mathcal{Q}$ into products of Bernoullis, type $\mathcal{S}$'s
into CLTs and inner regions as sums whose weights are the proportions of (row-wise) split data;
partitions are replaced with product nodes.

\begin{figure}[t]
  \newcolumntype{x}{>{\columncolor{boxolive}}c}
  \newcolumntype{y}{>{\columncolor{boxmunsel}}c}
  \begin{tikzpicture}
    \newSumNode[fill=boxgreen]{r}{0,0};
    \newProdNode[fill=boxred]{p1}{$(r) + (-2,-1.5)$};
    \newProdNode[fill=boxwheat]{p2}{$(r) + (0,-1.5)$};
    \newProdNode[fill=boxblue!50]{p3}{$(r) + (2,-1.5)$};
    \draw[edge] (r) -- node[midway,above left] {$\frac{|\set{D}_{\alpha_1}|}{|\set{D}|}$} (p1.north);
    \draw[edge] (r) -- node[midway,right] {$\frac{|\set{D}_{\alpha_2}|}{|\set{D}|}$} (p2.north);
    \draw[edge] (r) -- node[midway,above right] {$\frac{|\set{D}_{\alpha_3}|}{|\set{D}|}$} (p3.north);
    \begin{scope}[on background layer]
      \draw[draw=boxteal,very thick,fill=boxteal!30] ($(r) + (-0.5,-0.4)$) rectangle ($(r) + (0.5,0.4)$);
      \draw[draw=boxpink!70,very thick,fill=boxpink!30] ($(p1) + (-0.5,-0.4)$) rectangle ($(p1) + (0.5,0.4)$);
      \draw[draw=boxpink!70,very thick,fill=boxpink!30] ($(p2) + (-0.5,-0.4)$) rectangle ($(p2) + (0.5,0.4)$);
      \draw[draw=boxpink!70,very thick,fill=boxpink!30] ($(p3) + (-0.5,-0.4)$) rectangle ($(p3) + (0.5,0.4)$);
    \end{scope}

    \node[] (l1) at ($(p1) + (-1.3,-1.5)$) {\scriptsize$A\wedge B$};
    \newGaussNode[fill=boxorange!80]{c1}{$(p1) + (0,-1.5)$};
    \node (l2) at ($(p2) + (-0.67,-1.5)$) {\scriptsize$A\wedge\neg B$};
    \newGaussNode[fill=boxorange!80]{c2}{$(p2) + (0.67,-1.5)$};
    \node (l3) at ($(p3) + (0,-1.5)$) {\scriptsize$\neg\bigvee_i\alpha_i$};
    \newGaussNode[fill=boxorange!80]{c3}{$(p3) + (1.3,-1.5)$};
    \node[fill=boxolive] at ($(l1) + (0,-0.75)$) {$\alpha_1$};
    \node[fill=boxolive] at ($(l2) + (0,-0.75)$) {$\alpha_2$};
    \node[fill=boxolive] at ($(l3) + (0,-0.75)$) {$\alpha_3$};
    \node[fill=boxmunsel] at ($(c1) + (0,-0.75)$) {CLT};
    \node[fill=boxmunsel] at ($(c2) + (0,-0.75)$) {CLT};
    \node[fill=boxmunsel] at ($(c3) + (0,-0.75)$) {CLT};
    \draw[edge] (p1) -- (l1.north);
    \draw[edge] (p1) -- (c1.north);
    \draw[edge] (p2) -- (l2.north);
    \draw[edge] (p2) -- (c2.north);
    \draw[edge] (p3) -- (l3.north);
    \draw[edge] (p3) -- (c3.north);
    \node at ($(l1) + (-0.5,0.75)$) {$\mathcal{Q}$};
    \node at ($(l2) + (-0.5,0.75)$) {$\mathcal{Q}$};
    \node at ($(l3) + (0.5,0.75)$) {$\mathcal{R}$};
    \node at ($(c1) + (-0.5,0.75)$) {$\mathcal{S}$};
    \node at ($(c2) + (0.5,0.75)$) {$\mathcal{S}$};
    \node at ($(c3) + (0.5,0.75)$) {$\mathcal{S}$};
    \begin{scope}[on background layer]
      \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(l1) + (-0.5,-0.4)$) rectangle ($(l1) + (0.5,0.4)$);
      \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(l2) + (-0.5,-0.4)$) rectangle ($(l2) + (0.5,0.4)$);
      \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(l3) + (-0.5,-0.4)$) rectangle ($(l3) + (0.5,0.4)$);
      \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(c1) + (-0.5,-0.4)$) rectangle ($(c1) + (0.5,0.4)$);
      \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(c2) + (-0.5,-0.4)$) rectangle ($(c2) + (0.5,0.4)$);
      \draw[draw=boxgoldenrod,very thick,fill=boxgoldenrod!30] ($(c3) + (-0.5,-0.4)$) rectangle ($(c3) + (0.5,0.4)$);
    \end{scope}

    \node at ($(p1) + (-2.0,1.0)$) {\scalebox{0.8}{\begin{tabular}{xxyy}
            \hline
            \multicolumn{1}{c}{$A$} & \multicolumn{1}{c}{$B$} & \multicolumn{1}{c}{$C$} & \multicolumn{1}{c}{$D$}\\
            \hline
            0 & 0 & 1 & 0\\
            1 & 1 & 0 & 1\\
            1 & 0 & 1 & 0\\
            0 & 1 & 0 & 1\\
            1 & 1 & 1 & 1\\
            1 & 0 & 0 & 0\\
            \hline
      \end{tabular}}};

    \node at ($(p3) + (2,1.0)$) {\scalebox{0.8}{\begin{tabular}{cccc}
            \hline
            $A$ & $B$ & $C$ & $D$\\
            \hline
            \rowcolor{boxblue!50}
            0 & 0 & 1 & 0\\
            \rowcolor{boxred}
            1 & 1 & 0 & 1\\
            \rowcolor{boxwheat}
            1 & 0 & 1 & 0\\
            \rowcolor{boxblue!50}
            0 & 1 & 0 & 1\\
            \rowcolor{boxred}
            1 & 1 & 1 & 1\\
            \rowcolor{boxwheat}
            1 & 0 & 0 & 0\\
            \hline
      \end{tabular}}};

    \newProdNode[label=above:{$A\wedge\neg B$}]{pob}{$(r) + (7,-1.5)$};
    \newSumNode{sa}{$(pob) + (-1,-1.0)$};
    \node (a) at ($(sa) + (-0.5,-1)$) {$A$};
    \node (na) at ($(sa) + (0.5,-1)$) {$\neg A$};
    \newSumNode{sb}{$(pob) + (1,-1.0)$};
    \node (b) at ($(sb) + (-0.5,-1)$) {$B$};
    \node (nb) at ($(sb) + (0.5,-1)$) {$\neg B$};
    \draw[edge] (pob) -- (sa);
    \draw[edge] (pob) -- (sb);
    \draw[edge] (sa) -- node[midway,left] {1} (a);
    \draw[edge] (sa) -- node[midway,right] {0} (na);
    \draw[edge] (sb) -- node[midway,left] {0} (b);
    \draw[edge] (sb) -- node[midway,right] {1} (nb);
  \end{tikzpicture}
  \caption{The first iteration of \textproc{XPC}, where $t=2$ variables are selected, $A$ and $B$;
    $k=2$ conjunctions of literals are sampled, $\alpha_1=A\wedge B$, $\alpha_2=A\wedge\neg B$ and
    $\alpha_3=\neg(\alpha_1\vee\alpha_2)$; with primes set to a product of Bernoullis corresponding
    to each $\alpha_i$ and subs to CLTs. Leaf region nodes $\mathcal{S}$ are candidates for
    expansion. Sums, products and CLT input nodes are the resulting probabilistic circuit from the
    sampled region graph. Conjunctions of literals are expanded into product of Bernoullis whose
    weights are inferred from data, as the circuit on right shows; if no smoothing is applied, the
    circuit is deterministic.}
  \label{fig:xpcs}
\end{figure}

Determinism is not the only constraint that can be enforced. In fact, since every product is by
construction 2-standardized, the circuit is structure decomposable if not only the scopes of
$\mathcal{Q}$ regions follow a vtree, but the CLTs in $\mathcal{S}$ regions do as well. This forces
the sampling done in line \ref{alg:xpcs-expand:line:sample} to instead deterministically assign
$\set{Y}$ the scope of $\Sc(v^\gets)$, where $v$ is the vtree node associated with the product
splitting $\set{X}$.

\begin{algorithm}[t]
  \caption{\textproc{ExpandXPC}}\label{alg:xpcs-expand}
  \begin{algorithmic}[1]
    \Require Region $\Region$, data $\set{D}$, variables $\set{X}$, min. \# of assignments per
      partition $\delta$, \# of conjunctions of literals $k$, length of conjunctions $t$
    \State Let $\set{A}$ be a set of logical constraints initially empty
    \State Copy all data from $\set{D}$ to $\set{S}$
    \State Sample a subset $\set{Y}$ of size $t$ from $\set{X}$\label{alg:xpcs-expand:line:sample}
    \While{$|\set{A}|<k$}
      \State Sample a conjunction of literals $\alpha$ over $\set{Y}$ distinct from any in $\set{A}$
      \State $\set{Q}\gets\{\set{x}\,|\,\forall\set{x}\in\set{S}\wedge\alpha(\set{x})=1\}$
      \If{$|\set{Q}|\geq\delta$ \textbf{and} $|\set{S}\setminus\set{Q}|\geq\delta$}
        \State Append $\alpha$ to $\set{A}$
        \State $\set{S}\gets\set{D}\setminus\set{Q}$
        \State Create a partition node $\Partition$ as a child of $\Region$
        \State Create a region $\Region_p'$ of type $\mathcal{Q}$ and assign it as a prime of
          $\Partition$
        \State Assign scope $\set{Y}$ and data $\set{Q}$ to $\Region_p'$
        \State Create a region $\Region_s'$ of type $\mathcal{S}$ and assign it as a sub of
          $\Partition$
        \State Assign scope $\set{X}\setminus\set{Y}$ and data $\set{S}\setminus\set{Q}$ to $\Region_s'$
      \EndIf
    \EndWhile
    \IIf{no constraint is suitable}{unset $\Region$ as a candidate and \textbf{return}}
    \NIElse
      \State Create a partition node $\Partition$ as a child of $\Region$
      \State Create a region $\Region_p'$ of type $\mathcal{R}$ and assign it as a prime of
        $\Partition$
      \State Assign scope $\set{Y}$ and data $\set{S}$ to $\Region_p'$
      \State Create a region $\Region_s'$ of type $\mathcal{S}$ and assign it as a sub of
        $\Partition$
      \State Assign scope $\set{X}\setminus\set{Y}$ to $\Region_s'$
    \EndNIElse
  \end{algorithmic}
\end{algorithm}

\subsubsection{Complexity}

To simplify, we assume that all $k$ distinct logical constraints pass the condition of containing
at least $\delta$ samples in the dataset. With this, the complexity of $\textproc{ExpandXPC}$ is
$\bigo(t+k\cdot|\set{D}|)$, where the first term comes from sampling a subset of size $t$ from
$\set{X}$ and the second from selecting all assignments in $\set{D}$ that agree with the constraint
$\alpha$. Sampling conjunctions of literals can be done in constant time by representing $\alpha$
as a bit vector where a 1 indicates a positive literal and 0 a negative literal; sampling a number
in $\left[0,2^{|\set{Y}|-1}\right]$ (here assumed to be done in $\bigo(1)$) is equivalent to
producing $\alpha$.

The analysis for \textproc{XPC} relies on either the number of maximum iterations or available
candidates. At every call to \textproc{ExpandXPC}, we create at least $3(k+1)$ new PC nodes, of
which $k+1$ of them are $\mathcal{S}$ regions. Assuming that we let the algorithm run a fixed number
of iterations $i$, we get a total runtime of $\bigo\left(i\cdot\left(t+k\cdot|\set{D}|\right)\right)$.
for the main loop in \textproc{XPC}. We then need to compile the PC and learn CLTs for every
$\mathcal{S}$ leaf. Because we ran for $i$ iterations, we should have $i\cdot k$ CLTs to learn,
which is done in $\bigo(|\set{X}|^2\cdot\set{D})$, bringing the total runtime to
$\bigo\left(i\cdot\left(t+k\cdot|\set{D}|\right)+i\cdot k\cdot|\set{X}|^2\cdot|\set{D}|\right)$.
Note, however, that this is a rough upper bound on the true complexity, as both scope and data
shrink considerably at each depth.

\begin{algorithm}[t]
  \caption{\textproc{XPC}}\label{alg:xpcs}
  \begin{algorithmic}[1]
    \Require Dataset $\set{D}$, variables $\set{X}$, min. \# of assignments per partition $\delta$,
    \# of conjunctions of literals $k$, length of conjunctions $t$
    \Ensure A smooth, decomposable and deterministic probabilistic circuit
    \State Start with a region graph $\mathcal{G}$ with a single region node as root
    \While{there are candidate region nodes of type $\mathcal{S}$}
      \State Select a random region $\Region$ of type $\mathcal{S}$
      \State Let $\set{Q}$ be the subdata associated with $\Region$
      \State \Call{ExpandXPC}{$\Region$, $\set{Q}$, $\Sc(\Region)$, $\delta$, $k$, $t$}
    \EndWhile
    \For{each node $\Node\in\mathcal{G}$ in reverse topological order}
      \State Let $\set{D}_{\Node}$ be the data associated with $\Node$
      \If{$\Node$ is a leaf region node of type $\mathcal{Q}$}
        \State Replace $\Node$ with a product of Bernoullis according to $\set{D}$
      \ElsIf{$\Node$ is a leaf region node of type $\mathcal{S}$}
        \State Replace $\Node$ with a CLT learned from $\set{D}$
      \ElsIf{$\Node$ is a region node}
        \State Replace $\Node$ with a sum node whose weights are
          $w_{\Node,\Child}=\frac{\set{D}_{\Child}}{\set{D}_{\Node}}$ for each $\Child\in\Ch(\Node)$
      \Else
        \State Replace $\Node$ with a product node
      \EndIf
    \EndFor
    \State \textbf{return} $\mathcal{G}$'s root sum node
  \end{algorithmic}
\end{algorithm}

\subsubsection{Pros and Cons}

\paragraph{Pros.} \textproc{XPC} is flexible in the sense that it can produce both deterministic
and structure decomposable circuits with little change to the algorithm. More importantly, because
it essentially divides data in a \divclass{} approach, the most costly operation, i.e.\ learning
CLTs at the leaves, is done extremely fast, since the optimization is done only on a fraction of
the data and scope. As an example, learning \textproc{XPC}s from binary datasets of hundreds of
variables and tens of thousands of instances takes a matter of seconds, while most competitors
usually take hours for learning from the same data. In terms of performance, although single
circuit \textproc{XPC}s rarely beat state-of-the-art competitors, \citet{dimauro21} showed that by
merely aggregating sampled circuits into a simple mixture boosts performance considerably, reaching
competitive results.

\paragraph{Cons.} When it comes to circuit size, although a single \textproc{XPC} generated circuit
has comparable size to other state-of-the-art structure learning competitors, for these to be
competitive requires ensembles of a few dozens of components, meaning that in its final form, these
can be tens of times the size of other structure learners. Moreover, because of the number of
parameters involved in sampling these PCs ($\delta$, $k$, $t$, number of components per ensemble,
and whether to produce deterministic and/or structure decomposable PCs), a grid-search over
parameters is necessary to produce optimal results. Although this is generally faster than other
structure learning algorithms, the sheer size of all generated circuits from every hyperparameter
combination can easily overwhelm memory space.

\section{A Summary}

We finish this review of structure learning algorithms in probabilistic circuits by summarizing
some of the more important points raised throughout this chapter in \Cref{tab:learning}. We list
each algorithm seen in \Cref{ch:learning}, describing their class, time complexity of learning the
probabilistic circuit and any other auxiliary data structure, number of hyperparameters needed
during learning, which structural constraints are guaranteed to hold in the resulting PCs, and
which data (binary $\{0,1\}$, discrete $\mathbb{N}$ or continuous $\mathbb{R}$) are supported. We
use the same notation used throughout this section for data dimensions: $n$ is the number of
examples in a dataset and $m$ is the number of variables of dataset. Other variable names differ in
their meaning depending on each learning technique. We next describe some of the assumptions made
in order to more concisely summarize the information set in \Cref{tab:learning}.

For \textproc{LearnSPN}, we assume sums to be learned by $c$ iterations of $k$-means and products
through the G-test. We call $k$ the number of clusters to be learned, and only assume the bare
minimum as hyperparameters: $k$ and the G-test $p$-value, giving a lower bound of $2$ on the number
of hyperparameters. \textproc{LearnSPN} is easily extensible to continuous data by replacing the
G-test with any other continuous alternative, such as mutual information, and learning continuous
univariate distributions as inputs.

Recall that \textproc{ID-SPN} learns Markov networks as inputs. If we assume this process to follow
the same procedure proposed in \citet{rooshenas14}, then the number of hyperparameters needed for
just learning the structure of the Markov network inputs is at least three (per-edge penalty,
per-split penalty and score tolerance heuristic\footnote{See the Libra Toolkit manual for more
information \citep{libra}.}). Just like in \textproc{LearnSPN}, we also assume sums and products to
be learned from $k$-means and the G-test for \textproc{ID-SPN}, raising the number of
hyperparameters to 5, 2 for sums and products and 3 for inputs. We use the same notation as
\Cref{sec:idspn}: $i$ is the number of iterations for learning the Markov networks, $c$ is the size
of the Markov network being learned, $r$ is a constant bounding the number of improvements, and $k$
is the number of clusters used for sums.

We assume \textproc{Prometheus} to use its more scalable version of learning products by sampling
edges from the correlation graph and sums learned from $k$-means. Because the procedure for
learning products is parameterless, we are left with only $k$ as a hyperparameter for sums. We use
the same variable notation as the other \divclass{} algorithms.

For both \textproc{LearnPSDD} and \textproc{Strudel}, we consider only the maximum depth $m$ when
partially copying the circuit during a local transformation as a hyperparameter. We do not consider
the maximum number of iterations $i$ as a hyperparameter, as it acts more like a time constraint
rather than a parameter to be optimized. We denote $\mathcal{C}$ as the probabilistic circuit
being learned.

When describing \textproc{RAT-SPN}, we denote $r$, $d$, $s$ and $l$ as the number of subcircuits
learned, maximum depth of the generated region graph, number of sums per inner region node, and
number of inputs per leaf region node, all of which are accounted as hyperparameters in
\Cref{tab:learning}.

In the case of \textproc{XPC}, we call $i$ the number of expansions to carry out in total, $t$ the
length of sampled conjunctions of literals and $k$ the number of conjunctions to be sampled per
region. Of these, $t$ and $k$, together with the number of assignments per partition $\delta$, are
considered \textproc{XPC} hyperparameters, bringing the total number to three.

\begin{sidewaystable}
  \resizebox{\textheight}{!}{
    \begin{tabular}{c|clc|cccc|ccc|c}
    \hline
    \textbf{Name} & \textbf{Class} & \multicolumn{1}{c}{\textbf{Time Complexity}} & \textbf{\# hyperparams}
    & \textbf{Smooth?} & \textbf{Dec?} & \textbf{Det?} & \textbf{Str. Dec?} &
    \textbf{$\mathbf{\{0,1\}}$?} & $\mathbb{N}$\textbf{?} & $\mathbb{R}$\textbf{?} & \textbf{Reference}\\
    \hline
    \textproc{LearnSPN} & \divclass{} & $
    \begin{cases}
      \bigo\left(nkmc\right) & \text{, if sum}\\
      \bigo\left(nm^3\right) & \text{, if product}
    \end{cases}
    $ & $\geq 2$ & \cmark & \cmark & \xmark & \xmark & \cmark & \cmark & \cmark & \Cref{sec:learnspn}\\
    \textproc{ID-SPN} & \divclass{} & $
    \begin{cases}
      \bigo\left(nkmc\right) & \text{, if sum}\\
      \bigo\left(nm^3\right) & \text{, if product}\\
      \bigo\left(ic(rn+m)\right)  & \text{, if input}
    \end{cases}
    $ & $\geq 2+3$ & \cmark & \cmark & \xmark & \xmark & \cmark & \cmark & \xmark & \Cref{sec:idspn}\\
    \textproc{Prometheus} & \divclass{} & $
    \begin{cases}
      \bigo\left(nkmc\right) & \text{, if sum}\\
      \bigo\left(m(\log m)^2\right) & \text{, if product}
    \end{cases}
    $ & $\geq 1$ & \cmark & \cmark & \xmark & \xmark & \cmark & \cmark & \cmark & \Cref{sec:prometheus}\\
    \hline
    \textproc{LearnPSDD} & \incrclass{} & $
    \begin{cases}
      \bigo\left(m^2\right) & \text{, top-down vtree}\\
      \bigo\left(m^4\right) & \text{, bottom-up vtree}\\
      \bigo\left(i|\mathcal{C}|^2\right) & \text{, circuit structure}
    \end{cases}
    $ & $1$ & \cmark & \cmark & \cmark & \cmark & \cmark & \xmark & \xmark & \Cref{sec:learnpsdd}\\
    \textproc{Strudel} & \incrclass{} & $
    \begin{cases}
      \bigo\left(m^2 n\right) & \text{, CLT + vtree}\\
      \bigo\left(i\left(|\mathcal{C}|n+m^2\right)\right) & \text{, circuit structure}
    \end{cases}
    $ & $1$ & \cmark & \cmark & \cmark & \cmark & \cmark & \xmark & \xmark & \Cref{sec:strudel}\\
    \hline
    & & & & & & & & & & & \\
    \textproc{RAT-SPN} & \randclass{} & $\phantom{\{}\bigo\left(rd(s+l)\right)$ & $4$ & \cmark & \cmark
                       & \xmark & \xmark & \cmark & \cmark & \cmark & \Cref{sec:ratspn}\\
    & & & & & & & & & & & \\
    \textproc{XPC} & \randclass{} & $\phantom{\{}\bigo\left(i(t+kn)+ikm^2n\right)$ & $3$ & \cmark & \cmark
                   & \cmark & \cmark & \cmark & \xmark & \xmark & \Cref{sec:xpcs}\\[5pt]
    \hline
  \end{tabular}
  }
  \label{tab:learning}
\end{sidewaystable}
